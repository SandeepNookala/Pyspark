
1.Installing Pyspark and Creating new SparkSession:
===================================================
!pip install pyspark py4j

from pyspark.sql import SparkSession

spark = SparkSession.bulider.master('yarn/local').appName('dataframe').getOrCreate()




2.Creating dataFrame from hdfs Location:
========================================
reading data from hdfs remote server
df_hdfs = spark.read.format('csv').option().load('hdfs://172.16.38.131.8020/bigdata/cse/app_prod/cse.app_prod.csv')



3.Read CSV data into dataframe without header, schema:
======================================================
spark.read.format('csv').load('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/employee.csv').show(10)



4.Read csv data into dataframe with Header and schema.Fetch type,row count,10 rows and printSchema:
===================================================================================================
df_csv = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/employee.csv')

#ten rows
df_csv.show(10)

#printSchema?
df_csv.printSchema()

#count
df_csv.count()

#show type
type(df_csv)



5.Read csv dataframe with pipe seprated values.Fetch row count,10 rows and printSchema:
=======================================================================================
df_pipe = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').option('sep','|')
.load('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/emp_pipe.txt')

df_pipe.show(10)
df_pipe.printSchema()
df_pipe.count()



6.Read csv dataframe with double pipe seprated values ,Fetch row count,10 rows and printSchema:
===============================================================================================
df_double = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','nul').option('sep','||').load('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/emp_double_pipe.txt')

df_double.show(10)
df_double.count()
df_double.printSchema()



7.Inspect Data:
===============
#show dataframe
df_double.show(10)

#printSchema
df_double.printSchema()

#column names
df_double.columns

#column name with datatypes
df_double.dtypes

#firstrow
df_double.first()

#head - first five rows
df_double.head(5)

#take any random 3 rows
df_double.take(3)

#count no.of rows
df_double.count()

#count distinct no.of rows
df_double.distinct().count()

#tail - last two rows
df_double.tail(2)

# compute summery satistics for dataframe
df_double.describe().show()

#explain physical plan and logical plan
df_double.explain()



8.import functions:
===================
from pyspark.sql.functions import *


fun = spark.sql('show functions')
print(fun.count())
print(fun.show())



9.select operations:
====================
df_double.select( (col('MGR')/2).alias('new_mgr'),(col('sal')*2).alias('new_salary')).show(5)



10.Adding new columns with default values:
==========================================
df_double1 = df_double.withColumn('source',lit('test'))
df_double1.show(10)



11.Adding new columns:
======================
df_double2 = df_double.withColumn('NEW_MGR', col('MGR')*2).withColumn('NEW_SAL',col('SAL')/10)
df_double2.show(10)



12.dropping columns:
====================
df_double3 =df_double2.drop('MGR','SAL')
df_double3.show(10)



13.Renamed Column Names:
========================
df_double4= df_double3.withColumnRenamed('NEW_SAL','SALARY').withColumnRenamed('NEW_MGR','MANAGER')
df_double4.show(10)



14.drop null values from dataFrame:
===================================

# how = any ,it drops any null value in dataframe
df_drop_any =  df_pipe.dropna(how = 'any')
df_drop_any.count()


# how =all,drops the rows with all nulls in it
df_drop_all = df_pipe.dropna(how = 'all')
df_drop_all.count()


# how =all,drops the rows with custom nulls in it
df_drop_custom = df_pipe.dropna(how = 'any' ,thresh = 4)
df_drop_custom.count()


#drop the specific columns null values
df_drop_column = df_pipe.dropna(how = 'any',subset=['UPDATED_DATE','HIREDATE'])
df_drop_column.count()



15.fill null values in dataFrame:
================================
df_fill = df_pipe.fillna('MISSING_VALUE',['JOB','ENAME']).fillna(7000,'MGR').fillna(1000,'SAL')
df_fill.show()



16.Filter operations:
=====================

#salary of people greter than 4000
df_pipe.filter(col('SAL') >4000).show()


# & operator
#show employee details who's salar greater than 2500 and job is manager
df_pipe.filter( (col('JOB') == 'MANAGER') & (col('SAL') > 2500)).show()


# & operator
#show employee details who's salary less than 2000 and salary should be greater than 1000
df_pipe.filter((col('SAL') <2000) & ( col('SAL')>1000)).show()


# | operator
#show employee details who's salary less than 2000 or salary should be greater than 1000
df_pipe.filter((col('SAL')>2000) | (col('SAL')<1000)).show()


#not operator
df_pipe.filter( ~ (col('JOB') == 'MANAGER')).show()


# filter individual Columns null values
df_pipe.filter('HIREDATE is null').show()
df_pipe.filter('MGR is null').show()



17.GroupBy operations:
======================
#groupby deptno and people count
df_pipe.groupby('DEPTNO').count().show()


#groupby deptno and avg salary
df_pipe.groupby('DEPTNO').avg('SAL').show()


#groupby job role and people count
df_pipe.groupBy('JOB').count().show()


#Group by job role and avg of salaries
df_pipe.groupby('JOB').avg('SAL').alias('AVG_SAL').show()



18.sort/orderby operations - In pyspark orderBy and sort are same:
==================================================================
df_pipe.sort(col('SAL').desc()).show(5)
df_pipe.orderBy(col('SAL').desc()).show(5)



19.sort by/sortwithinpartitions operations- In pyspark sortwithinpartitions same as sortby:
====================
df_pipe.sortWithinPartitions( col('SAL').desc()).show()



20.repartition:
===============
repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions, we can repartition based on column specific to increse the performence

df_pipe1 = df_pipe.repartition(3).withColumn('repartitions',spark_partition_id())
df_pipe1.show(10)
df_pipe1.rdd.getNumPartitions()



21.coalesce:
============
coalesce : is a Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.

df_pipe2 = df_pipe1.coalesce(1).withColumn('coalesce',spark_partition_id())
df_pipe2.show()
df_pipe2.rdd.getNumPartitions()



22.joining two dataframes - Generate Revenue Column and order by highest Revenue:
================================================================================
df_product = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/product_info_1.csv')

df_sale = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/sales_data_1.csv')

df_product.show(10)
df_product.printSchema()
df_sale.show(10)
df_sale.printSchema()

product_sale = df_product.join(df_sale,df_product.product_id == df_sale.product_id,'inner').drop(df_sale.product_id).withColumn('Revenue', col('unit_price')*col('qty')).orderBy(col('Revenue').desc())
product_sale.show()



23.Save Dataframe into hive table:
=================================
product_sale.write.partitionBy('product_name').saveAsTable('product_sale')



24.Read Dataframe from Hive table:
=================================
spark.read.table('product_sale').show()



25.Pivot Table : year wise each product revenue:
================================================
product_sale.groupBy('year').pivot('product_name').sum('Revenue').show(truncate = False)