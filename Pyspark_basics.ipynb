{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4HxuWH7wDANA",
        "95UKMxw_Nypz",
        "U_2N-cGxR0at",
        "EA2YRT5ULuz7",
        "x3LSF65HMUEf",
        "1RFcRqrBTL8j",
        "IINtHsNlceIi",
        "yaS5w6_Wd_Kv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pyspark in Googlecolab"
      ],
      "metadata": {
        "id": "baxEy4c7QtUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1Hf2vCSQxYW",
        "outputId": "3ae72fd1-0782-4365-aa42-9e70f496e965"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.9/dist-packages (0.10.9.7)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824028 sha256=1f445062181fa025dcc1f3be13451efe710282fb21134223fedca7a5007cb7de\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/e3/9b/0525ce8a69478916513509d43693511463c6468db0de237c86\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.7\n",
            "    Uninstalling py4j-0.10.9.7:\n",
            "      Successfully uninstalled py4j-0.10.9.7\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating new SparkContext"
      ],
      "metadata": {
        "id": "DGSahIzuR9aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "l0nWrdXoRFwf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "y-09CKdQR2Tz",
        "outputId": "447c9691-fd5a-485d-f044-2f951399c5ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d40f7ea4da10:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L = [i for i in range(0,20,2)]\n",
        "\n",
        "L1 = [ (i ,j) for i in range(10) if i%2 == 0  for j in range(10) if j%2 != 0 ]\n",
        "\n",
        "L2 = [('s',1),('a',2),('n',3),('d',4),('e',5),('e',6),('p',7)]\n",
        "\n",
        "print(L)\n",
        "print(L1)\n",
        "print(L2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn7FEjn1tusf",
        "outputId": "4f723d4c-c76b-444b-c8ba-d75998c69967"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "[(0, 1), (0, 3), (0, 5), (0, 7), (0, 9), (2, 1), (2, 3), (2, 5), (2, 7), (2, 9), (4, 1), (4, 3), (4, 5), (4, 7), (4, 9), (6, 1), (6, 3), (6, 5), (6, 7), (6, 9), (8, 1), (8, 3), (8, 5), (8, 7), (8, 9)]\n",
            "[('s', 1), ('a', 2), ('n', 3), ('d', 4), ('e', 5), ('e', 6), ('p', 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating RDD from list"
      ],
      "metadata": {
        "id": "VJ6L9nE7SR7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(L)\n",
        "\n",
        "rdd1 = sc.parallelize(L1)\n",
        "\n",
        "rdd2 = sc.parallelize(L2)"
      ],
      "metadata": {
        "id": "PZpRlPdgR0TV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd.collect(),type(rdd))\n",
        "print(rdd1.collect(),type(rdd1))\n",
        "print(rdd2.collect(),type(rdd2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l99zfiSt9px",
        "outputId": "3eaaf3a7-4123-4182-abd7-0a965b891c73"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18] <class 'pyspark.rdd.RDD'>\n",
            "[(0, 1), (0, 3), (0, 5), (0, 7), (0, 9), (2, 1), (2, 3), (2, 5), (2, 7), (2, 9), (4, 1), (4, 3), (4, 5), (4, 7), (4, 9), (6, 1), (6, 3), (6, 5), (6, 7), (6, 9), (8, 1), (8, 3), (8, 5), (8, 7), (8, 9)] <class 'pyspark.rdd.RDD'>\n",
            "[('s', 1), ('a', 2), ('n', 3), ('d', 4), ('e', 5), ('e', 6), ('p', 7)] <class 'pyspark.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To know All methods in rdd"
      ],
      "metadata": {
        "id": "SaqbTaK6h28x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir(rdd)            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp3DbOx-W45r",
        "outputId": "6cdb2373-dcac-43e7-9f3c-777d578bb815"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getnewargs__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__orig_bases__',\n",
              " '__parameters__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_computeFractionForSampleSize',\n",
              " '_defaultReducePartitions',\n",
              " '_id',\n",
              " '_is_barrier',\n",
              " '_is_protocol',\n",
              " '_jrdd',\n",
              " '_jrdd_deserializer',\n",
              " '_memory_limit',\n",
              " '_pickled',\n",
              " '_reserialize',\n",
              " '_to_java_object_rdd',\n",
              " 'aggregate',\n",
              " 'aggregateByKey',\n",
              " 'barrier',\n",
              " 'cache',\n",
              " 'cartesian',\n",
              " 'checkpoint',\n",
              " 'cleanShuffleDependencies',\n",
              " 'coalesce',\n",
              " 'cogroup',\n",
              " 'collect',\n",
              " 'collectAsMap',\n",
              " 'collectWithJobGroup',\n",
              " 'combineByKey',\n",
              " 'context',\n",
              " 'count',\n",
              " 'countApprox',\n",
              " 'countApproxDistinct',\n",
              " 'countByKey',\n",
              " 'countByValue',\n",
              " 'ctx',\n",
              " 'distinct',\n",
              " 'filter',\n",
              " 'first',\n",
              " 'flatMap',\n",
              " 'flatMapValues',\n",
              " 'fold',\n",
              " 'foldByKey',\n",
              " 'foreach',\n",
              " 'foreachPartition',\n",
              " 'fullOuterJoin',\n",
              " 'getCheckpointFile',\n",
              " 'getNumPartitions',\n",
              " 'getResourceProfile',\n",
              " 'getStorageLevel',\n",
              " 'glom',\n",
              " 'groupBy',\n",
              " 'groupByKey',\n",
              " 'groupWith',\n",
              " 'has_resource_profile',\n",
              " 'histogram',\n",
              " 'id',\n",
              " 'intersection',\n",
              " 'isCheckpointed',\n",
              " 'isEmpty',\n",
              " 'isLocallyCheckpointed',\n",
              " 'is_cached',\n",
              " 'is_checkpointed',\n",
              " 'join',\n",
              " 'keyBy',\n",
              " 'keys',\n",
              " 'leftOuterJoin',\n",
              " 'localCheckpoint',\n",
              " 'lookup',\n",
              " 'map',\n",
              " 'mapPartitions',\n",
              " 'mapPartitionsWithIndex',\n",
              " 'mapPartitionsWithSplit',\n",
              " 'mapValues',\n",
              " 'max',\n",
              " 'mean',\n",
              " 'meanApprox',\n",
              " 'min',\n",
              " 'name',\n",
              " 'partitionBy',\n",
              " 'partitioner',\n",
              " 'persist',\n",
              " 'pipe',\n",
              " 'randomSplit',\n",
              " 'reduce',\n",
              " 'reduceByKey',\n",
              " 'reduceByKeyLocally',\n",
              " 'repartition',\n",
              " 'repartitionAndSortWithinPartitions',\n",
              " 'rightOuterJoin',\n",
              " 'sample',\n",
              " 'sampleByKey',\n",
              " 'sampleStdev',\n",
              " 'sampleVariance',\n",
              " 'saveAsHadoopDataset',\n",
              " 'saveAsHadoopFile',\n",
              " 'saveAsNewAPIHadoopDataset',\n",
              " 'saveAsNewAPIHadoopFile',\n",
              " 'saveAsPickleFile',\n",
              " 'saveAsSequenceFile',\n",
              " 'saveAsTextFile',\n",
              " 'setName',\n",
              " 'sortBy',\n",
              " 'sortByKey',\n",
              " 'stats',\n",
              " 'stdev',\n",
              " 'subtract',\n",
              " 'subtractByKey',\n",
              " 'sum',\n",
              " 'sumApprox',\n",
              " 'take',\n",
              " 'takeOrdered',\n",
              " 'takeSample',\n",
              " 'toDF',\n",
              " 'toDebugString',\n",
              " 'toLocalIterator',\n",
              " 'top',\n",
              " 'treeAggregate',\n",
              " 'treeReduce',\n",
              " 'union',\n",
              " 'unpersist',\n",
              " 'values',\n",
              " 'variance',\n",
              " 'withResources',\n",
              " 'zip',\n",
              " 'zipWithIndex',\n",
              " 'zipWithUniqueId']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Help"
      ],
      "metadata": {
        "id": "mnSqPh0uXthA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(rdd)   #to know exact syntax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjBEQJg2Xw12",
        "outputId": "9a4d0c74-a538-49c3-9c7e-d68649a206c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on RDD in module pyspark.rdd object:\n",
            "\n",
            "class RDD(typing.Generic)\n",
            " |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
            " |  \n",
            " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
            " |  Represents an immutable, partitioned collection of elements that can be\n",
            " |  operated on in parallel.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      RDD\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
            " |      Return the union of this RDD and another one.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
            " |      >>> (rdd + rdd).collect()\n",
            " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
            " |  \n",
            " |  __getnewargs__(self) -> NoReturn\n",
            " |  \n",
            " |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n",
            " |      Aggregate the elements of each partition, and then the results for all\n",
            " |      the partitions, using a given combine functions and a neutral \"zero\n",
            " |      value.\"\n",
            " |      \n",
            " |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
            " |      as its result value to avoid object allocation; however, it should not\n",
            " |      modify ``t2``.\n",
            " |      \n",
            " |      The first function (seqOp) can return a different result type, U, than\n",
            " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
            " |      an U and one operation for merging two U\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
            " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
            " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
            " |      (10, 4)\n",
            " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
            " |      (0, 0)\n",
            " |  \n",
            " |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f3e2ec3cca0>) -> 'RDD[Tuple[K, U]]'\n",
            " |      Aggregate the values of each key, using given combine functions and a neutral\n",
            " |      \"zero value\". This function can return a different result type, U, than the type\n",
            " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
            " |      a U and one operation for merging two U's, The former operation is used for merging\n",
            " |      values within a partition, and the latter is used for merging values between\n",
            " |      partitions. To avoid memory allocation, both of these functions are\n",
            " |      allowed to modify and return their first argument instead of creating a new U.\n",
            " |  \n",
            " |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n",
            " |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
            " |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
            " |      entire stage and relaunch all tasks for this stage.\n",
            " |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
            " |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDDBarrier`\n",
            " |          instance that provides actions within a barrier stage.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.BarrierTaskContext\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      For additional information see\n",
            " |      \n",
            " |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n",
            " |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n",
            " |      \n",
            " |      This API is experimental\n",
            " |  \n",
            " |  cache(self: 'RDD[T]') -> 'RDD[T]'\n",
            " |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
            " |  \n",
            " |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
            " |      Return the Cartesian product of this RDD and another one, that is, the\n",
            " |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
            " |      ``b`` is in `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2])\n",
            " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
            " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
            " |  \n",
            " |  checkpoint(self) -> None\n",
            " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
            " |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
            " |      all references to its parent RDDs will be removed. This function must\n",
            " |      be called before any job has been executed on this RDD. It is strongly\n",
            " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
            " |      on a file will require recomputation.\n",
            " |  \n",
            " |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n",
            " |      Removes an RDD's shuffles and it's non-persisted ancestors.\n",
            " |      \n",
            " |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n",
            " |      If you use the RDD after this call, you should checkpoint and materialize it first.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      blocking : bool, optional\n",
            " |         block on shuffle cleanup tasks. Disabled by default.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is a developer API.\n",
            " |  \n",
            " |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n",
            " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
            " |      [[1], [2, 3], [4, 5]]\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
            " |      [[1, 2, 3, 4, 5]]\n",
            " |  \n",
            " |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n",
            " |      For each key k in `self` or `other`, return a resulting RDD that\n",
            " |      contains a tuple with the list of values for that key in `self` as\n",
            " |      well as `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
            " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
            " |  \n",
            " |  collect(self: 'RDD[T]') -> List[~T]\n",
            " |      Return a list that contains all of the elements in this RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |  \n",
            " |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n",
            " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting data is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
            " |      >>> m[1]\n",
            " |      2\n",
            " |      >>> m[3]\n",
            " |      4\n",
            " |  \n",
            " |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n",
            " |      When collect rdd, use this method to specify job group.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      .. deprecated:: 3.1.0\n",
            " |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n",
            " |  \n",
            " |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f3e2ec3cca0>) -> 'RDD[Tuple[K, U]]'\n",
            " |      Generic function to combine the elements for each key using a custom\n",
            " |      set of aggregation functions.\n",
            " |      \n",
            " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
            " |      type\" C.\n",
            " |      \n",
            " |      Users provide three functions:\n",
            " |      \n",
            " |          - `createCombiner`, which turns a V into a C (e.g., creates\n",
            " |            a one-element list)\n",
            " |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n",
            " |            a list)\n",
            " |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n",
            " |            the lists)\n",
            " |      \n",
            " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
            " |      modify and return their first argument instead of creating a new C.\n",
            " |      \n",
            " |      In addition, users can control the partitioning of the output RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      V and C can be different -- for example, one might group an RDD of type\n",
            " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
            " |      >>> def to_list(a):\n",
            " |      ...     return [a]\n",
            " |      ...\n",
            " |      >>> def append(a, b):\n",
            " |      ...     a.append(b)\n",
            " |      ...     return a\n",
            " |      ...\n",
            " |      >>> def extend(a, b):\n",
            " |      ...     a.extend(b)\n",
            " |      ...     return a\n",
            " |      ...\n",
            " |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
            " |      [('a', [1, 2]), ('b', [1])]\n",
            " |  \n",
            " |  count(self) -> int\n",
            " |      Return the number of elements in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([2, 3, 4]).count()\n",
            " |      3\n",
            " |  \n",
            " |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n",
            " |      Approximate version of count() that returns a potentially incomplete\n",
            " |      result within a timeout, even if not all tasks have finished.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
            " |      >>> rdd.countApprox(1000, 1.0)\n",
            " |      1000\n",
            " |  \n",
            " |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n",
            " |      Return approximate number of distinct elements in the RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      relativeSD : float, optional\n",
            " |          Relative accuracy. Smaller values create\n",
            " |          counters that require more space.\n",
            " |          It must be greater than 0.000017.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The algorithm used is based on streamlib's implementation of\n",
            " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
            " |      of The Art Cardinality Estimation Algorithm\", available here\n",
            " |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
            " |      >>> 900 < n < 1100\n",
            " |      True\n",
            " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
            " |      >>> 16 < n < 24\n",
            " |      True\n",
            " |  \n",
            " |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n",
            " |      Count the number of elements for each key, and return the result to the\n",
            " |      master as a dictionary.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.countByKey().items())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n",
            " |      Return the count of each unique value in this RDD as a dictionary of\n",
            " |      (value, count) pairs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
            " |      [(1, 2), (2, 3)]\n",
            " |  \n",
            " |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
            " |      Return a new RDD containing the distinct elements in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
            " |      [1, 2, 3]\n",
            " |  \n",
            " |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n",
            " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
            " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
            " |      [2, 4]\n",
            " |  \n",
            " |  first(self: 'RDD[T]') -> ~T\n",
            " |      Return the first element in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([2, 3, 4]).first()\n",
            " |      2\n",
            " |      >>> sc.parallelize([]).first()\n",
            " |      Traceback (most recent call last):\n",
            " |          ...\n",
            " |      ValueError: RDD is empty\n",
            " |  \n",
            " |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by first applying a function to all elements of this\n",
            " |      RDD, and then flattening the results.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
            " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
            " |      [1, 1, 1, 2, 2, 3]\n",
            " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
            " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
            " |  \n",
            " |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n",
            " |      Pass each value in the key-value pair RDD through a flatMap function\n",
            " |      without changing the keys; this also retains the original RDD's\n",
            " |      partitioning.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
            " |      >>> def f(x): return x\n",
            " |      >>> x.flatMapValues(f).collect()\n",
            " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
            " |  \n",
            " |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n",
            " |      Aggregate the elements of each partition, and then the results for all\n",
            " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
            " |      \n",
            " |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
            " |      as its result value to avoid object allocation; however, it should not\n",
            " |      modify ``t2``.\n",
            " |      \n",
            " |      This behaves somewhat differently from fold operations implemented\n",
            " |      for non-distributed collections in functional languages like Scala.\n",
            " |      This fold operation may be applied to partitions individually, and then\n",
            " |      fold those results into the final result, rather than apply the fold\n",
            " |      to each element sequentially in some defined ordering. For functions\n",
            " |      that are not commutative, the result may differ from that of a fold\n",
            " |      applied to a non-distributed collection.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
            " |      15\n",
            " |  \n",
            " |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f3e2ec3cca0>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Merge the values for each key using an associative function \"func\"\n",
            " |      and a neutral \"zeroValue\" which may be added to the result an\n",
            " |      arbitrary number of times, and must not change the result\n",
            " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> from operator import add\n",
            " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n",
            " |      Applies a function to all elements of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> def f(x): print(x)\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
            " |  \n",
            " |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n",
            " |      Applies a function to each partition of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> def f(iterator):\n",
            " |      ...     for x in iterator:\n",
            " |      ...          print(x)\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
            " |  \n",
            " |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n",
            " |      Perform a right outer join of `self` and `other`.\n",
            " |      \n",
            " |      For each element (k, v) in `self`, the resulting RDD will either\n",
            " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
            " |      (k, (v, None)) if no elements in `other` have key k.\n",
            " |      \n",
            " |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
            " |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
            " |      (k, (None, w)) if no elements in `self` have key k.\n",
            " |      \n",
            " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
            " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
            " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
            " |  \n",
            " |  getCheckpointFile(self) -> Optional[str]\n",
            " |      Gets the name of the file to which this RDD was checkpointed\n",
            " |      \n",
            " |      Not defined if RDD is checkpointed locally.\n",
            " |  \n",
            " |  getNumPartitions(self) -> int\n",
            " |      Returns the number of partitions in RDD\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
            " |      >>> rdd.getNumPartitions()\n",
            " |      2\n",
            " |  \n",
            " |  getResourceProfile(self) -> Optional[pyspark.resource.profile.ResourceProfile]\n",
            " |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n",
            " |      if it wasn't specified.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :py:class:`pyspark.resource.ResourceProfile`\n",
            " |          The user specified profile or None if none were specified\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental\n",
            " |  \n",
            " |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n",
            " |      Get the RDD's current storage level.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd1 = sc.parallelize([1,2])\n",
            " |      >>> rdd1.getStorageLevel()\n",
            " |      StorageLevel(False, False, False, False, 1)\n",
            " |      >>> print(rdd1.getStorageLevel())\n",
            " |      Serialized 1x Replicated\n",
            " |  \n",
            " |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n",
            " |      Return an RDD created by coalescing all elements within each partition\n",
            " |      into a list.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
            " |      >>> sorted(rdd.glom().collect())\n",
            " |      [[1, 2], [3, 4]]\n",
            " |  \n",
            " |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f3e2ec3cca0>) -> 'RDD[Tuple[K, Iterable[T]]]'\n",
            " |      Return an RDD of grouped items.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
            " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
            " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
            " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
            " |  \n",
            " |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f3e2ec3cca0>) -> 'RDD[Tuple[K, Iterable[V]]]'\n",
            " |      Group the values for each key in the RDD into a single sequence.\n",
            " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If you are grouping in order to perform an aggregation (such as a\n",
            " |      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
            " |      provide much better performance.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
            " |      [('a', [1, 1]), ('b', [1])]\n",
            " |  \n",
            " |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n",
            " |      Alias for cogroup but with support for multiple RDDs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
            " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
            " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
            " |  \n",
            " |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n",
            " |      Compute a histogram using the provided buckets. The buckets\n",
            " |      are all open to the right except for the last which is closed.\n",
            " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
            " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
            " |      and 50 we would have a histogram of 1,0,1.\n",
            " |      \n",
            " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
            " |      this can be switched from an O(log n) insertion to O(1) per\n",
            " |      element (where n is the number of buckets).\n",
            " |      \n",
            " |      Buckets must be sorted, not contain any duplicates, and have\n",
            " |      at least two elements.\n",
            " |      \n",
            " |      If `buckets` is a number, it will generate buckets which are\n",
            " |      evenly spaced between the minimum and maximum of the RDD. For\n",
            " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
            " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
            " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
            " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
            " |      will be used.\n",
            " |      \n",
            " |      The return value is a tuple of buckets and histogram.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(51))\n",
            " |      >>> rdd.histogram(2)\n",
            " |      ([0, 25, 50], [25, 26])\n",
            " |      >>> rdd.histogram([0, 5, 25, 50])\n",
            " |      ([0, 5, 25, 50], [5, 20, 26])\n",
            " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
            " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
            " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
            " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
            " |      (('a', 'b', 'c'), [2, 2])\n",
            " |  \n",
            " |  id(self) -> int\n",
            " |      A unique ID for this RDD (within its SparkContext).\n",
            " |  \n",
            " |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n",
            " |      Return the intersection of this RDD and another one. The output will\n",
            " |      not contain any duplicate elements, even if the input RDDs did.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method performs a shuffle internally.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
            " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
            " |      >>> rdd1.intersection(rdd2).collect()\n",
            " |      [1, 2, 3]\n",
            " |  \n",
            " |  isCheckpointed(self) -> bool\n",
            " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
            " |  \n",
            " |  isEmpty(self) -> bool\n",
            " |      Returns true if and only if the RDD contains no elements at all.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      An RDD may be empty even when it has at least 1 partition.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([]).isEmpty()\n",
            " |      True\n",
            " |      >>> sc.parallelize([1]).isEmpty()\n",
            " |      False\n",
            " |  \n",
            " |  isLocallyCheckpointed(self) -> bool\n",
            " |      Return whether this RDD is marked for local checkpointing.\n",
            " |      \n",
            " |      Exposed for testing.\n",
            " |  \n",
            " |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n",
            " |      Return an RDD containing all pairs of elements with matching keys in\n",
            " |      `self` and `other`.\n",
            " |      \n",
            " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
            " |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
            " |      \n",
            " |      Performs a hash join across the cluster.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
            " |      >>> sorted(x.join(y).collect())\n",
            " |      [('a', (1, 2)), ('a', (1, 3))]\n",
            " |  \n",
            " |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n",
            " |      Creates tuples of the elements in this RDD by applying `f`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
            " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
            " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
            " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
            " |  \n",
            " |  keys(self: 'RDD[Tuple[K, V]]') -> 'RDD[K]'\n",
            " |      Return an RDD with the keys of each tuple.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
            " |      >>> m.collect()\n",
            " |      [1, 3]\n",
            " |  \n",
            " |  leftOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, Optional[U]]]]'\n",
            " |      Perform a left outer join of `self` and `other`.\n",
            " |      \n",
            " |      For each element (k, v) in `self`, the resulting RDD will either\n",
            " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
            " |      (k, (v, None)) if no elements in `other` have key k.\n",
            " |      \n",
            " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
            " |      [('a', (1, 2)), ('b', (4, None))]\n",
            " |  \n",
            " |  localCheckpoint(self) -> None\n",
            " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
            " |      \n",
            " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
            " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
            " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
            " |      \n",
            " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
            " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
            " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
            " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
            " |      \n",
            " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
            " |      with their cached blocks. If you must use both features, you are advised to set\n",
            " |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
            " |      \n",
            " |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
            " |  \n",
            " |  lookup(self: 'RDD[Tuple[K, V]]', key: ~K) -> List[~V]\n",
            " |      Return the list of values in the RDD for key `key`. This operation\n",
            " |      is done efficiently if the RDD has a known partitioner by only\n",
            " |      searching the partition that the key maps to.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> l = range(1000)\n",
            " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
            " |      >>> rdd.lookup(42)  # slow\n",
            " |      [42]\n",
            " |      >>> sorted = rdd.sortByKey()\n",
            " |      >>> sorted.lookup(42)  # fast\n",
            " |      [42]\n",
            " |      >>> sorted.lookup(1024)\n",
            " |      []\n",
            " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
            " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
            " |      ['c']\n",
            " |  \n",
            " |  map(self: 'RDD[T]', f: Callable[[~T], ~U], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each element of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
            " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
            " |      [('a', 1), ('b', 1), ('c', 1)]\n",
            " |  \n",
            " |  mapPartitions(self: 'RDD[T]', f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each partition of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
            " |      >>> def f(iterator): yield sum(iterator)\n",
            " |      >>> rdd.mapPartitions(f).collect()\n",
            " |      [3, 7]\n",
            " |  \n",
            " |  mapPartitionsWithIndex(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each partition of this RDD,\n",
            " |      while tracking the index of the original partition.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
            " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
            " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
            " |      6\n",
            " |  \n",
            " |  mapPartitionsWithSplit(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each partition of this RDD,\n",
            " |      while tracking the index of the original partition.\n",
            " |      \n",
            " |      .. deprecated:: 0.9.0\n",
            " |          use :py:meth:`RDD.mapPartitionsWithIndex` instead.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
            " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
            " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
            " |      6\n",
            " |  \n",
            " |  mapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], ~U]) -> 'RDD[Tuple[K, U]]'\n",
            " |      Pass each value in the key-value pair RDD through a map function\n",
            " |      without changing the keys; this also retains the original RDD's\n",
            " |      partitioning.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
            " |      >>> def f(x): return len(x)\n",
            " |      >>> x.mapValues(f).collect()\n",
            " |      [('a', 3), ('b', 1)]\n",
            " |  \n",
            " |  max(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
            " |      Find the maximum item in this RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : function, optional\n",
            " |          A function used to generate key for comparing\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
            " |      >>> rdd.max()\n",
            " |      43.0\n",
            " |      >>> rdd.max(key=str)\n",
            " |      5.0\n",
            " |  \n",
            " |  mean(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the mean of this RDD's elements.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
            " |      2.0\n",
            " |  \n",
            " |  meanApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
            " |      Approximate operation to return the mean within a timeout\n",
            " |      or meet the confidence.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
            " |      >>> r = sum(range(1000)) / 1000.0\n",
            " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
            " |      True\n",
            " |  \n",
            " |  min(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
            " |      Find the minimum item in this RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : function, optional\n",
            " |          A function used to generate key for comparing\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
            " |      >>> rdd.min()\n",
            " |      2.0\n",
            " |      >>> rdd.min(key=str)\n",
            " |      10.0\n",
            " |  \n",
            " |  name(self) -> Optional[str]\n",
            " |      Return the name of this RDD.\n",
            " |  \n",
            " |  partitionBy(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int], partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f3e2ec3cca0>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
            " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
            " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
            " |      0\n",
            " |  \n",
            " |  persist(self: 'RDD[T]', storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(False, True, False, False, 1)) -> 'RDD[T]'\n",
            " |      Set this RDD's storage level to persist its values across operations\n",
            " |      after the first time it is computed. This can only be used to assign\n",
            " |      a new storage level if the RDD does not have a storage level set yet.\n",
            " |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
            " |      >>> rdd.persist().is_cached\n",
            " |      True\n",
            " |  \n",
            " |  pipe(self, command: str, env: Optional[Dict[str, str]] = None, checkCode: bool = False) -> 'RDD[str]'\n",
            " |      Return an RDD created by piping elements to a forked external process.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      command : str\n",
            " |          command to run.\n",
            " |      env : dict, optional\n",
            " |          environment variables to set.\n",
            " |      checkCode : bool, optional\n",
            " |          whether or not to check the return value of the shell command.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
            " |      ['1', '2', '', '3']\n",
            " |  \n",
            " |  randomSplit(self: 'RDD[T]', weights: Sequence[Union[int, float]], seed: Optional[int] = None) -> 'List[RDD[T]]'\n",
            " |      Randomly splits this RDD with the provided weights.\n",
            " |      \n",
            " |      weights : list\n",
            " |          weights for splits, will be normalized if they don't sum to 1\n",
            " |      seed : int, optional\n",
            " |          random seed\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          split RDDs in a list\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(500), 1)\n",
            " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
            " |      >>> len(rdd1.collect() + rdd2.collect())\n",
            " |      500\n",
            " |      >>> 150 < rdd1.count() < 250\n",
            " |      True\n",
            " |      >>> 250 < rdd2.count() < 350\n",
            " |      True\n",
            " |  \n",
            " |  reduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T]) -> ~T\n",
            " |      Reduces the elements of this RDD using the specified commutative and\n",
            " |      associative binary operator. Currently reduces partitions locally.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
            " |      15\n",
            " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
            " |      10\n",
            " |      >>> sc.parallelize([]).reduce(add)\n",
            " |      Traceback (most recent call last):\n",
            " |          ...\n",
            " |      ValueError: Can not reduce() empty RDD\n",
            " |  \n",
            " |  reduceByKey(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f3e2ec3cca0>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Merge the values for each key using an associative and commutative reduce function.\n",
            " |      \n",
            " |      This will also perform the merging locally on each mapper before\n",
            " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
            " |      \n",
            " |      Output will be partitioned with `numPartitions` partitions, or\n",
            " |      the default parallelism level if `numPartitions` is not specified.\n",
            " |      Default partitioner is hash-partition.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  reduceByKeyLocally(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V]) -> Dict[~K, ~V]\n",
            " |      Merge the values for each key using an associative and commutative reduce function, but\n",
            " |      return the results immediately to the master as a dictionary.\n",
            " |      \n",
            " |      This will also perform the merging locally on each mapper before\n",
            " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  repartition(self: 'RDD[T]', numPartitions: int) -> 'RDD[T]'\n",
            " |       Return a new RDD that has exactly numPartitions partitions.\n",
            " |      \n",
            " |       Can increase or decrease the level of parallelism in this RDD.\n",
            " |       Internally, this uses a shuffle to redistribute data.\n",
            " |       If you are decreasing the number of partitions in this RDD, consider\n",
            " |       using `coalesce`, which can avoid performing a shuffle.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
            " |       >>> sorted(rdd.glom().collect())\n",
            " |       [[1], [2, 3], [4, 5], [6, 7]]\n",
            " |       >>> len(rdd.repartition(2).glom().collect())\n",
            " |       2\n",
            " |       >>> len(rdd.repartition(10).glom().collect())\n",
            " |       10\n",
            " |  \n",
            " |  repartitionAndSortWithinPartitions(self: 'RDD[Tuple[Any, Any]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[Any], int] = <function portable_hash at 0x7f3e2ec3cca0>, ascending: bool = True, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f3e2eb11040>) -> 'RDD[Tuple[Any, Any]]'\n",
            " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
            " |      sort records by their keys.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
            " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
            " |      >>> rdd2.glom().collect()\n",
            " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
            " |  \n",
            " |  rightOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], U]]]'\n",
            " |      Perform a right outer join of `self` and `other`.\n",
            " |      \n",
            " |      For each element (k, w) in `other`, the resulting RDD will either\n",
            " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
            " |      if no elements in `self` have key k.\n",
            " |      \n",
            " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
            " |      [('a', (2, 1)), ('b', (None, 4))]\n",
            " |  \n",
            " |  sample(self: 'RDD[T]', withReplacement: bool, fraction: float, seed: Optional[int] = None) -> 'RDD[T]'\n",
            " |      Return a sampled subset of this RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      withReplacement : bool\n",
            " |          can elements be sampled multiple times (replaced when sampled out)\n",
            " |      fraction : float\n",
            " |          expected size of the sample as a fraction of this RDD's size\n",
            " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
            " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
            " |      seed : int, optional\n",
            " |          seed for the random number generator\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
            " |      count of the given :class:`DataFrame`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(100), 4)\n",
            " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
            " |      True\n",
            " |  \n",
            " |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
            " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
            " |      Create a sample of this RDD using variable sampling rates for\n",
            " |      different keys as specified by fractions, a key to sampling rate map.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
            " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
            " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
            " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
            " |      True\n",
            " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
            " |      True\n",
            " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
            " |      True\n",
            " |  \n",
            " |  sampleStdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the sample standard deviation of this RDD's elements (which\n",
            " |      corrects for bias in estimating the standard deviation by dividing by\n",
            " |      N-1 instead of N).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
            " |      1.0\n",
            " |  \n",
            " |  sampleVariance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the sample variance of this RDD's elements (which corrects\n",
            " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
            " |      1.0\n",
            " |  \n",
            " |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
            " |      converted for output using either user specified converters or, by default,\n",
            " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      conf : dict\n",
            " |          Hadoop job configuration\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |  \n",
            " |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, compressionCodecClass: Optional[str] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
            " |      will be inferred if not specified. Keys and values are converted for output using either\n",
            " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
            " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
            " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to Hadoop file\n",
            " |      outputFormatClass : str\n",
            " |          fully qualified classname of Hadoop OutputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
            " |      keyClass : str, optional\n",
            " |          fully qualified classname of key Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
            " |      valueClass : str, optional\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |      conf : dict, optional\n",
            " |          (None by default)\n",
            " |      compressionCodecClass : str\n",
            " |          fully qualified classname of the compression codec class\n",
            " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
            " |  \n",
            " |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
            " |      converted for output using either user specified converters or, by default,\n",
            " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      conf : dict\n",
            " |          Hadoop job configuration\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |  \n",
            " |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
            " |      will be inferred if not specified. Keys and values are converted for output using either\n",
            " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
            " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
            " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
            " |      \n",
            " |      path : str\n",
            " |          path to Hadoop file\n",
            " |      outputFormatClass : str\n",
            " |          fully qualified classname of Hadoop OutputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
            " |      keyClass : str, optional\n",
            " |          fully qualified classname of key Writable class\n",
            " |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
            " |      valueClass : str, optional\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |      conf : dict, optional\n",
            " |          Hadoop job configuration (None by default)\n",
            " |  \n",
            " |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n",
            " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
            " |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n",
            " |      is 10.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
            " |      >>> tmpFile.close()\n",
            " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
            " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
            " |      ['1', '2', 'rdd', 'spark']\n",
            " |  \n",
            " |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
            " |      RDD's key and value types. The mechanism is as follows:\n",
            " |      \n",
            " |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n",
            " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to sequence file\n",
            " |      compressionCodecClass : str, optional\n",
            " |          fully qualified classname of the compression codec class\n",
            " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
            " |  \n",
            " |  saveAsTextFile(self, path: str, compressionCodecClass: Optional[str] = None) -> None\n",
            " |      Save this RDD as a text file, using string representations of elements.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to text file\n",
            " |      compressionCodecClass : str, optional\n",
            " |          fully qualified classname of the compression codec class\n",
            " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
            " |      >>> tempFile.close()\n",
            " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
            " |      >>> from fileinput import input\n",
            " |      >>> from glob import glob\n",
            " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
            " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
            " |      \n",
            " |      Empty lines are tolerated when saving to text files.\n",
            " |      \n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
            " |      >>> tempFile2.close()\n",
            " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
            " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
            " |      '\\n\\n\\nbar\\nfoo\\n'\n",
            " |      \n",
            " |      Using compressionCodecClass\n",
            " |      \n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
            " |      >>> tempFile3.close()\n",
            " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
            " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
            " |      >>> from fileinput import input, hook_compressed\n",
            " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
            " |      >>> ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n",
            " |      'bar\\nfoo\\n'\n",
            " |  \n",
            " |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n",
            " |      Assign a name to this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd1 = sc.parallelize([1, 2])\n",
            " |      >>> rdd1.setName('RDD1').name()\n",
            " |      'RDD1'\n",
            " |  \n",
            " |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
            " |      Sorts this RDD by the given keyfunc\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
            " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
            " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
            " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
            " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
            " |  \n",
            " |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f3e2eb11160>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
            " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
            " |      ('1', 3)\n",
            " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
            " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
            " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
            " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
            " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
            " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
            " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
            " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
            " |  \n",
            " |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n",
            " |      Return a :class:`StatCounter` object that captures the mean, variance\n",
            " |      and count of the RDD's elements in one operation.\n",
            " |  \n",
            " |  stdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the standard deviation of this RDD's elements.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
            " |      0.816...\n",
            " |  \n",
            " |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
            " |      Return each value in `self` that is not contained in `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
            " |      >>> sorted(x.subtract(y).collect())\n",
            " |      [('a', 1), ('b', 4), ('b', 5)]\n",
            " |  \n",
            " |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
            " |      Return each (key, value) pair in `self` that has no pair with matching\n",
            " |      key in `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
            " |      >>> sorted(x.subtractByKey(y).collect())\n",
            " |      [('b', 4), ('b', 5)]\n",
            " |  \n",
            " |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Add up the elements in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
            " |      6.0\n",
            " |  \n",
            " |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
            " |      Approximate operation to return the sum within a timeout\n",
            " |      or meet the confidence.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
            " |      >>> r = sum(range(1000))\n",
            " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
            " |      True\n",
            " |  \n",
            " |  take(self: 'RDD[T]', num: int) -> List[~T]\n",
            " |      Take the first num elements of the RDD.\n",
            " |      \n",
            " |      It works by first scanning one partition, and use the results from\n",
            " |      that partition to estimate the number of additional partitions needed\n",
            " |      to satisfy the limit.\n",
            " |      \n",
            " |      Translated from the Scala implementation in RDD#take().\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
            " |      [2, 3]\n",
            " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
            " |      [2, 3, 4, 5, 6]\n",
            " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
            " |      [91, 92, 93]\n",
            " |  \n",
            " |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
            " |      Get the N elements from an RDD ordered in ascending order or as\n",
            " |      specified by the optional key function.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
            " |      [1, 2, 3, 4, 5, 6]\n",
            " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
            " |      [10, 9, 7, 6, 5, 4]\n",
            " |  \n",
            " |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n",
            " |      Return a fixed-size sampled subset of this RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(0, 10))\n",
            " |      >>> len(rdd.takeSample(True, 20, 1))\n",
            " |      20\n",
            " |      >>> len(rdd.takeSample(False, 5, 2))\n",
            " |      5\n",
            " |      >>> len(rdd.takeSample(False, 15, 3))\n",
            " |      10\n",
            " |  \n",
            " |  toDF(self: 'RDD[Any]', schema: Optional[Any] = None, sampleRatio: Optional[float] = None) -> 'DataFrame'\n",
            " |  \n",
            " |  toDebugString(self) -> Optional[bytes]\n",
            " |      A description of this RDD and its recursive dependencies for debugging.\n",
            " |  \n",
            " |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n",
            " |      Return an iterator that contains all of the elements in this RDD.\n",
            " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
            " |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      prefetchPartitions : bool, optional\n",
            " |          If Spark should pre-fetch the next partition\n",
            " |          before it is needed.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(10))\n",
            " |      >>> [x for x in rdd.toLocalIterator()]\n",
            " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            " |  \n",
            " |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
            " |      Get the top N elements from an RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      It returns the list sorted in descending order.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
            " |      [12]\n",
            " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
            " |      [6, 5]\n",
            " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
            " |      [4, 3, 2]\n",
            " |  \n",
            " |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n",
            " |      Aggregates the elements of this RDD in a multi-level tree\n",
            " |      pattern.\n",
            " |      \n",
            " |      depth : int, optional\n",
            " |          suggested depth of the tree (default: 2)\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> add = lambda x, y: x + y\n",
            " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
            " |      >>> rdd.treeAggregate(0, add, add)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
            " |      -5\n",
            " |  \n",
            " |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n",
            " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      f : function\n",
            " |      depth : int, optional\n",
            " |          suggested depth of the tree (default: 2)\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> add = lambda x, y: x + y\n",
            " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
            " |      >>> rdd.treeReduce(add)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 1)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 2)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 5)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 10)\n",
            " |      -5\n",
            " |  \n",
            " |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
            " |      Return the union of this RDD and another one.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
            " |      >>> rdd.union(rdd).collect()\n",
            " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
            " |  \n",
            " |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n",
            " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
            " |      memory and disk.\n",
            " |      \n",
            " |      .. versionchanged:: 3.0.0\n",
            " |         Added optional argument `blocking` to specify whether to block until all\n",
            " |         blocks are deleted.\n",
            " |  \n",
            " |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n",
            " |      Return an RDD with the values of each tuple.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
            " |      >>> m.collect()\n",
            " |      [2, 4]\n",
            " |  \n",
            " |  variance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the variance of this RDD's elements.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
            " |      0.666...\n",
            " |  \n",
            " |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n",
            " |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n",
            " |      This is only supported on certain cluster managers and currently requires dynamic\n",
            " |      allocation to be enabled. It will result in new executors with the resources specified\n",
            " |      being acquired to calculate the RDD.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental\n",
            " |  \n",
            " |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
            " |      Zips this RDD with another one, returning key-value pairs with the\n",
            " |      first element in each RDD second element in each RDD, etc. Assumes\n",
            " |      that the two RDDs have the same number of partitions and the same\n",
            " |      number of elements in each partition (e.g. one was made through\n",
            " |      a map on the other).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize(range(0,5))\n",
            " |      >>> y = sc.parallelize(range(1000, 1005))\n",
            " |      >>> x.zip(y).collect()\n",
            " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
            " |  \n",
            " |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
            " |      Zips this RDD with its element indices.\n",
            " |      \n",
            " |      The ordering is first based on the partition index and then the\n",
            " |      ordering of items within each partition. So the first item in\n",
            " |      the first partition gets index 0, and the last item in the last\n",
            " |      partition receives the largest index.\n",
            " |      \n",
            " |      This method needs to trigger a spark job when this RDD contains\n",
            " |      more than one partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
            " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
            " |  \n",
            " |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
            " |      Zips this RDD with generated unique Long ids.\n",
            " |      \n",
            " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
            " |      n is the number of partitions. So there may exist gaps, but this\n",
            " |      method won't trigger a spark job, which is different from\n",
            " |      :meth:`zipWithIndex`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
            " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  context\n",
            " |      The :class:`SparkContext` that this RDD was created on.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  __parameters__ = (+T_co,)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD Actions "
      ],
      "metadata": {
        "id": "sygejvlpaRWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(rdd2.collect())             #collect (convert RDD to in-memory list)\n",
        "print(rdd2.take(5))               #take() (prints first elements)\n",
        "print(rdd2.top(5))                #top( prints top elements)\n",
        "print(rdd2.takeSample(False,4))   #takeSample (take some sample random values from list, if it's true it will repeat same value again ,False means unique)       "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laMARQsGbEuB",
        "outputId": "27164a8f-1f35-428c-9996-5cf66917e3c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('s', 1), ('a', 2), ('n', 3), ('d', 4), ('e', 5), ('e', 6), ('p', 7)]\n",
            "[('s', 1), ('a', 2), ('n', 3), ('d', 4), ('e', 5)]\n",
            "[('s', 1), ('p', 7), ('n', 3), ('e', 6), ('e', 5)]\n",
            "[('e', 6), ('s', 1), ('e', 5), ('a', 2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Action   - Aggregate functions - gives single output value\n",
        "# Action   - min,max,sum(),mean(),stdev\n",
        "# Action   - count( no.of elements)\n",
        "\n",
        "print(rdd.count())\n",
        "print(rdd.min())\n",
        "print(rdd.max())\n",
        "print(rdd.sum())\n",
        "print(rdd.mean())\n",
        "print(rdd.stdev())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVWM8NJndm9W",
        "outputId": "45f432ae-7d6b-40b9-a1a2-fe87303a0973"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "0\n",
            "18\n",
            "90\n",
            "9.0\n",
            "5.744562646538029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - stats- complete info about count,mean,stdev,max,min\n",
        "rdd.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmQ_Ta4Z6K55",
        "outputId": "c6d9e8c4-2b8d-4ef8-de7e-b34ba4059d72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(count: 10, mean: 9.0, stdev: 5.744562646538029, max: 18.0, min: 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - Aggregate functions - gives single output value\n",
        "# Actions- reduce that aggregates a data set(RDD) element using function\n",
        "\n",
        "print(rdd.reduce(lambda x,y : x+y))\n",
        "\n",
        "print(rdd.reduce(lambda x,y : x*y))\n",
        "\n",
        "print(rdd.reduce(lambda x,y : x-y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlSk6Dz5hWJh",
        "outputId": "98cf188d-c09f-45f6-e3ea-47b6299aee72"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90\n",
            "0\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - CountByValue - count of same values\n",
        "\n",
        "rdd2.countByValue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS8Brnesh-Ij",
        "outputId": "35530bda-b636-4b3b-e8f3-3e4855428175"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('s', 1): 1,\n",
              "             ('a', 2): 1,\n",
              "             ('n', 3): 1,\n",
              "             ('d', 4): 1,\n",
              "             ('e', 5): 1,\n",
              "             ('e', 6): 1,\n",
              "             ('p', 7): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - CountByValue - count of same values\n",
        "rdd2.countByKey()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbpbCI1ktKD6",
        "outputId": "ea443132-aecc-4c15-a5a9-2fd3aa8a4156"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'s': 1, 'a': 1, 'n': 1, 'd': 1, 'e': 2, 'p': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions \n",
        "from operator import *\n",
        "\n",
        "rdd.fold(1,add)  #fold - aggregate the elements of each partition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at7I4hjetaah",
        "outputId": "7a6ca8a0-595e-4cfd-915b-c90f19c75d2b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - \n",
        "rdd.variance()         #variance (all n values variance)\n",
        "rdd.sampleVariance()   #sample variance - (n-1) values variance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XzAJJiex4hy",
        "outputId": "18ca7b4f-669c-4d44-aad8-eb40e97b68a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.666666666666664"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "F = sc.textFile(\"/content/sample_data/california_housing_test.csv\") # Read file csv file using RDD\n",
        "F.top(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOJyjDZu1tHt",
        "outputId": "9d3b4158-c646-4e0c-af1a-f4de081cd57e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-124.180000,40.790000,40.000000,1398.000000,311.000000,788.000000,279.000000,1.466800,64600.000000',\n",
              " '-124.170000,41.800000,16.000000,2739.000000,480.000000,1259.000000,436.000000,3.755700,109400.000000',\n",
              " '-124.160000,41.920000,19.000000,1668.000000,324.000000,841.000000,283.000000,2.133600,75000.000000',\n",
              " '-124.160000,40.800000,52.000000,2167.000000,480.000000,908.000000,451.000000,1.611100,74700.000000',\n",
              " '-124.160000,40.790000,46.000000,3042.000000,597.000000,1206.000000,541.000000,2.113500,90600.000000',\n",
              " '-124.160000,40.770000,35.000000,2141.000000,438.000000,1053.000000,434.000000,2.852900,85600.000000',\n",
              " '-124.150000,40.780000,41.000000,2127.000000,358.000000,911.000000,349.000000,3.171100,104200.000000',\n",
              " '-124.140000,40.800000,32.000000,1373.000000,312.000000,872.000000,306.000000,2.500000,72600.000000',\n",
              " '-124.140000,40.720000,18.000000,2581.000000,499.000000,1375.000000,503.000000,2.844600,100500.000000',\n",
              " '-124.140000,40.600000,27.000000,1148.000000,206.000000,521.000000,219.000000,4.025000,128100.000000']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nRHaBFY0u6j",
        "outputId": "3c8092fe-58f2-4572-e86c-63f6c0866c31"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - \n",
        "\n",
        "csv = F.coalesce(3).saveAsTextFile('/content/sample_data/csv1')         #saveAsTextFile   -- text file format\n",
        "\n",
        "pickle = F.coalesce(2).saveAsPickleFile('/content/sample_data/pickle1')    #saveAsPickleFile -- binary file format"
      ],
      "metadata": {
        "id": "uODMWd8A1zu2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD transformations"
      ],
      "metadata": {
        "id": "FH8VBrqToCVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map - Return new distributed dataset formed by passing each element of source through a function \n",
        "\n",
        "x= rdd.map(lambda x : (x,x*3))\n",
        "\n",
        "print( 'Values:',rdd.collect())\n",
        "print('Values:',x.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufhcuSm1oM2O",
        "outputId": "33d36b5c-5000-4675-b289-7eb2056aae37"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "Values: [(0, 0), (2, 6), (4, 12), (6, 18), (8, 24), (10, 30), (12, 36), (14, 42), (16, 48), (18, 54)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatMap -Similar to map ,but each input iteam can be mapped to 0 or more output items\n",
        "\n",
        "y = rdd.flatMap(lambda x : (x, x*4,x/4,x**4))\n",
        "\n",
        "y.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_8biIZuuJi2",
        "outputId": "831227ed-f3bd-40af-ad48-7cbac9c6c883"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 0,\n",
              " 0.0,\n",
              " 0,\n",
              " 2,\n",
              " 8,\n",
              " 0.5,\n",
              " 16,\n",
              " 4,\n",
              " 16,\n",
              " 1.0,\n",
              " 256,\n",
              " 6,\n",
              " 24,\n",
              " 1.5,\n",
              " 1296,\n",
              " 8,\n",
              " 32,\n",
              " 2.0,\n",
              " 4096,\n",
              " 10,\n",
              " 40,\n",
              " 2.5,\n",
              " 10000,\n",
              " 12,\n",
              " 48,\n",
              " 3.0,\n",
              " 20736,\n",
              " 14,\n",
              " 56,\n",
              " 3.5,\n",
              " 38416,\n",
              " 16,\n",
              " 64,\n",
              " 4.0,\n",
              " 65536,\n",
              " 18,\n",
              " 72,\n",
              " 4.5,\n",
              " 104976]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter - Return a new dataset formed by selecting those elements of source\n",
        "\n",
        "z= rdd.filter( lambda x : x%2 == 1)\n",
        "\n",
        "z.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQRkhTXqpbaD",
        "outputId": "224c4921-3388-4425-de8c-4c9ef75aa1c1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Partition given list and print partition lists\n",
        "\n",
        "l = [1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
        "\n",
        "P = sc.parallelize(l,4)\n",
        "\n",
        "P.glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9dGNJwsoe_p",
        "outputId": "06d1fc55-6d96-494a-a4ad-fceda8ebc4bb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12, 13, 14]]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mapPartitions - Similar to map but runs separetely on each partition \n",
        "\n",
        "def x(iterator) : yield sum(iterator)\n",
        "\n",
        "mp = P.mapPartitions(x)\n",
        "\n",
        "\n",
        "mp.glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT1x-bX_JUqY",
        "outputId": "7b6768f3-0379-4fb0-9669-b5f704ddd0ff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[6], [15], [24], [60]]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mapPartitionWIthIndex - Similar to Mappartition ,but also provides an integer value is index of partition\n",
        "def x1(Index,iterator) : yield (Index,sum(iterator))\n",
        "\n",
        "mpi = P.mapPartitionsWithIndex(x1)\n",
        "\n",
        "mpi.glom().collect()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRtMSxvOu7Hq",
        "outputId": "2332ea86-c870-4902-a1b4-20d42fd89b8d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 6)], [(1, 15)], [(2, 24)], [(3, 60)]]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample - a fraction of fraction data ,with or without replacement ,using a given random number\n",
        "\n",
        "# sample(withreplacement.fraction,seed)  ,if (with replacement) - true - will repeat same value ,else (withreplacement) -False - unique values\n",
        "\n",
        "s = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "S=s.sample(True,1)\n",
        "\n",
        "print(s.collect())\n",
        "print(S.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvxxYfpTv1Fl",
        "outputId": "678d4daa-6896-409b-8a5d-a823bbd2947b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "[2, 4, 4, 6, 8, 9, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#union,intersection\n",
        "\n",
        "A = sc.parallelize(range(20))\n",
        "B = sc.parallelize(range(0,20,2))\n",
        "print(A.collect())\n",
        "print(B.collect())\n",
        "\n",
        "C = A.union(B)\n",
        "D = A.intersection(B)\n",
        "\n",
        "print(C.collect())\n",
        "print(D.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykAeVPp0yD37",
        "outputId": "4514f041-a4b5-4f95-a4fd-ab4fb4728b19"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "[0, 4, 8, 12, 16, 2, 6, 10, 14, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# distinct \n",
        "\n",
        "D = C.distinct()\n",
        "print(D.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kvdGBP2ywlE",
        "outputId": "06b5992f-4e18-42c0-9a94-cb5ff8eccb8c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 4, 8, 12, 16, 1, 5, 9, 13, 17, 2, 6, 10, 14, 18, 3, 7, 11, 15, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# groupByKey\n",
        "\n",
        "L = [('A',1),('A',2),('B',3),('B',4),('C',5),('C',6)]\n",
        "\n",
        "g = sc.parallelize(L)\n",
        "y= g.groupByKey()\n",
        "\n",
        "\n",
        "print([(j[0],[i for i in j[1]]) for j in y.collect()] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2CZ6RcL2G86",
        "outputId": "4f8133fe-d0fa-4312-ab16-607008f226f0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('C', [5, 6]), ('A', [1, 2]), ('B', [3, 4])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Caching \n",
        "\n"
      ],
      "metadata": {
        "id": "aCJm8ARjBOTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#caching -caching is  used to save the data(RDD/Dataframe/Dataset) in a cluster-wide in memory,\n",
        "#cache() method default saves data in MEMORY_ONLY.\n",
        "#Used to store small amount od data. This is Very Useful for accessing repeated data .\n",
        "#such as querying a small data set or when running an iterative algorithm\n",
        "\n",
        "rdd = sc.textFile('/content/sample_data/california_housing_test.csv')\n"
      ],
      "metadata": {
        "id": "OIZHLmxLBmJT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting total time taken for count before cache Enabled\n",
        "# print(t.timeit(number =5))\n",
        "\n",
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "rdd.count()\n",
        "rdd.min()\n",
        "rdd.max()\n",
        "rdd.collect()\n",
        "\n",
        "\n",
        "end = timeit.default_timer()\n",
        "\n",
        "print('elapsed time:',(end-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt1Vqt4jERfn",
        "outputId": "b3ce4080-1cc6-48af-8e9d-432b345bf70a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 0.31569416799999317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# caching rdd \n",
        "# cahe will save dataesets default in memory\n",
        "rdd.cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39PbEciVGQsC",
        "outputId": "198ddf6c-2e1e-4e54-d004-238449b5c7df"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/california_housing_test.csv MapPartitionsRDD[77] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Getting total time taken for count dfter cache Enabled\n",
        "# print(t.timeit(number =5))\n",
        "\n",
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "rdd.count()\n",
        "rdd.min()\n",
        "rdd.max()\n",
        "rdd.collect()\n",
        "\n",
        "\n",
        "end = timeit.default_timer()\n",
        "\n",
        "print('elapsed time:',(end-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Nqsb_gIQPa",
        "outputId": "34471ee8-ab0f-4930-eb99-a7056a449da3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 0.39976513199997044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# persist"
      ],
      "metadata": {
        "id": "hp8Y6NPEJyu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same like cahe ( stores data in memeory) but we can store large amout of  data \n",
        "# persist() method is used to store it to the user-defined storage levels like (memory only , disk only ,memory and disk only etc)\n",
        "\n",
        "rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
        " \n",
        "#rdd.persist(pyspark.StorageLevel.DISK_ONLY)\n",
        "\n",
        "#rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWYzccXYKFgS",
        "outputId": "74e06957-801f-4112-8137-387fe29069c7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/california_housing_test.csv MapPartitionsRDD[77] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clears cache and persist data manually\n",
        "rdd.unpersist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7iW9vcvK81F",
        "outputId": "971c24c6-7922-49b3-a467-771cc905bf4b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/california_housing_test.csv MapPartitionsRDD[77] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adventages of Cache and persist "
      ],
      "metadata": {
        "id": "4HxuWH7wDANA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cost efficient – Spark computations are very expensive hence reusing the computations are used to save cost.\n",
        "#Time efficient – Reusing the repeated computations saves lots of time.\n",
        "#Execution time – Saves execution time of the job and we can perform more jobs on the same cluster."
      ],
      "metadata": {
        "id": "GflEhc44DcOD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Broadcast variables"
      ],
      "metadata": {
        "id": "95UKMxw_Nypz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# allow programmer to keep a read only variable cached on each machine rather than shipping a copy of it with tasks\n",
        "# spark useses efficient broadcast algoritham to reduce communication cost\n",
        "\n",
        "Board = sc.broadcast([1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "print(Board.value)\n",
        "print(type(Board))\n"
      ],
      "metadata": {
        "id": "FoM2xsckOGu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f54b16-426c-4079-c338-e1598352912a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "<class 'pyspark.broadcast.Broadcast'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the boarcast variables by using unpersist or destroy\n",
        "\n",
        "#Board.unpersist()\n",
        "\n",
        "Board.destroy"
      ],
      "metadata": {
        "id": "cI6RlK10QDyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a99899e-22dc-4dfd-d526-0769481b0606"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Broadcast.destroy of <pyspark.broadcast.Broadcast object at 0x7f3e2e6fcaf0>>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accumulators"
      ],
      "metadata": {
        "id": "U_2N-cGxR0at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Accumulators are variables that are only \"added\" to through an associative and commutative operation and can therefore efficiently supported in parallel\n",
        "# they can be used to implement counters or sums .\n",
        "#spark natively supports accumulators of numeric types and programmers can add support for new types.\n",
        "from pyspark.accumulators import Accumulator\n",
        "\n",
        "a = sc.accumulator(100)\n",
        "\n",
        "\n",
        "accu = sc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x : ac.add(x))\n",
        "ac.value\n"
      ],
      "metadata": {
        "id": "-4Tw1NmSSRRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "753db6d2-d16a-4fc7-9fac-16c7180f7aaf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1090"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(accum)"
      ],
      "metadata": {
        "id": "0u39udawVZNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9471501-2d11-4395-c2c8-854ea3993b81"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on Accumulator in module pyspark.accumulators object:\n",
            "\n",
            "class Accumulator(typing.Generic)\n",
            " |  Accumulator(aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n",
            " |  \n",
            " |  A shared variable that can be accumulated, i.e., has a commutative and associative \"add\"\n",
            " |  operation. Worker tasks on a Spark cluster can add values to an Accumulator with the `+=`\n",
            " |  operator, but only the driver program is allowed to access its value, using `value`.\n",
            " |  Updates from the workers get propagated automatically to the driver program.\n",
            " |  \n",
            " |  While :class:`SparkContext` supports accumulators for primitive data types like :class:`int` and\n",
            " |  :class:`float`, users can also define accumulators for custom types by providing a custom\n",
            " |  :py:class:`AccumulatorParam` object. Refer to its doctest for an example.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> a = sc.accumulator(1)\n",
            " |  >>> a.value\n",
            " |  1\n",
            " |  >>> a.value = 2\n",
            " |  >>> a.value\n",
            " |  2\n",
            " |  >>> a += 5\n",
            " |  >>> a.value\n",
            " |  7\n",
            " |  >>> sc.accumulator(1.0).value\n",
            " |  1.0\n",
            " |  >>> sc.accumulator(1j).value\n",
            " |  1j\n",
            " |  >>> rdd = sc.parallelize([1,2,3])\n",
            " |  >>> def f(x):\n",
            " |  ...     global a\n",
            " |  ...     a += x\n",
            " |  >>> rdd.foreach(f)\n",
            " |  >>> a.value\n",
            " |  13\n",
            " |  >>> b = sc.accumulator(0)\n",
            " |  >>> def g(x):\n",
            " |  ...     b.add(x)\n",
            " |  >>> rdd.foreach(g)\n",
            " |  >>> b.value\n",
            " |  6\n",
            " |  \n",
            " |  >>> rdd.map(lambda x: a.value).collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |  Traceback (most recent call last):\n",
            " |      ...\n",
            " |  Py4JJavaError: ...\n",
            " |  \n",
            " |  >>> def h(x):\n",
            " |  ...     global a\n",
            " |  ...     a.value = 7\n",
            " |  >>> rdd.foreach(h) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |  Traceback (most recent call last):\n",
            " |      ...\n",
            " |  Py4JJavaError: ...\n",
            " |  \n",
            " |  >>> sc.accumulator([1.0, 2.0, 3.0]) # doctest: +IGNORE_EXCEPTION_DETAIL\n",
            " |  Traceback (most recent call last):\n",
            " |      ...\n",
            " |  TypeError: ...\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Accumulator\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __iadd__(self, term: ~T) -> 'Accumulator[T]'\n",
            " |      The += operator; adds a term to this accumulator's value\n",
            " |  \n",
            " |  __init__(self, aid: int, value: ~T, accum_param: 'AccumulatorParam[T]')\n",
            " |      Create a new Accumulator with a given initial value and AccumulatorParam object\n",
            " |  \n",
            " |  __reduce__(self) -> Tuple[Callable[[int, ~T, ForwardRef('AccumulatorParam[T]')], ForwardRef('Accumulator[T]')], Tuple[int, ~T, ForwardRef('AccumulatorParam[T]')]]\n",
            " |      Custom serialization; saves the zero value from our AccumulatorParam\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __str__(self) -> str\n",
            " |      Return str(self).\n",
            " |  \n",
            " |  add(self, term: ~T) -> None\n",
            " |      Adds a term to this accumulator's value\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  value\n",
            " |      Get the accumulator's value; only usable in driver program\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[~T],)\n",
            " |  \n",
            " |  __parameters__ = (~T,)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations  - Joins"
      ],
      "metadata": {
        "id": "EA2YRT5ULuz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#join - takes two pair of rdd ,return only matched records from both RDD's\n",
        "\n",
        "J1 = sc.parallelize([('A',2),('B',3),('C',4),('D',5),('E',6),('F',7)])\n",
        "\n",
        "J2 = sc.parallelize([('C',2),('D',3),('E',4),('F',5),('G',6),('H',7)])\n",
        "\n",
        "join = J1.join(J2)\n",
        "\n",
        "print(join.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl3dfn_xMkRB",
        "outputId": "422f8d33-a8ee-454f-90ca-2e966ca82c61"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('F', (7, 5)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# leftouterjoin  - returns matched records from both rdd and unmatched records from left rdd\n",
        "\n",
        "\n",
        "left = J1.leftOuterJoin(J2)\n",
        "\n",
        "print(left.collect())\n",
        "print(left.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9EHyuXfNRau",
        "outputId": "6eb8ee02-40b7-468b-b022-26ca53a1871a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', (2, None)), ('B', (3, None)), ('F', (7, 5)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4))]\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RightOuterJoin - returns matched records from both rdd and unmatched from right rdd\n",
        "\n",
        "right = J1.rightOuterJoin(J2)\n",
        "\n",
        "\n",
        "print(right.collect())\n",
        "print(right.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e71I6-k6PWxK",
        "outputId": "cae32bda-5bf6-47c3-cfd7-9e6ee8155744"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('F', (7, 5)), ('H', (None, 7)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4)), ('G', (None, 6))]\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fullOuterJoin - returned all records from both RDD'S\n",
        "\n",
        "full = J1.fullOuterJoin(J2)\n",
        "\n",
        "print(full.collect())\n",
        "\n",
        "print(full.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qwMF9V4PmAt",
        "outputId": "e7ed042d-9443-49d7-a224-7a8e05ef3308"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', (2, None)), ('B', (3, None)), ('F', (7, 5)), ('H', (None, 7)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4)), ('G', (None, 6))]\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cartesian join - cross product of both elements of RDD\n",
        "\n",
        "cert = J1.cartesian(J2)\n",
        "\n",
        "print(cert.collect())\n",
        "\n",
        "print(cert.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9tAmVPLQxAa",
        "outputId": "bf7235eb-0169-4363-eed4-56110525eb41"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('A', 2), ('C', 2)), (('A', 2), ('D', 3)), (('A', 2), ('E', 4)), (('B', 3), ('C', 2)), (('B', 3), ('D', 3)), (('B', 3), ('E', 4)), (('C', 4), ('C', 2)), (('C', 4), ('D', 3)), (('C', 4), ('E', 4)), (('A', 2), ('F', 5)), (('A', 2), ('G', 6)), (('A', 2), ('H', 7)), (('B', 3), ('F', 5)), (('B', 3), ('G', 6)), (('B', 3), ('H', 7)), (('C', 4), ('F', 5)), (('C', 4), ('G', 6)), (('C', 4), ('H', 7)), (('D', 5), ('C', 2)), (('D', 5), ('D', 3)), (('D', 5), ('E', 4)), (('E', 6), ('C', 2)), (('E', 6), ('D', 3)), (('E', 6), ('E', 4)), (('F', 7), ('C', 2)), (('F', 7), ('D', 3)), (('F', 7), ('E', 4)), (('D', 5), ('F', 5)), (('D', 5), ('G', 6)), (('D', 5), ('H', 7)), (('E', 6), ('F', 5)), (('E', 6), ('G', 6)), (('E', 6), ('H', 7)), (('F', 7), ('F', 5)), (('F', 7), ('G', 6)), (('F', 7), ('H', 7))]\n",
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark-submit local"
      ],
      "metadata": {
        "id": "x3LSF65HMUEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**wc.py :**\n",
        "\n",
        "from pyspark.sql import sparkSession\n",
        "\n",
        "spark = sparkSession.builder.master('local').appName('wcout').getOrCreate()\n",
        "\n",
        "<<<<< logic here >>>>>>\n",
        "\n",
        "\n",
        "**syntax: spark-submit in local**\n",
        "\n",
        "spark-submit \"configrationoptions\" \"programfile_path\" file:input_path file:output_path\n",
        "\n",
        "EX:\n",
        "\n",
        "spark_submit /home/cloudera/wc.py file://home/cloudera/csvfile file://home/cloudera/wcout\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A3Aanoz6NZL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark-Submit Yarn cluster"
      ],
      "metadata": {
        "id": "1RFcRqrBTL8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wc.py:**\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('yarn').appName('wcout').getOrCreate()\n",
        "\n",
        "<<<<< logic here >>>>>>\n",
        "\n",
        "**syntax: spark-submit in yarn**\n",
        "\n",
        "spark-submit \"configrationoptions\" \"programfile_path\" file:input_path file:output_path\n",
        "\n",
        "spark-submit /home/cloudera/wc.py hdfs://localhost:8020/file1 hdfs://localhost:8020/wcout"
      ],
      "metadata": {
        "id": "x9fnClxtTS7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spark-submit without configuration values :"
      ],
      "metadata": {
        "id": "IINtHsNlceIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ByDefault values are in Yarn cluster\n",
        "\n",
        "driver memory   :  1GB memory, \n",
        "executor memory :  1GB memory, \n",
        "number of cores per executor : 1 "
      ],
      "metadata": {
        "id": "VADqGEUDa3qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIFO/FAIR"
      ],
      "metadata": {
        "id": "yaS5w6_Wd_Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Spark support two Schedulers:\n",
        "\n",
        "1.FIFO : (First in First out ) only one application at a time.second application under waiting state until first one completes.\n",
        "\n",
        "2.FAIR : In order to allow multiple apps to run on cluster the applications must be submitted to Fair scheduler\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e74FQ1Ssen4_"
      }
    }
  ]
}