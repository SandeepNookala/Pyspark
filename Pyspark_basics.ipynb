{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "baxEy4c7QtUo",
        "DGSahIzuR9aW",
        "VJ6L9nE7SR7w",
        "mnSqPh0uXthA",
        "sygejvlpaRWY",
        "FH8VBrqToCVn",
        "aCJm8ARjBOTy",
        "hp8Y6NPEJyu3",
        "4HxuWH7wDANA",
        "95UKMxw_Nypz",
        "U_2N-cGxR0at",
        "EA2YRT5ULuz7",
        "x3LSF65HMUEf",
        "1RFcRqrBTL8j",
        "IINtHsNlceIi",
        "yaS5w6_Wd_Kv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pyspark in Googlecolab"
      ],
      "metadata": {
        "id": "baxEy4c7QtUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1Hf2vCSQxYW",
        "outputId": "95571744-dfb7-43cb-fbe6-aee547fa1382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.3.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.8/dist-packages (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating new SparkContext"
      ],
      "metadata": {
        "id": "DGSahIzuR9aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "l0nWrdXoRFwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-09CKdQR2Tz",
        "outputId": "dbdf45e0-54c1-4453-b275-5d52b4f133f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=pyspark-shell>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://84cd8b74837f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating RDD from list"
      ],
      "metadata": {
        "id": "VJ6L9nE7SR7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize([1,2,3,4,5,6])"
      ],
      "metadata": {
        "id": "PZpRlPdgR0TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU35HxZZWV5F",
        "outputId": "ce807925-cd72-4554-babf-6a1ae56a89ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN_6jvMZVITS",
        "outputId": "add89185-6202-4a52-aa2d-dc6b5c20374b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.max()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wrfSl32VqIO",
        "outputId": "f3214a71-ec2a-4204-fa7a-dd8061899d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = sc.parallelize(['A','A','B','B','C'])"
      ],
      "metadata": {
        "id": "QK5Uz1WWVqyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VR1C2SbWMMD",
        "outputId": "ea34b921-df2c-4ced-e646-d8f02547c591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'A', 'B', 'B', 'C']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWlPcBa2WpF3",
        "outputId": "62908236-78ea-4a27-b040-70fd1ebeb65f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to know methods on rdd\n",
        "dir(x)            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp3DbOx-W45r",
        "outputId": "4b83d7c0-d391-4f17-92a5-3e7a2ec2cdce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__add__',\n",
              " '__class__',\n",
              " '__class_getitem__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getnewargs__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__orig_bases__',\n",
              " '__parameters__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__slots__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_computeFractionForSampleSize',\n",
              " '_defaultReducePartitions',\n",
              " '_id',\n",
              " '_is_barrier',\n",
              " '_is_protocol',\n",
              " '_jrdd',\n",
              " '_jrdd_deserializer',\n",
              " '_memory_limit',\n",
              " '_pickled',\n",
              " '_reserialize',\n",
              " '_to_java_object_rdd',\n",
              " 'aggregate',\n",
              " 'aggregateByKey',\n",
              " 'barrier',\n",
              " 'cache',\n",
              " 'cartesian',\n",
              " 'checkpoint',\n",
              " 'cleanShuffleDependencies',\n",
              " 'coalesce',\n",
              " 'cogroup',\n",
              " 'collect',\n",
              " 'collectAsMap',\n",
              " 'collectWithJobGroup',\n",
              " 'combineByKey',\n",
              " 'context',\n",
              " 'count',\n",
              " 'countApprox',\n",
              " 'countApproxDistinct',\n",
              " 'countByKey',\n",
              " 'countByValue',\n",
              " 'ctx',\n",
              " 'distinct',\n",
              " 'filter',\n",
              " 'first',\n",
              " 'flatMap',\n",
              " 'flatMapValues',\n",
              " 'fold',\n",
              " 'foldByKey',\n",
              " 'foreach',\n",
              " 'foreachPartition',\n",
              " 'fullOuterJoin',\n",
              " 'getCheckpointFile',\n",
              " 'getNumPartitions',\n",
              " 'getResourceProfile',\n",
              " 'getStorageLevel',\n",
              " 'glom',\n",
              " 'groupBy',\n",
              " 'groupByKey',\n",
              " 'groupWith',\n",
              " 'has_resource_profile',\n",
              " 'histogram',\n",
              " 'id',\n",
              " 'intersection',\n",
              " 'isCheckpointed',\n",
              " 'isEmpty',\n",
              " 'isLocallyCheckpointed',\n",
              " 'is_cached',\n",
              " 'is_checkpointed',\n",
              " 'join',\n",
              " 'keyBy',\n",
              " 'keys',\n",
              " 'leftOuterJoin',\n",
              " 'localCheckpoint',\n",
              " 'lookup',\n",
              " 'map',\n",
              " 'mapPartitions',\n",
              " 'mapPartitionsWithIndex',\n",
              " 'mapPartitionsWithSplit',\n",
              " 'mapValues',\n",
              " 'max',\n",
              " 'mean',\n",
              " 'meanApprox',\n",
              " 'min',\n",
              " 'name',\n",
              " 'partitionBy',\n",
              " 'partitioner',\n",
              " 'persist',\n",
              " 'pipe',\n",
              " 'randomSplit',\n",
              " 'reduce',\n",
              " 'reduceByKey',\n",
              " 'reduceByKeyLocally',\n",
              " 'repartition',\n",
              " 'repartitionAndSortWithinPartitions',\n",
              " 'rightOuterJoin',\n",
              " 'sample',\n",
              " 'sampleByKey',\n",
              " 'sampleStdev',\n",
              " 'sampleVariance',\n",
              " 'saveAsHadoopDataset',\n",
              " 'saveAsHadoopFile',\n",
              " 'saveAsNewAPIHadoopDataset',\n",
              " 'saveAsNewAPIHadoopFile',\n",
              " 'saveAsPickleFile',\n",
              " 'saveAsSequenceFile',\n",
              " 'saveAsTextFile',\n",
              " 'setName',\n",
              " 'sortBy',\n",
              " 'sortByKey',\n",
              " 'stats',\n",
              " 'stdev',\n",
              " 'subtract',\n",
              " 'subtractByKey',\n",
              " 'sum',\n",
              " 'sumApprox',\n",
              " 'take',\n",
              " 'takeOrdered',\n",
              " 'takeSample',\n",
              " 'toDF',\n",
              " 'toDebugString',\n",
              " 'toLocalIterator',\n",
              " 'top',\n",
              " 'treeAggregate',\n",
              " 'treeReduce',\n",
              " 'union',\n",
              " 'unpersist',\n",
              " 'values',\n",
              " 'variance',\n",
              " 'withResources',\n",
              " 'zip',\n",
              " 'zipWithIndex',\n",
              " 'zipWithUniqueId']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Help"
      ],
      "metadata": {
        "id": "mnSqPh0uXthA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(x)   #toknow exact syntax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjBEQJg2Xw12",
        "outputId": "82be9d7a-bcc0-49e0-99bd-50b50b5f147a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on RDD in module pyspark.rdd object:\n",
            "\n",
            "class RDD(typing.Generic)\n",
            " |  RDD(*args, **kwds)\n",
            " |  \n",
            " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
            " |  Represents an immutable, partitioned collection of elements that can be\n",
            " |  operated on in parallel.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      RDD\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
            " |      Return the union of this RDD and another one.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
            " |      >>> (rdd + rdd).collect()\n",
            " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
            " |  \n",
            " |  __getnewargs__(self) -> NoReturn\n",
            " |  \n",
            " |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __repr__(self) -> str\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n",
            " |      Aggregate the elements of each partition, and then the results for all\n",
            " |      the partitions, using a given combine functions and a neutral \"zero\n",
            " |      value.\"\n",
            " |      \n",
            " |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
            " |      as its result value to avoid object allocation; however, it should not\n",
            " |      modify ``t2``.\n",
            " |      \n",
            " |      The first function (seqOp) can return a different result type, U, than\n",
            " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
            " |      an U and one operation for merging two U\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
            " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
            " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
            " |      (10, 4)\n",
            " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
            " |      (0, 0)\n",
            " |  \n",
            " |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Union[int, NoneType] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7ff14f30c8b0>) -> 'RDD[Tuple[K, U]]'\n",
            " |      Aggregate the values of each key, using given combine functions and a neutral\n",
            " |      \"zero value\". This function can return a different result type, U, than the type\n",
            " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
            " |      a U and one operation for merging two U's, The former operation is used for merging\n",
            " |      values within a partition, and the latter is used for merging values between\n",
            " |      partitions. To avoid memory allocation, both of these functions are\n",
            " |      allowed to modify and return their first argument instead of creating a new U.\n",
            " |  \n",
            " |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n",
            " |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
            " |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
            " |      entire stage and relaunch all tasks for this stage.\n",
            " |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
            " |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
            " |      \n",
            " |      .. versionadded:: 2.4.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :class:`RDDBarrier`\n",
            " |          instance that provides actions within a barrier stage.\n",
            " |      \n",
            " |      See Also\n",
            " |      --------\n",
            " |      pyspark.BarrierTaskContext\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      For additional information see\n",
            " |      \n",
            " |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n",
            " |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n",
            " |      \n",
            " |      This API is experimental\n",
            " |  \n",
            " |  cache(self: 'RDD[T]') -> 'RDD[T]'\n",
            " |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
            " |  \n",
            " |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
            " |      Return the Cartesian product of this RDD and another one, that is, the\n",
            " |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
            " |      ``b`` is in `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2])\n",
            " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
            " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
            " |  \n",
            " |  checkpoint(self) -> None\n",
            " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
            " |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
            " |      all references to its parent RDDs will be removed. This function must\n",
            " |      be called before any job has been executed on this RDD. It is strongly\n",
            " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
            " |      on a file will require recomputation.\n",
            " |  \n",
            " |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n",
            " |      Removes an RDD's shuffles and it's non-persisted ancestors.\n",
            " |      \n",
            " |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n",
            " |      If you use the RDD after this call, you should checkpoint and materialize it first.\n",
            " |      \n",
            " |      .. versionadded:: 3.3.0\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      blocking : bool, optional\n",
            " |         block on shuffle cleanup tasks. Disabled by default.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is a developer API.\n",
            " |  \n",
            " |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n",
            " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
            " |      [[1], [2, 3], [4, 5]]\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
            " |      [[1, 2, 3, 4, 5]]\n",
            " |  \n",
            " |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Union[int, NoneType] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n",
            " |      For each key k in `self` or `other`, return a resulting RDD that\n",
            " |      contains a tuple with the list of values for that key in `self` as\n",
            " |      well as `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(x.cogroup(y).collect()))]\n",
            " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
            " |  \n",
            " |  collect(self: 'RDD[T]') -> List[~T]\n",
            " |      Return a list that contains all of the elements in this RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |  \n",
            " |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n",
            " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting data is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
            " |      >>> m[1]\n",
            " |      2\n",
            " |      >>> m[3]\n",
            " |      4\n",
            " |  \n",
            " |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n",
            " |      When collect rdd, use this method to specify job group.\n",
            " |      \n",
            " |      .. versionadded:: 3.0.0\n",
            " |      .. deprecated:: 3.1.0\n",
            " |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n",
            " |  \n",
            " |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Union[int, NoneType] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7ff14f30c8b0>) -> 'RDD[Tuple[K, U]]'\n",
            " |      Generic function to combine the elements for each key using a custom\n",
            " |      set of aggregation functions.\n",
            " |      \n",
            " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
            " |      type\" C.\n",
            " |      \n",
            " |      Users provide three functions:\n",
            " |      \n",
            " |          - `createCombiner`, which turns a V into a C (e.g., creates\n",
            " |            a one-element list)\n",
            " |          - `mergeValue`, to merge a V into a C (e.g., adds it to the end of\n",
            " |            a list)\n",
            " |          - `mergeCombiners`, to combine two C's into a single one (e.g., merges\n",
            " |            the lists)\n",
            " |      \n",
            " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
            " |      modify and return their first argument instead of creating a new C.\n",
            " |      \n",
            " |      In addition, users can control the partitioning of the output RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      V and C can be different -- for example, one might group an RDD of type\n",
            " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
            " |      >>> def to_list(a):\n",
            " |      ...     return [a]\n",
            " |      ...\n",
            " |      >>> def append(a, b):\n",
            " |      ...     a.append(b)\n",
            " |      ...     return a\n",
            " |      ...\n",
            " |      >>> def extend(a, b):\n",
            " |      ...     a.extend(b)\n",
            " |      ...     return a\n",
            " |      ...\n",
            " |      >>> sorted(x.combineByKey(to_list, append, extend).collect())\n",
            " |      [('a', [1, 2]), ('b', [1])]\n",
            " |  \n",
            " |  count(self) -> int\n",
            " |      Return the number of elements in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([2, 3, 4]).count()\n",
            " |      3\n",
            " |  \n",
            " |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n",
            " |      Approximate version of count() that returns a potentially incomplete\n",
            " |      result within a timeout, even if not all tasks have finished.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
            " |      >>> rdd.countApprox(1000, 1.0)\n",
            " |      1000\n",
            " |  \n",
            " |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n",
            " |      Return approximate number of distinct elements in the RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      relativeSD : float, optional\n",
            " |          Relative accuracy. Smaller values create\n",
            " |          counters that require more space.\n",
            " |          It must be greater than 0.000017.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The algorithm used is based on streamlib's implementation of\n",
            " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
            " |      of The Art Cardinality Estimation Algorithm\", available here\n",
            " |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
            " |      >>> 900 < n < 1100\n",
            " |      True\n",
            " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
            " |      >>> 16 < n < 24\n",
            " |      True\n",
            " |  \n",
            " |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n",
            " |      Count the number of elements for each key, and return the result to the\n",
            " |      master as a dictionary.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.countByKey().items())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n",
            " |      Return the count of each unique value in this RDD as a dictionary of\n",
            " |      (value, count) pairs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
            " |      [(1, 2), (2, 3)]\n",
            " |  \n",
            " |  distinct(self: 'RDD[T]', numPartitions: Union[int, NoneType] = None) -> 'RDD[T]'\n",
            " |      Return a new RDD containing the distinct elements in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
            " |      [1, 2, 3]\n",
            " |  \n",
            " |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n",
            " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
            " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
            " |      [2, 4]\n",
            " |  \n",
            " |  first(self: 'RDD[T]') -> ~T\n",
            " |      Return the first element in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([2, 3, 4]).first()\n",
            " |      2\n",
            " |      >>> sc.parallelize([]).first()\n",
            " |      Traceback (most recent call last):\n",
            " |          ...\n",
            " |      ValueError: RDD is empty\n",
            " |  \n",
            " |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by first applying a function to all elements of this\n",
            " |      RDD, and then flattening the results.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
            " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
            " |      [1, 1, 1, 2, 2, 3]\n",
            " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
            " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
            " |  \n",
            " |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n",
            " |      Pass each value in the key-value pair RDD through a flatMap function\n",
            " |      without changing the keys; this also retains the original RDD's\n",
            " |      partitioning.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
            " |      >>> def f(x): return x\n",
            " |      >>> x.flatMapValues(f).collect()\n",
            " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
            " |  \n",
            " |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n",
            " |      Aggregate the elements of each partition, and then the results for all\n",
            " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
            " |      \n",
            " |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
            " |      as its result value to avoid object allocation; however, it should not\n",
            " |      modify ``t2``.\n",
            " |      \n",
            " |      This behaves somewhat differently from fold operations implemented\n",
            " |      for non-distributed collections in functional languages like Scala.\n",
            " |      This fold operation may be applied to partitions individually, and then\n",
            " |      fold those results into the final result, rather than apply the fold\n",
            " |      to each element sequentially in some defined ordering. For functions\n",
            " |      that are not commutative, the result may differ from that of a fold\n",
            " |      applied to a non-distributed collection.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
            " |      15\n",
            " |  \n",
            " |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Union[int, NoneType] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7ff14f30c8b0>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Merge the values for each key using an associative function \"func\"\n",
            " |      and a neutral \"zeroValue\" which may be added to the result an\n",
            " |      arbitrary number of times, and must not change the result\n",
            " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> from operator import add\n",
            " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n",
            " |      Applies a function to all elements of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> def f(x): print(x)\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
            " |  \n",
            " |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n",
            " |      Applies a function to each partition of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> def f(iterator):\n",
            " |      ...     for x in iterator:\n",
            " |      ...          print(x)\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
            " |  \n",
            " |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Union[int, NoneType] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n",
            " |      Perform a right outer join of `self` and `other`.\n",
            " |      \n",
            " |      For each element (k, v) in `self`, the resulting RDD will either\n",
            " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
            " |      (k, (v, None)) if no elements in `other` have key k.\n",
            " |      \n",
            " |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
            " |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
            " |      (k, (None, w)) if no elements in `self` have key k.\n",
            " |      \n",
            " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
            " |      >>> sorted(x.fullOuterJoin(y).collect())\n",
            " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
            " |  \n",
            " |  getCheckpointFile(self) -> Union[str, NoneType]\n",
            " |      Gets the name of the file to which this RDD was checkpointed\n",
            " |      \n",
            " |      Not defined if RDD is checkpointed locally.\n",
            " |  \n",
            " |  getNumPartitions(self) -> int\n",
            " |      Returns the number of partitions in RDD\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
            " |      >>> rdd.getNumPartitions()\n",
            " |      2\n",
            " |  \n",
            " |  getResourceProfile(self) -> Union[pyspark.resource.profile.ResourceProfile, NoneType]\n",
            " |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n",
            " |      if it wasn't specified.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      :py:class:`pyspark.resource.ResourceProfile`\n",
            " |          The user specified profile or None if none were specified\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental\n",
            " |  \n",
            " |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n",
            " |      Get the RDD's current storage level.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd1 = sc.parallelize([1,2])\n",
            " |      >>> rdd1.getStorageLevel()\n",
            " |      StorageLevel(False, False, False, False, 1)\n",
            " |      >>> print(rdd1.getStorageLevel())\n",
            " |      Serialized 1x Replicated\n",
            " |  \n",
            " |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n",
            " |      Return an RDD created by coalescing all elements within each partition\n",
            " |      into a list.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
            " |      >>> sorted(rdd.glom().collect())\n",
            " |      [[1, 2], [3, 4]]\n",
            " |  \n",
            " |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Union[int, NoneType] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7ff14f30c8b0>) -> 'RDD[Tuple[K, Iterable[T]]]'\n",
            " |      Return an RDD of grouped items.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
            " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
            " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
            " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
            " |  \n",
            " |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Union[int, NoneType] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7ff14f30c8b0>) -> 'RDD[Tuple[K, Iterable[V]]]'\n",
            " |      Group the values for each key in the RDD into a single sequence.\n",
            " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If you are grouping in order to perform an aggregation (such as a\n",
            " |      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
            " |      provide much better performance.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
            " |      [('a', [1, 1]), ('b', [1])]\n",
            " |  \n",
            " |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n",
            " |      Alias for cogroup but with support for multiple RDDs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> z = sc.parallelize([(\"b\", 42)])\n",
            " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(w.groupWith(x, y, z).collect()))]\n",
            " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
            " |  \n",
            " |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n",
            " |      Compute a histogram using the provided buckets. The buckets\n",
            " |      are all open to the right except for the last which is closed.\n",
            " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
            " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
            " |      and 50 we would have a histogram of 1,0,1.\n",
            " |      \n",
            " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
            " |      this can be switched from an O(log n) insertion to O(1) per\n",
            " |      element (where n is the number of buckets).\n",
            " |      \n",
            " |      Buckets must be sorted, not contain any duplicates, and have\n",
            " |      at least two elements.\n",
            " |      \n",
            " |      If `buckets` is a number, it will generate buckets which are\n",
            " |      evenly spaced between the minimum and maximum of the RDD. For\n",
            " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
            " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
            " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
            " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
            " |      will be used.\n",
            " |      \n",
            " |      The return value is a tuple of buckets and histogram.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(51))\n",
            " |      >>> rdd.histogram(2)\n",
            " |      ([0, 25, 50], [25, 26])\n",
            " |      >>> rdd.histogram([0, 5, 25, 50])\n",
            " |      ([0, 5, 25, 50], [5, 20, 26])\n",
            " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
            " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
            " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
            " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
            " |      (('a', 'b', 'c'), [2, 2])\n",
            " |  \n",
            " |  id(self) -> int\n",
            " |      A unique ID for this RDD (within its SparkContext).\n",
            " |  \n",
            " |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n",
            " |      Return the intersection of this RDD and another one. The output will\n",
            " |      not contain any duplicate elements, even if the input RDDs did.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method performs a shuffle internally.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
            " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
            " |      >>> rdd1.intersection(rdd2).collect()\n",
            " |      [1, 2, 3]\n",
            " |  \n",
            " |  isCheckpointed(self) -> bool\n",
            " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
            " |  \n",
            " |  isEmpty(self) -> bool\n",
            " |      Returns true if and only if the RDD contains no elements at all.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      An RDD may be empty even when it has at least 1 partition.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([]).isEmpty()\n",
            " |      True\n",
            " |      >>> sc.parallelize([1]).isEmpty()\n",
            " |      False\n",
            " |  \n",
            " |  isLocallyCheckpointed(self) -> bool\n",
            " |      Return whether this RDD is marked for local checkpointing.\n",
            " |      \n",
            " |      Exposed for testing.\n",
            " |  \n",
            " |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Union[int, NoneType] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n",
            " |      Return an RDD containing all pairs of elements with matching keys in\n",
            " |      `self` and `other`.\n",
            " |      \n",
            " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
            " |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
            " |      \n",
            " |      Performs a hash join across the cluster.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
            " |      >>> sorted(x.join(y).collect())\n",
            " |      [('a', (1, 2)), ('a', (1, 3))]\n",
            " |  \n",
            " |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n",
            " |      Creates tuples of the elements in this RDD by applying `f`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
            " |      >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
            " |      >>> [(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]\n",
            " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
            " |  \n",
            " |  keys(self: 'RDD[Tuple[K, V]]') -> 'RDD[K]'\n",
            " |      Return an RDD with the keys of each tuple.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
            " |      >>> m.collect()\n",
            " |      [1, 3]\n",
            " |  \n",
            " |  leftOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Union[int, NoneType] = None) -> 'RDD[Tuple[K, Tuple[V, Optional[U]]]]'\n",
            " |      Perform a left outer join of `self` and `other`.\n",
            " |      \n",
            " |      For each element (k, v) in `self`, the resulting RDD will either\n",
            " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
            " |      (k, (v, None)) if no elements in `other` have key k.\n",
            " |      \n",
            " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> sorted(x.leftOuterJoin(y).collect())\n",
            " |      [('a', (1, 2)), ('b', (4, None))]\n",
            " |  \n",
            " |  localCheckpoint(self) -> None\n",
            " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
            " |      \n",
            " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
            " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
            " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
            " |      \n",
            " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
            " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
            " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
            " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
            " |      \n",
            " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
            " |      with their cached blocks. If you must use both features, you are advised to set\n",
            " |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
            " |      \n",
            " |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
            " |  \n",
            " |  lookup(self: 'RDD[Tuple[K, V]]', key: ~K) -> List[~V]\n",
            " |      Return the list of values in the RDD for key `key`. This operation\n",
            " |      is done efficiently if the RDD has a known partitioner by only\n",
            " |      searching the partition that the key maps to.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> l = range(1000)\n",
            " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
            " |      >>> rdd.lookup(42)  # slow\n",
            " |      [42]\n",
            " |      >>> sorted = rdd.sortByKey()\n",
            " |      >>> sorted.lookup(42)  # fast\n",
            " |      [42]\n",
            " |      >>> sorted.lookup(1024)\n",
            " |      []\n",
            " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
            " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
            " |      ['c']\n",
            " |  \n",
            " |  map(self: 'RDD[T]', f: Callable[[~T], ~U], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each element of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
            " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
            " |      [('a', 1), ('b', 1), ('c', 1)]\n",
            " |  \n",
            " |  mapPartitions(self: 'RDD[T]', f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each partition of this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
            " |      >>> def f(iterator): yield sum(iterator)\n",
            " |      >>> rdd.mapPartitions(f).collect()\n",
            " |      [3, 7]\n",
            " |  \n",
            " |  mapPartitionsWithIndex(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each partition of this RDD,\n",
            " |      while tracking the index of the original partition.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
            " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
            " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
            " |      6\n",
            " |  \n",
            " |  mapPartitionsWithSplit(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
            " |      Return a new RDD by applying a function to each partition of this RDD,\n",
            " |      while tracking the index of the original partition.\n",
            " |      \n",
            " |      .. deprecated:: 0.9.0\n",
            " |          use :py:meth:`RDD.mapPartitionsWithIndex` instead.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
            " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
            " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
            " |      6\n",
            " |  \n",
            " |  mapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], ~U]) -> 'RDD[Tuple[K, U]]'\n",
            " |      Pass each value in the key-value pair RDD through a map function\n",
            " |      without changing the keys; this also retains the original RDD's\n",
            " |      partitioning.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
            " |      >>> def f(x): return len(x)\n",
            " |      >>> x.mapValues(f).collect()\n",
            " |      [('a', 3), ('b', 1)]\n",
            " |  \n",
            " |  max(self: 'RDD[T]', key: Union[Callable[[~T], ForwardRef('S')], NoneType] = None) -> ~T\n",
            " |      Find the maximum item in this RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : function, optional\n",
            " |          A function used to generate key for comparing\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
            " |      >>> rdd.max()\n",
            " |      43.0\n",
            " |      >>> rdd.max(key=str)\n",
            " |      5.0\n",
            " |  \n",
            " |  mean(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the mean of this RDD's elements.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
            " |      2.0\n",
            " |  \n",
            " |  meanApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
            " |      Approximate operation to return the mean within a timeout\n",
            " |      or meet the confidence.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
            " |      >>> r = sum(range(1000)) / 1000.0\n",
            " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
            " |      True\n",
            " |  \n",
            " |  min(self: 'RDD[T]', key: Union[Callable[[~T], ForwardRef('S')], NoneType] = None) -> ~T\n",
            " |      Find the minimum item in this RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      key : function, optional\n",
            " |          A function used to generate key for comparing\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
            " |      >>> rdd.min()\n",
            " |      2.0\n",
            " |      >>> rdd.min(key=str)\n",
            " |      10.0\n",
            " |  \n",
            " |  name(self) -> Union[str, NoneType]\n",
            " |      Return the name of this RDD.\n",
            " |  \n",
            " |  partitionBy(self: 'RDD[Tuple[K, V]]', numPartitions: Union[int, NoneType], partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7ff14f30c8b0>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
            " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
            " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
            " |      0\n",
            " |  \n",
            " |  persist(self: 'RDD[T]', storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(False, True, False, False, 1)) -> 'RDD[T]'\n",
            " |      Set this RDD's storage level to persist its values across operations\n",
            " |      after the first time it is computed. This can only be used to assign\n",
            " |      a new storage level if the RDD does not have a storage level set yet.\n",
            " |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
            " |      >>> rdd.persist().is_cached\n",
            " |      True\n",
            " |  \n",
            " |  pipe(self, command: str, env: Union[Dict[str, str], NoneType] = None, checkCode: bool = False) -> 'RDD[str]'\n",
            " |      Return an RDD created by piping elements to a forked external process.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      command : str\n",
            " |          command to run.\n",
            " |      env : dict, optional\n",
            " |          environment variables to set.\n",
            " |      checkCode : bool, optional\n",
            " |          whether or not to check the return value of the shell command.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
            " |      ['1', '2', '', '3']\n",
            " |  \n",
            " |  randomSplit(self: 'RDD[T]', weights: Sequence[Union[int, float]], seed: Union[int, NoneType] = None) -> 'List[RDD[T]]'\n",
            " |      Randomly splits this RDD with the provided weights.\n",
            " |      \n",
            " |      weights : list\n",
            " |          weights for splits, will be normalized if they don't sum to 1\n",
            " |      seed : int, optional\n",
            " |          random seed\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      list\n",
            " |          split RDDs in a list\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(500), 1)\n",
            " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
            " |      >>> len(rdd1.collect() + rdd2.collect())\n",
            " |      500\n",
            " |      >>> 150 < rdd1.count() < 250\n",
            " |      True\n",
            " |      >>> 250 < rdd2.count() < 350\n",
            " |      True\n",
            " |  \n",
            " |  reduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T]) -> ~T\n",
            " |      Reduces the elements of this RDD using the specified commutative and\n",
            " |      associative binary operator. Currently reduces partitions locally.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
            " |      15\n",
            " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
            " |      10\n",
            " |      >>> sc.parallelize([]).reduce(add)\n",
            " |      Traceback (most recent call last):\n",
            " |          ...\n",
            " |      ValueError: Can not reduce() empty RDD\n",
            " |  \n",
            " |  reduceByKey(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V], numPartitions: Union[int, NoneType] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7ff14f30c8b0>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Merge the values for each key using an associative and commutative reduce function.\n",
            " |      \n",
            " |      This will also perform the merging locally on each mapper before\n",
            " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
            " |      \n",
            " |      Output will be partitioned with `numPartitions` partitions, or\n",
            " |      the default parallelism level if `numPartitions` is not specified.\n",
            " |      Default partitioner is hash-partition.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  reduceByKeyLocally(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V]) -> Dict[~K, ~V]\n",
            " |      Merge the values for each key using an associative and commutative reduce function, but\n",
            " |      return the results immediately to the master as a dictionary.\n",
            " |      \n",
            " |      This will also perform the merging locally on each mapper before\n",
            " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from operator import add\n",
            " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
            " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
            " |      [('a', 2), ('b', 1)]\n",
            " |  \n",
            " |  repartition(self: 'RDD[T]', numPartitions: int) -> 'RDD[T]'\n",
            " |       Return a new RDD that has exactly numPartitions partitions.\n",
            " |      \n",
            " |       Can increase or decrease the level of parallelism in this RDD.\n",
            " |       Internally, this uses a shuffle to redistribute data.\n",
            " |       If you are decreasing the number of partitions in this RDD, consider\n",
            " |       using `coalesce`, which can avoid performing a shuffle.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
            " |       >>> sorted(rdd.glom().collect())\n",
            " |       [[1], [2, 3], [4, 5], [6, 7]]\n",
            " |       >>> len(rdd.repartition(2).glom().collect())\n",
            " |       2\n",
            " |       >>> len(rdd.repartition(10).glom().collect())\n",
            " |       10\n",
            " |  \n",
            " |  repartitionAndSortWithinPartitions(self: 'RDD[Tuple[Any, Any]]', numPartitions: Union[int, NoneType] = None, partitionFunc: Callable[[Any], int] = <function portable_hash at 0x7ff14f30c8b0>, ascending: bool = True, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7ff14f23fd30>) -> 'RDD[Tuple[Any, Any]]'\n",
            " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
            " |      sort records by their keys.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
            " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
            " |      >>> rdd2.glom().collect()\n",
            " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
            " |  \n",
            " |  rightOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Union[int, NoneType] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], U]]]'\n",
            " |      Perform a right outer join of `self` and `other`.\n",
            " |      \n",
            " |      For each element (k, w) in `other`, the resulting RDD will either\n",
            " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
            " |      if no elements in `self` have key k.\n",
            " |      \n",
            " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 2)])\n",
            " |      >>> sorted(y.rightOuterJoin(x).collect())\n",
            " |      [('a', (2, 1)), ('b', (None, 4))]\n",
            " |  \n",
            " |  sample(self: 'RDD[T]', withReplacement: bool, fraction: float, seed: Union[int, NoneType] = None) -> 'RDD[T]'\n",
            " |      Return a sampled subset of this RDD.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      withReplacement : bool\n",
            " |          can elements be sampled multiple times (replaced when sampled out)\n",
            " |      fraction : float\n",
            " |          expected size of the sample as a fraction of this RDD's size\n",
            " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
            " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
            " |      seed : int, optional\n",
            " |          seed for the random number generator\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
            " |      count of the given :class:`DataFrame`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(100), 4)\n",
            " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
            " |      True\n",
            " |  \n",
            " |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Union[int, NoneType] = None) -> 'RDD[Tuple[K, V]]'\n",
            " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
            " |      Create a sample of this RDD using variable sampling rates for\n",
            " |      different keys as specified by fractions, a key to sampling rate map.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
            " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
            " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
            " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
            " |      True\n",
            " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
            " |      True\n",
            " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
            " |      True\n",
            " |  \n",
            " |  sampleStdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the sample standard deviation of this RDD's elements (which\n",
            " |      corrects for bias in estimating the standard deviation by dividing by\n",
            " |      N-1 instead of N).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
            " |      1.0\n",
            " |  \n",
            " |  sampleVariance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the sample variance of this RDD's elements (which corrects\n",
            " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
            " |      1.0\n",
            " |  \n",
            " |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Union[str, NoneType] = None, valueConverter: Union[str, NoneType] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
            " |      converted for output using either user specified converters or, by default,\n",
            " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      conf : dict\n",
            " |          Hadoop job configuration\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |  \n",
            " |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Union[str, NoneType] = None, valueClass: Union[str, NoneType] = None, keyConverter: Union[str, NoneType] = None, valueConverter: Union[str, NoneType] = None, conf: Union[Dict[str, str], NoneType] = None, compressionCodecClass: Union[str, NoneType] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
            " |      will be inferred if not specified. Keys and values are converted for output using either\n",
            " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
            " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
            " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to Hadoop file\n",
            " |      outputFormatClass : str\n",
            " |          fully qualified classname of Hadoop OutputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
            " |      keyClass : str, optional\n",
            " |          fully qualified classname of key Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
            " |      valueClass : str, optional\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |      conf : dict, optional\n",
            " |          (None by default)\n",
            " |      compressionCodecClass : str\n",
            " |          fully qualified classname of the compression codec class\n",
            " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
            " |  \n",
            " |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Union[str, NoneType] = None, valueConverter: Union[str, NoneType] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
            " |      converted for output using either user specified converters or, by default,\n",
            " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      conf : dict\n",
            " |          Hadoop job configuration\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |  \n",
            " |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Union[str, NoneType] = None, valueClass: Union[str, NoneType] = None, keyConverter: Union[str, NoneType] = None, valueConverter: Union[str, NoneType] = None, conf: Union[Dict[str, str], NoneType] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
            " |      will be inferred if not specified. Keys and values are converted for output using either\n",
            " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
            " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
            " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
            " |      \n",
            " |      path : str\n",
            " |          path to Hadoop file\n",
            " |      outputFormatClass : str\n",
            " |          fully qualified classname of Hadoop OutputFormat\n",
            " |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
            " |      keyClass : str, optional\n",
            " |          fully qualified classname of key Writable class\n",
            " |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
            " |      valueClass : str, optional\n",
            " |          fully qualified classname of value Writable class\n",
            " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
            " |      keyConverter : str, optional\n",
            " |          fully qualified classname of key converter (None by default)\n",
            " |      valueConverter : str, optional\n",
            " |          fully qualified classname of value converter (None by default)\n",
            " |      conf : dict, optional\n",
            " |          Hadoop job configuration (None by default)\n",
            " |  \n",
            " |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n",
            " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
            " |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n",
            " |      is 10.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tmpFile = NamedTemporaryFile(delete=True)\n",
            " |      >>> tmpFile.close()\n",
            " |      >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
            " |      >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\n",
            " |      ['1', '2', 'rdd', 'spark']\n",
            " |  \n",
            " |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Union[str, NoneType] = None) -> None\n",
            " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
            " |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
            " |      RDD's key and value types. The mechanism is as follows:\n",
            " |      \n",
            " |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n",
            " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to sequence file\n",
            " |      compressionCodecClass : str, optional\n",
            " |          fully qualified classname of the compression codec class\n",
            " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
            " |  \n",
            " |  saveAsTextFile(self, path: str, compressionCodecClass: Union[str, NoneType] = None) -> None\n",
            " |      Save this RDD as a text file, using string representations of elements.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      path : str\n",
            " |          path to text file\n",
            " |      compressionCodecClass : str, optional\n",
            " |          fully qualified classname of the compression codec class\n",
            " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tempFile = NamedTemporaryFile(delete=True)\n",
            " |      >>> tempFile.close()\n",
            " |      >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
            " |      >>> from fileinput import input\n",
            " |      >>> from glob import glob\n",
            " |      >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
            " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
            " |      \n",
            " |      Empty lines are tolerated when saving to text files.\n",
            " |      \n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
            " |      >>> tempFile2.close()\n",
            " |      >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
            " |      >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
            " |      '\\n\\n\\nbar\\nfoo\\n'\n",
            " |      \n",
            " |      Using compressionCodecClass\n",
            " |      \n",
            " |      >>> from tempfile import NamedTemporaryFile\n",
            " |      >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
            " |      >>> tempFile3.close()\n",
            " |      >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
            " |      >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
            " |      >>> from fileinput import input, hook_compressed\n",
            " |      >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\n",
            " |      >>> ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n",
            " |      'bar\\nfoo\\n'\n",
            " |  \n",
            " |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n",
            " |      Assign a name to this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd1 = sc.parallelize([1, 2])\n",
            " |      >>> rdd1.setName('RDD1').name()\n",
            " |      'RDD1'\n",
            " |  \n",
            " |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Union[int, NoneType] = None) -> 'RDD[T]'\n",
            " |      Sorts this RDD by the given keyfunc\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
            " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
            " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
            " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
            " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
            " |  \n",
            " |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Union[bool, NoneType] = True, numPartitions: Union[int, NoneType] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7ff14f23fe50>) -> 'RDD[Tuple[K, V]]'\n",
            " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
            " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
            " |      ('1', 3)\n",
            " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
            " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
            " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
            " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
            " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
            " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
            " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
            " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
            " |  \n",
            " |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n",
            " |      Return a :class:`StatCounter` object that captures the mean, variance\n",
            " |      and count of the RDD's elements in one operation.\n",
            " |  \n",
            " |  stdev(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the standard deviation of this RDD's elements.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
            " |      0.816...\n",
            " |  \n",
            " |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Union[int, NoneType] = None) -> 'RDD[T]'\n",
            " |      Return each value in `self` that is not contained in `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
            " |      >>> sorted(x.subtract(y).collect())\n",
            " |      [('a', 1), ('b', 4), ('b', 5)]\n",
            " |  \n",
            " |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Union[int, NoneType] = None) -> 'RDD[Tuple[K, V]]'\n",
            " |      Return each (key, value) pair in `self` that has no pair with matching\n",
            " |      key in `other`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
            " |      >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
            " |      >>> sorted(x.subtractByKey(y).collect())\n",
            " |      [('b', 4), ('b', 5)]\n",
            " |  \n",
            " |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Add up the elements in this RDD.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
            " |      6.0\n",
            " |  \n",
            " |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n",
            " |      Approximate operation to return the sum within a timeout\n",
            " |      or meet the confidence.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
            " |      >>> r = sum(range(1000))\n",
            " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
            " |      True\n",
            " |  \n",
            " |  take(self: 'RDD[T]', num: int) -> List[~T]\n",
            " |      Take the first num elements of the RDD.\n",
            " |      \n",
            " |      It works by first scanning one partition, and use the results from\n",
            " |      that partition to estimate the number of additional partitions needed\n",
            " |      to satisfy the limit.\n",
            " |      \n",
            " |      Translated from the Scala implementation in RDD#take().\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
            " |      [2, 3]\n",
            " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
            " |      [2, 3, 4, 5, 6]\n",
            " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
            " |      [91, 92, 93]\n",
            " |  \n",
            " |  takeOrdered(self: 'RDD[T]', num: int, key: Union[Callable[[~T], ForwardRef('S')], NoneType] = None) -> List[~T]\n",
            " |      Get the N elements from an RDD ordered in ascending order or as\n",
            " |      specified by the optional key function.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
            " |      [1, 2, 3, 4, 5, 6]\n",
            " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
            " |      [10, 9, 7, 6, 5, 4]\n",
            " |  \n",
            " |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Union[int, NoneType] = None) -> List[~T]\n",
            " |      Return a fixed-size sampled subset of this RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(0, 10))\n",
            " |      >>> len(rdd.takeSample(True, 20, 1))\n",
            " |      20\n",
            " |      >>> len(rdd.takeSample(False, 5, 2))\n",
            " |      5\n",
            " |      >>> len(rdd.takeSample(False, 15, 3))\n",
            " |      10\n",
            " |  \n",
            " |  toDF(self: 'RDD[Any]', schema: Union[Any, NoneType] = None, sampleRatio: Union[float, NoneType] = None) -> 'DataFrame'\n",
            " |  \n",
            " |  toDebugString(self) -> Union[bytes, NoneType]\n",
            " |      A description of this RDD and its recursive dependencies for debugging.\n",
            " |  \n",
            " |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n",
            " |      Return an iterator that contains all of the elements in this RDD.\n",
            " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
            " |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      prefetchPartitions : bool, optional\n",
            " |          If Spark should pre-fetch the next partition\n",
            " |          before it is needed.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize(range(10))\n",
            " |      >>> [x for x in rdd.toLocalIterator()]\n",
            " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            " |  \n",
            " |  top(self: 'RDD[T]', num: int, key: Union[Callable[[~T], ForwardRef('S')], NoneType] = None) -> List[~T]\n",
            " |      Get the top N elements from an RDD.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This method should only be used if the resulting array is expected\n",
            " |      to be small, as all the data is loaded into the driver's memory.\n",
            " |      \n",
            " |      It returns the list sorted in descending order.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
            " |      [12]\n",
            " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
            " |      [6, 5]\n",
            " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
            " |      [4, 3, 2]\n",
            " |  \n",
            " |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n",
            " |      Aggregates the elements of this RDD in a multi-level tree\n",
            " |      pattern.\n",
            " |      \n",
            " |      depth : int, optional\n",
            " |          suggested depth of the tree (default: 2)\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> add = lambda x, y: x + y\n",
            " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
            " |      >>> rdd.treeAggregate(0, add, add)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
            " |      -5\n",
            " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
            " |      -5\n",
            " |  \n",
            " |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n",
            " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      f : function\n",
            " |      depth : int, optional\n",
            " |          suggested depth of the tree (default: 2)\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> add = lambda x, y: x + y\n",
            " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
            " |      >>> rdd.treeReduce(add)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 1)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 2)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 5)\n",
            " |      -5\n",
            " |      >>> rdd.treeReduce(add, 10)\n",
            " |      -5\n",
            " |  \n",
            " |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
            " |      Return the union of this RDD and another one.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
            " |      >>> rdd.union(rdd).collect()\n",
            " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
            " |  \n",
            " |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n",
            " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
            " |      memory and disk.\n",
            " |      \n",
            " |      .. versionchanged:: 3.0.0\n",
            " |         Added optional argument `blocking` to specify whether to block until all\n",
            " |         blocks are deleted.\n",
            " |  \n",
            " |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n",
            " |      Return an RDD with the values of each tuple.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
            " |      >>> m.collect()\n",
            " |      [2, 4]\n",
            " |  \n",
            " |  variance(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
            " |      Compute the variance of this RDD's elements.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
            " |      0.666...\n",
            " |  \n",
            " |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n",
            " |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n",
            " |      This is only supported on certain cluster managers and currently requires dynamic\n",
            " |      allocation to be enabled. It will result in new executors with the resources specified\n",
            " |      being acquired to calculate the RDD.\n",
            " |      \n",
            " |      .. versionadded:: 3.1.0\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      This API is experimental\n",
            " |  \n",
            " |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
            " |      Zips this RDD with another one, returning key-value pairs with the\n",
            " |      first element in each RDD second element in each RDD, etc. Assumes\n",
            " |      that the two RDDs have the same number of partitions and the same\n",
            " |      number of elements in each partition (e.g. one was made through\n",
            " |      a map on the other).\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> x = sc.parallelize(range(0,5))\n",
            " |      >>> y = sc.parallelize(range(1000, 1005))\n",
            " |      >>> x.zip(y).collect()\n",
            " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
            " |  \n",
            " |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
            " |      Zips this RDD with its element indices.\n",
            " |      \n",
            " |      The ordering is first based on the partition index and then the\n",
            " |      ordering of items within each partition. So the first item in\n",
            " |      the first partition gets index 0, and the last item in the last\n",
            " |      partition receives the largest index.\n",
            " |      \n",
            " |      This method needs to trigger a spark job when this RDD contains\n",
            " |      more than one partitions.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
            " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
            " |  \n",
            " |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
            " |      Zips this RDD with generated unique Long ids.\n",
            " |      \n",
            " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
            " |      n is the number of partitions. So there may exist gaps, but this\n",
            " |      method won't trigger a spark job, which is different from\n",
            " |      :meth:`zipWithIndex`.\n",
            " |      \n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
            " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  context\n",
            " |      The :class:`SparkContext` that this RDD was created on.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  __parameters__ = (+T_co,)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwds)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD Actions "
      ],
      "metadata": {
        "id": "sygejvlpaRWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "l= [1,2,3,4,5,6,7,8,9,10,12,]\n",
        "rdd2= sc.parallelize(l)"
      ],
      "metadata": {
        "id": "A9ro5zveaZX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions : collect (convert RDD to in-memory list)\n",
        "\n",
        "rdd2.collect()          "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laMARQsGbEuB",
        "outputId": "9d25dd8a-a2df-4478-bdf7-716aeca464ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Action - take() (prints first elements)\n",
        "\n",
        "rdd2.take(5)"
      ],
      "metadata": {
        "id": "s2idznb2bHDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26689afa-db91-4cca-e22d-29cb7f7e967c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Action - top( prints top elements)\n",
        "\n",
        "rdd2.top(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCiTGbDpbOK-",
        "outputId": "4460938b-f1bb-4379-ee99-eb53eb491004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12, 10, 9, 8, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions :  takeSample (take some sample random values from list, if it's true it will repeat same value again ,False means unique)\n",
        "\n",
        "rdd2.takeSample(False,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDYAUIA5sSj0",
        "outputId": "30c785c6-b0e3-4128-9b51-41e6a8c6b839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 12, 9, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - Aggregate functions - gives single output value\n",
        "#Action - count( no.of elements)\n",
        "\n",
        "rdd2.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzT3YCvUbfC1",
        "outputId": "7e2fa028-ef2e-4a96-9371-2436cb88ae3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - Aggregate functions - gives single output value\n",
        "# Action - min,max\n",
        "\n",
        "print(rdd2.min())\n",
        "print(rdd2.max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVWM8NJndm9W",
        "outputId": "94404279-4af9-4b10-8f50-afb08900f7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - Aggregate functions - gives single output value\n",
        "#Actions - sum(),mean(),stdev\n",
        "\n",
        "print(rdd2.sum())\n",
        "print(rdd2.mean())\n",
        "print(rdd2.stdev())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PFAQtiydxLt",
        "outputId": "5390f7c6-e529-4201-ea12-ff405fcd8e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "67\n",
            "6.090909090909091\n",
            "3.3153786416019035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - stats- complete info about count,mean,stdev,max,min\n",
        "rdd2.stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmQ_Ta4Z6K55",
        "outputId": "e90f7400-2a56-4a05-f4ae-04a588914f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(count: 11, mean: 6.090909090909091, stdev: 3.3153786416019035, max: 12.0, min: 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - Aggregate functions - gives single output value\n",
        "# Actions- reduce that aggregates a data set(RDD) element using function\n",
        "\n",
        "print(rdd2.reduce(lambda x,y: x*y))\n",
        "print(rdd2.reduce(lambda x,y: x+y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlSk6Dz5hWJh",
        "outputId": "d1baa505-e853-45b8-8315-aa3c7a541766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43545600\n",
            "67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - CountByValue - count of same values\n",
        "\n",
        "rdd2.countByValue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS8Brnesh-Ij",
        "outputId": "56e0564a-b119-4078-c2a1-3b72b83a7cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {1: 1,\n",
              "             2: 1,\n",
              "             3: 1,\n",
              "             4: 1,\n",
              "             5: 1,\n",
              "             6: 1,\n",
              "             7: 1,\n",
              "             8: 1,\n",
              "             9: 1,\n",
              "             10: 1,\n",
              "             12: 1})"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - CountByKey \n",
        "\n",
        "l1= [('r',1),('k',2),('p',2),('q',2),('r',1),('i',2),('r',1),('i',2)]\n",
        "\n",
        "rdd3=sc.parallelize(l1)\n",
        "rdd3.countByKey()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eZo9__trxwW",
        "outputId": "46db028f-153f-428a-ec54-77f9d7b11788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int, {'r': 3, 'k': 1, 'p': 1, 'q': 1, 'i': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - CountByValue - count of same values\n",
        "rdd3.countByValue()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbpbCI1ktKD6",
        "outputId": "78722120-c46b-4c5c-f785-2048c1ec0422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('r', 1): 3, ('k', 2): 1, ('p', 2): 1, ('q', 2): 1, ('i', 2): 2})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - fold - aggregate the elements of each partition\n",
        "from operator import *\n",
        "\n",
        "rdd2.fold(1,add)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at7I4hjetaah",
        "outputId": "f6dd6199-915d-4511-ad18-6d39469d560a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ACtions- range of even numbers\n",
        "\n",
        "R= sc.parallelize(range(0,20,2))\n",
        "R.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyIn--7Ptfg3",
        "outputId": "e84d3fe8-6fb6-4142-8721-e1dccf47ed8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actions - variance (all n values variance)\n",
        "R.variance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XzAJJiex4hy",
        "outputId": "c122b8ce-7190-4c91-e9e1-fafcaa831552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33.0"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - sample variance - (n-1) values variance\n",
        "\n",
        "R.sampleVariance()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1jJBGL1yRV1",
        "outputId": "b72ce053-9aa3-42d9-af08-b228116e5416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.666666666666664"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read file csv file using RDD\n",
        "\n",
        "rdd1 = sc.textFile(\"/content/emp.csv\")\n",
        "rdd1.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOJyjDZu1tHt",
        "outputId": "b97b4950-805f-481e-b48e-538be7739899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO,UPDATED_DATE',\n",
              " '7369,SMITH,CLERK,7902,17-12-1980,800,300,20,01-01-2022',\n",
              " '7499,ALLEN,SALESMAN,7698,20-02-1981,1600,300,30,01-01-2022',\n",
              " '7521,WARD,SALESMAN,7698,22-02-1981,1250,500,30,01-01-2022',\n",
              " '7566,JONES,MANAGER,7839,04-02-1981,2975,null,20,05-01-2022',\n",
              " '7654,MARTIN,SALESMAN,7698,21-09-1981,1250,1400,30,03-01-2022',\n",
              " '7698,SGR,null,7839,05-01-1981,2850,1600,30,04-01-2022',\n",
              " '7782,RAVI,MANAGER,7839,06-09-1981,2450,100,null,02-01-2022',\n",
              " '7788,SCOTT,ANALYST,7566,19-04-1987,null,null,20,02-01-2022',\n",
              " 'null,null,PRESIDENT,null,01-11-1981,null,null,null,02-01-2022',\n",
              " '7844,TURNER,SALESMAN,7698,09-08-1981,1500,null,30,02-01-2022',\n",
              " '7876,ADAMS,CLERK,7788,23-05-1987,1100,null,20,03-01-2022',\n",
              " '7900,JAMES,CLERK,7698,12-03-1981,950,null,30,03-01-2022',\n",
              " '7902,FORD,ANALYST,7566,12-03-1981,3000,null,20,03-01-2022',\n",
              " '7369,SMITH,CLERK,7902,17-12-1980,800,null,20,04-01-2022',\n",
              " '7499,ALLEN,SALESMAN,7698,20-02-1981,1600,300,30,04-01-2022',\n",
              " '7521,WARD,SALESMAN,7698,22-02-1981,1250,500,30,04-01-2022']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - saveAsTextFile\n",
        "\n",
        "rdd1.coalesce(3).saveAsTextFile('/content/sample_data/rdd1')"
      ],
      "metadata": {
        "id": "uODMWd8A1zu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Actions - saveAsPickleFile\n",
        "\n",
        "rdd1.coalesce(2).saveAsPickleFile('/content/sample_data/r2')"
      ],
      "metadata": {
        "id": "LJy0pVX22iNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD transformations"
      ],
      "metadata": {
        "id": "FH8VBrqToCVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map - Return new distributed dataset formed by passing each element of source through a function \n",
        "\n",
        "x= rdd.map(lambda x : (x,x*3))\n",
        "\n",
        "print( 'Values:',rdd.collect())\n",
        "print('Values:',x.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufhcuSm1oM2O",
        "outputId": "8a0e3e27-01a9-4fb3-9208-43317c2035dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values: [1, 2, 3, 4, 5, 6]\n",
            "Values: [(1, 3), (2, 6), (3, 9), (4, 12), (5, 15), (6, 18)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatMap -Similar to map ,but each input iteam can be mapped to 0 or more output items\n",
        "\n",
        "y = rdd.flatMap(lambda x : (x, x*4,x/4,x**4))\n",
        "\n",
        "y.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_8biIZuuJi2",
        "outputId": "3da1dcaa-3aff-4153-cab9-960441a80129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 4,\n",
              " 0.25,\n",
              " 1,\n",
              " 2,\n",
              " 8,\n",
              " 0.5,\n",
              " 16,\n",
              " 3,\n",
              " 12,\n",
              " 0.75,\n",
              " 81,\n",
              " 4,\n",
              " 16,\n",
              " 1.0,\n",
              " 256,\n",
              " 5,\n",
              " 20,\n",
              " 1.25,\n",
              " 625,\n",
              " 6,\n",
              " 24,\n",
              " 1.5,\n",
              " 1296]"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filter - Return a new dataset formed by selecting those elements of source\n",
        "\n",
        "z= rdd.filter( lambda x : x%2 == 1)\n",
        "\n",
        "z.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQRkhTXqpbaD",
        "outputId": "73d5fd46-4442-4050-98ae-59cdd69d66d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 3, 5]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Partition given list and print partition lists\n",
        "\n",
        "l = [1,2,3,4,5,6,7,8,9,10]\n",
        "\n",
        "P = sc.parallelize(l,4)\n",
        "\n",
        "P.glom().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9dGNJwsoe_p",
        "outputId": "f53e4e17-1c01-4573-9605-360bbdad6fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2], [3, 4], [5, 6], [7, 8, 9, 10]]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#mapPartitions - Similar to map but runs separetely on each partition \n",
        "\n",
        "def f(iterator) : yield sum(iterator)\n",
        "\n",
        "M = P.mapPartitions(f)\n",
        "\n",
        "print(P.glom().collect())\n",
        "print(M.glom().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRtMSxvOu7Hq",
        "outputId": "677e4d5a-062a-49e2-dace-61709f35433f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [3, 4], [5, 6], [7, 8, 9, 10]]\n",
            "[[3], [7], [11], [34]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mapPartitionWIthIndex - Similar to Mappartition ,but also provides an integer value is index of partition\n",
        "\n",
        "def ff(Index,iterator): yield (Index,sum(iterator))\n",
        "\n",
        "I = P.mapPartitionsWithIndex(ff)\n",
        "\n",
        "print(P.glom().collect())\n",
        "print(I.glom().collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAJB0BzcqjaI",
        "outputId": "ecc0472d-a8a7-4997-8993-0a7576ecf141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2], [3, 4], [5, 6], [7, 8, 9, 10]]\n",
            "[[(0, 3)], [(1, 7)], [(2, 11)], [(3, 34)]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sample - a fraction of fraction data ,with or without replacement ,using a given random number\n",
        "\n",
        "# sample(withreplacement.fraction,seed)  ,if (with replacement) - true - will repeat same value ,else (withreplacement) -False - unique values\n",
        "\n",
        "s = sc.parallelize([1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "S=s.sample(True,1)\n",
        "\n",
        "print(s.collect())\n",
        "print(S.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvxxYfpTv1Fl",
        "outputId": "beebbf34-ec5e-4f78-c01b-5045556093d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "[2, 3, 4, 5, 5, 6, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#union,intersection\n",
        "\n",
        "A = sc.parallelize(range(20))\n",
        "B = sc.parallelize(range(0,20,2))\n",
        "print(A.collect())\n",
        "print(B.collect())\n",
        "\n",
        "C = A.union(B)\n",
        "D = A.intersection(B)\n",
        "\n",
        "print(C.collect())\n",
        "print(D.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykAeVPp0yD37",
        "outputId": "17ada3f1-ae02-429f-fb12-cfb58ec6cb1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
            "[0, 4, 8, 12, 16, 2, 6, 10, 14, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# distinct \n",
        "\n",
        "D = C.distinct()\n",
        "print(D.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kvdGBP2ywlE",
        "outputId": "30538ce4-57fb-4a8e-bbc3-0e01d914e7ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 4, 8, 12, 16, 1, 5, 9, 13, 17, 2, 6, 10, 14, 18, 3, 7, 11, 15, 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# groupByKey\n",
        "\n",
        "L = [('A',1),('A',2),('B',3),('B',4),('C',5),('C',6)]\n",
        "\n",
        "g = sc.parallelize(L)\n",
        "y= g.groupByKey()\n",
        "\n",
        "\n",
        "print([(j[0],[i for i in j[1]]) for j in y.collect()] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2CZ6RcL2G86",
        "outputId": "e8fd228c-f82e-4d82-cf42-b6001fdf729d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('C', [5, 6]), ('A', [1, 2]), ('B', [3, 4])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Caching \n",
        "\n"
      ],
      "metadata": {
        "id": "aCJm8ARjBOTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#caching -caching is  used to save the data(RDD/Dataframe/Dataset) in a cluster-wide in memory,cache() method default saves data in MEMORY_ONLY.Used to store small amount od data. This is Very Useful for accessing repeated data .such as querying a small data set or when running an iterative algorithm\n",
        "\n",
        "rdd = sc.textFile('/content/sample_data/california_housing_test.csv')\n"
      ],
      "metadata": {
        "id": "OIZHLmxLBmJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting total time taken for count before cache Enabled\n",
        "# print(t.timeit(number =5))\n",
        "\n",
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "rdd.count()\n",
        "rdd.min()\n",
        "rdd.max()\n",
        "rdd.collect()\n",
        "\n",
        "\n",
        "end = timeit.default_timer()\n",
        "\n",
        "print('elapsed time:',(end-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt1Vqt4jERfn",
        "outputId": "233857d8-a265-4ca4-d262-00d6a4500fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 0.6071388719999504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# caching rdd \n",
        "# cahe will save dataesets default in memory\n",
        "rdd.cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39PbEciVGQsC",
        "outputId": "5afc3176-ea1a-423a-95fb-db6d1f48e165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/california_housing_test.csv MapPartitionsRDD[85] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Getting total time taken for count dfter cache Enabled\n",
        "# print(t.timeit(number =5))\n",
        "\n",
        "import timeit\n",
        "\n",
        "start = timeit.default_timer()\n",
        "\n",
        "rdd.count()\n",
        "rdd.min()\n",
        "rdd.max()\n",
        "rdd.collect()\n",
        "\n",
        "\n",
        "end = timeit.default_timer()\n",
        "\n",
        "print('elapsed time:',(end-start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Nqsb_gIQPa",
        "outputId": "c576dc39-6010-417a-855b-d8bdffa11b3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "elapsed time: 0.7174905510000826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# persist"
      ],
      "metadata": {
        "id": "hp8Y6NPEJyu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#same like cahe ( stores data in memeory) but we can store large amout of  data and also persist() method is used to store it to the user-defined storage levels like (memory only , disk only ,memory and disk only etc)\n",
        "\n",
        "#rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
        " \n",
        "#rdd.persist(pyspark.StorageLevel.DISK_ONLY)\n",
        "\n",
        "rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWYzccXYKFgS",
        "outputId": "4e2241c0-f0e6-4242-d9bf-fdc9dd01c8e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/california_housing_test.csv MapPartitionsRDD[85] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clears cache and persist data manually\n",
        "rdd.unpersist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7iW9vcvK81F",
        "outputId": "a67dc337-9dc6-4cb6-9426-5a3dd51bd195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "/content/sample_data/california_housing_test.csv MapPartitionsRDD[85] at textFile at NativeMethodAccessorImpl.java:0"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adventages of Cache and persist "
      ],
      "metadata": {
        "id": "4HxuWH7wDANA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cost efficient  Spark computations are very expensive hence reusing the computations are used to save cost.\n",
        "#Time efficient  Reusing the repeated computations saves lots of time.\n",
        "#Execution time  Saves execution time of the job and we can perform more jobs on the same cluster."
      ],
      "metadata": {
        "id": "GflEhc44DcOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Broadcast variables"
      ],
      "metadata": {
        "id": "95UKMxw_Nypz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# allow programmer to keep a read only variable cached on each machine rather than shipping a copy of it with tasks\n",
        "# spark useses efficient broadcast algoritham to reduce communication cost\n",
        "\n",
        "Board = sc.broadcast([1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "print(Board.value)\n",
        "print(type(Board))\n"
      ],
      "metadata": {
        "id": "FoM2xsckOGu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear the boarcast variables by using unpersist or destroy\n",
        "\n",
        "#Board.unpersist()\n",
        "\n",
        "Board.destroy"
      ],
      "metadata": {
        "id": "cI6RlK10QDyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accumulators"
      ],
      "metadata": {
        "id": "U_2N-cGxR0at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accumulators are variables that are only \"added\" to through an associative and commutative operation and can therefore efficiently supported in parallel\n",
        "# they can be used to implement counters or sums .\n",
        "#spark natively supports accumulators of numeric types and programmers can add support for new types.\n",
        "\n",
        "accum = sc.accumulator(100)\n",
        "\n",
        "sc.parallelize([1,2,3,4,5,6,7,8,9]).foreach(lambda x : accum.add(x))\n",
        "\n",
        "accum.value"
      ],
      "metadata": {
        "id": "-4Tw1NmSSRRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(accum)"
      ],
      "metadata": {
        "id": "0u39udawVZNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations  - Joins"
      ],
      "metadata": {
        "id": "EA2YRT5ULuz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#join - takes two pair of rdd ,return only matched records from both RDD's\n",
        "\n",
        "J1 = sc.parallelize([('A',2),('B',3),('C',4),('D',5),('E',6),('F',7)])\n",
        "\n",
        "J2 = sc.parallelize([('C',2),('D',3),('E',4),('F',5),('G',6),('H',7)])\n",
        "\n",
        "join = J1.join(J2)\n",
        "\n",
        "print(join.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl3dfn_xMkRB",
        "outputId": "561855e2-e482-4b6d-a7a3-f267dde221bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('F', (7, 5)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# leftouterjoin  - returns matched records from both rdd and unmatched records from left rdd\n",
        "\n",
        "left = J1.leftOuterJoin(J2)\n",
        "print( left.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9EHyuXfNRau",
        "outputId": "f04136e7-a1bc-401f-ac14-7eddfb5e4e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', (2, None)), ('B', (3, None)), ('F', (7, 5)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RightOuterJoin - returns matched records from both rdd and unmatched from right rdd\n",
        "\n",
        "right = J1.rightOuterJoin(J2)\n",
        "print(right.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e71I6-k6PWxK",
        "outputId": "6563fa8e-05c9-483d-c832-9c7e9dae0296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('F', (7, 5)), ('H', (None, 7)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4)), ('G', (None, 6))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fullOuterJoin - returned all records from both RDD'S\n",
        "\n",
        "full = J1.fullOuterJoin(J2)\n",
        "print(full.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qwMF9V4PmAt",
        "outputId": "ea5db6b3-8d1a-41c1-8577-717266e29801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('A', (2, None)), ('B', (3, None)), ('F', (7, 5)), ('H', (None, 7)), ('C', (4, 2)), ('D', (5, 3)), ('E', (6, 4)), ('G', (None, 6))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cartesian join - cross product of both elements of RDD\n",
        "\n",
        "cert = J1.cartesian(J2)\n",
        "print(cert.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9tAmVPLQxAa",
        "outputId": "8f52c0d0-439c-4f7a-a03f-27c5cf9b174a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('A', 2), ('C', 2)), (('A', 2), ('D', 3)), (('A', 2), ('E', 4)), (('B', 3), ('C', 2)), (('B', 3), ('D', 3)), (('B', 3), ('E', 4)), (('C', 4), ('C', 2)), (('C', 4), ('D', 3)), (('C', 4), ('E', 4)), (('A', 2), ('F', 5)), (('A', 2), ('G', 6)), (('A', 2), ('H', 7)), (('B', 3), ('F', 5)), (('B', 3), ('G', 6)), (('B', 3), ('H', 7)), (('C', 4), ('F', 5)), (('C', 4), ('G', 6)), (('C', 4), ('H', 7)), (('D', 5), ('C', 2)), (('D', 5), ('D', 3)), (('D', 5), ('E', 4)), (('E', 6), ('C', 2)), (('E', 6), ('D', 3)), (('E', 6), ('E', 4)), (('F', 7), ('C', 2)), (('F', 7), ('D', 3)), (('F', 7), ('E', 4)), (('D', 5), ('F', 5)), (('D', 5), ('G', 6)), (('D', 5), ('H', 7)), (('E', 6), ('F', 5)), (('E', 6), ('G', 6)), (('E', 6), ('H', 7)), (('F', 7), ('F', 5)), (('F', 7), ('G', 6)), (('F', 7), ('H', 7))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark-submit local"
      ],
      "metadata": {
        "id": "x3LSF65HMUEf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**wc.py :**\n",
        "\n",
        "from pyspark.sql import sparkSession\n",
        "\n",
        "spark = sparkSession.builder.master('local').appName('wcout').getOrCreate()\n",
        "\n",
        "<<<<< logic here >>>>>>\n",
        "\n",
        "\n",
        "**syntax: spark-submit in local**\n",
        "\n",
        "spark-submit \"configrationoptions\" \"programfile_path\" file:input_path file:output_path\n",
        "\n",
        "EX:\n",
        "\n",
        "spark_submit /home/cloudera/wc.py file://home/cloudera/csvfile file://home/cloudera/wcout\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A3Aanoz6NZL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark-Submit Yarn cluster"
      ],
      "metadata": {
        "id": "1RFcRqrBTL8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**wc.py:**\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('yarn').appName('wcout').getOrCreate()\n",
        "\n",
        "<<<<< logic here >>>>>>\n",
        "\n",
        "**syntax: spark-submit in yarn**\n",
        "\n",
        "spark-submit \"configrationoptions\" \"programfile_path\" file:input_path file:output_path\n",
        "\n",
        "spark-submit /home/cloudera/wc.py hdfs://localhost:8020/file1 hdfs://localhost:8020/wcout"
      ],
      "metadata": {
        "id": "x9fnClxtTS7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spark-submit without configuration values :"
      ],
      "metadata": {
        "id": "IINtHsNlceIi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ByDefault values are in Yarn cluster\n",
        "\n",
        "driver memory   :  1GB memory, \n",
        "executor memory :  1GB memory, \n",
        "number of cores per executor : 1 "
      ],
      "metadata": {
        "id": "VADqGEUDa3qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FIFO/FAIR"
      ],
      "metadata": {
        "id": "yaS5w6_Wd_Kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Spark support two Schedulers:\n",
        "\n",
        "1.FIFO : (First in First out ) only one application at a time.second application under waiting state until first one completes.\n",
        "\n",
        "2.FAIR : In order to allow multiple apps to run on cluster the applications must be submitted to Fair scheduler\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e74FQ1Ssen4_"
      }
    }
  ]
}