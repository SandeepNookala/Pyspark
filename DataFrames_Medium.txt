1.read csv file data:
=====================
df_emp = spark.read.format('csv').option('header',True).option('sep','||').option('nullValue','null').option('inferSchema',True).load('/Volumes/workspace/sandeep/input/emp_double_pipe.txt')

df_emp.printSchema()

df_emp.show(5)

df_emp.count()



2.New column with Defult values:
================================
from pyspark.sql.functions import *
df_emp_new = df_emp.withColumn('Country_code',lit('IND'))
df_emp_new.show()



3.Concatinating two columns:
===========================================
df_emp_conc =df_emp_new.withColumn('name_country',concat('EName','Country_Code') )
df_emp_conc2 =df_emp_new.withColumn('name_country',concat('EName',lit('-'),'Country_Code') )
df_emp_conc.show(5)
df_emp_conc2.show(5)



4.Concatinating two columns with separator:
===========================================
df_emp_conc2=df_emp_new.withColumn('name_country',concat_ws('-','ENAME','country_code','JOB'))
df_emp_conc2.show()



5.Change data type of column:
=============================
df_emp_type = df_emp_conc.withColumn('EMPNO', col('EMPNO').cast('string'))
df_emp_type.printSchema()



6.convert from String (dd-mm-yyyy) date format to spark date format (yyyy-mm-dd):
=================================================================================
df_emp_date = df_emp_type.withColumn('UPDATED_DATE', to_date('UPDATED_DATE','dd-mm-yyyy'))
df_emp_date.printSchema()
df_emp_date.show()



7.Adding current timestamp to dataframe:
========================================
df_emp_curr = df_emp_date.withColumn("current_timestamp",current_timestamp())
df_emp_curr.show()



8.create year,month,day columns from Date Column:
=================================================
df_emp_parti = df_emp_curr.withColumn('day',date_format('UPDATED_DATE','dd')).withColumn('month',date_format('updated_date','MM')).withColumn('year',date_format('updated_date','yyyy'))
df_emp_parti.show()



9.Adding new columns with different values:
===========================================
df4 = df3.withColumn('Positions',when( col('JOB') == 'SALESMAN', 'Level4')
               .when( col('JOB') == 'CLERK' ,'Level3')
               .when( col('JOB') == 'ANALYST' ,'Level3')
               .when( col('JOB') == 'doctor' ,'Level3')
               .when( col('JOB') == 'MANAGER','Level2')
               .when(  col('JOB') == 'PRESIDENT','Level1'))

df4.show(10)



10.Create A partitioned dataframe by year,month,day and save as a parquet file:
=============================================================================
df_emp_new_col.write.partitionBy('year','month','day').mode('overwrite').format('parquet').save('/Volumes/workspace/sandeep/output/date')



11.Create A partitioned dataframe on JOB and save as a parquet file:
====================================================================
df_emp_new_col.write.partitionBy('JOB').format('parquet').mode('overwrite').save('/Volumes/workspace/sandeep/output/JOB/')



12.Create A partitioned dataframe by year,month,day save it to Employee Hire table:
===================================================================================
df_emp_new_col.write.partitionBy('year','month','day').saveAsTable('Employee_Hire_table')



13.Read parquet files with names from folder:
=============================================
read_file_name = spark.read.format('parquet').load('/Volumes/workspace/sandeep/output/JOB').withColumn('filename',col('_metadata.file_path'))
read_file_name.select('filename').show(truncate= False)



14.parquet files count:
=======================
read_file_name.groupBy('filename').count().show(truncate = False)



#15.read all parquet files from a directory inside another directory (nested/sub directory)
============================================================================================
spark.read.format('parquet').option('recursiveFilrLookup',True).load('/Volumes/workspace/sandeep/output/JOB/').show(5)



16.Dataframe Write Modes:
=========================
1.overwrite – mode is used to overwrite the existing file.
2.append – To add the data to the existing file.
3.ignore – Ignores write operation when the file already exists.
4.error – This is a default option when the file already exists, it returns an error.


df_emp_new_col.write.mode('overwrite').saveAsTable('Employee_Hire_table')
df_emp_new_col.write.mode('append').saveAsTable('Employee_Hire_table')
df_emp_new_col.write.mode('ignore').saveAsTable('Employee_Hire_table')



17.Repartition:
===============
df_emp_part = df_emp_new_col.repartition(3).withColumn('partition_id',spark_partition_id())



18.how to get no.of rows in each partition:
===========================================
df_emp_part.select('partition_id').groupBy('partition_id').count().show()


19.how to handle multi delimiter files:
=======================================
df_multi = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').option('sep','||').load('/Volumes/workspace/sandeep/input/emp_multiple_delimeter.txt')
df_multi.show()

df_multi_2= df_multi.withColumn('Split',split('SAL',','))
df_multi_2.show()

df_multi_final = df_multi_2.withColumn('SAL', col('Split')[0]).withColumn('CoMM',col('split')[1]).withColumn('DEPT_NO',col('split')[2]).withColumn('UPDATED_DATE',col('split')[3]).drop(col("Split"))
df_multi_final.show()
	
	
	
20.Remove duplicates in dataFrame:
==================================
1.distinct()
2.dropDuplicates()/drop_duplicates()
3.Remove duplicates using row_number()


df_dup = spark.read.format('csv').option('header',True).option('nullValue','null').option('inferSchema',True).load('/Volumes/workspace/sandeep/input/employee_dup.csv')



#distinct

df_dup.distinct().count()


#dropduplicates - it will keep only first record ,we will not get latest date data
#so we need to use order by function and desc to drop duplicates

df_unique = df_dup.orderBy( col('EMPNO').desc() ).dropDuplicates(['EMPNO'])
df_unique.show()


# window Function with row number

from pyspark.sql.functions import *
from pyspark.sql.window import *

win = Window.partitionBy('EMPNO').orderBy('EMPNO')

df_win = df_dup.withColumn('row_number',row_number().over(win))
good_data = df_win.filter( col('row_number') == 1)
good_data.show()

bad_data = df_win.filter( col('row_number') != 1)
bad_data.show()


#good data
good_data = df_row_number.filter( col('row_number') ==1 )
good_data.show(10)
good_data.count()



22. Save Duplicate data as data Frame:
======================================
#Duplicate data

duplicate_data  = df_row_number.filter( col('row_number')>1)
duplicate_data.show(10)
duplicate_data.count()



22.Add/generate sequence id /surrogate key column:
==================================================

1.monotonically_increasing_id()
2.crc32 - generate random numbers
3.md5 - hash key generater function
4.sha2 -hash key generate function
5.row_number() - window function


#1.monotonically_increasing_id()
df.withColumn('id',monotonically_increasing_id()).show(5,truncate = False)
df_emp.withColumn('mono_id',monotonically_increasing_id()+1).show(truncate=False)


#2.crc32 hash key
#1.generate random numbers - it is works only on string data type
#2.We should not use crc32 surrogate key generation on larger table because it can generate duplicate sequence if more than 100k/1M records 

df_emp.withColumn('crc32',crc32( col('EMPNO').cast('string'))).show(truncate=False)


#3.md5 - 32 bit hash key
#not suggeted for if records more than 1 Million it can generate duplicates

df_emp.withColumn('md5', md5 ( col('EMPNO').cast('string'))).show(truncate = False)


#4.sha2 - hash key value
#suggeted for huge data 256 0r 512 bits

df_emp_sha_256 = df_emp.withColumn('sha2',sha2(col('EMPNO').cast('string'),256))
df_emp_sha_256.show()
df_emp_sha_512 = df_emp.withColumn('sha2',sha2( col('EMPNO').cast('string'),512))
df_emp_sha_512.show()


#5.row_number

win_2 = Window.orderBy('EMPNO')
df_emp.withColumn( 'row_number' , row_number().over(win_2)).show()



23.Incremental loading:
=======================
loading data from source (daily) ----> Tansform -----> loading warehouse)

# read data of day0 file

emp_day0 = spark.read.format('csv').option('header',True).option('nullValue','null').option('inferSchema',True).load('/Volumes/workspace/sandeep/input/employee_day0.csv')

# write partitioned data into warhouse employee table
emp_day0.write.partitionBy('DEPTNO').saveAsTable('employee')

# query warhouse tables
spark.sql('select * from emp_dept').show()
spark.sql('select count(*) from emp_dept').show()


# read data of day1 file
emp_day1 = spark.read.format('csv').option('header',True).option('nullValue','null').option('inferSchema',True).load('/Volumes/workspace/sandeep/input/employee_day1.csv')

# append into warhouse employee table
emp_day1.write.mode('append').saveAsTable('employee')


# query warhouse tables
spark.sql('select * from emp_dept').show()
spark.sql('select count(*) from emp_dept').show()



24.fill missing data in textFile and convert into Dataframe:
============================================================
df_miss = spark.read.format('csv').option('sep',' ').load('/Volumes/workspace/sandeep/input/fill missing.txt').fillna('missing_data')

df_miss.show()



25.Creating Data Frame from mysql table:
========================================
df_mysql = spark.read.format('jdbc')
.option('url','jdbc:mysql://localhost:3306')
.option('driver','com.mysql.jdbc.Driver')
.option('user','root')
.option('password','sandeep')
.option('query','select * from sandeep.emp_table')
.load()



26.Creating DataFrame from Json file:
=====================================
df_json = spark.read.format('json').load('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/input/emp.json')

df_json.show(10)
df_json.printSchema()
df_json.count()



27.Creating DataFrame from multiLine Json file:
===============================================
df_mul_json = spark.read.format('json').option('multiline',True).option('inferSchema',True).load('/Volumes/workspace/sandeep/input/nested_json.json')

df_mul_json.show()

from pyspark.sql.functions import *
df_mul_json_1 = df_mul_json.withColumn('batters',explode('batters.batter')).withColumn('batters_id',col('batters.id')).withColumn('batters_type',col('batters.type'))
df_mul_json_1.show()


df_mul_json_2 = df_mul_json_1.withColumn('topping',explode('topping')).withColumn('topping_id',col('topping.id')).withColumn('topping.type',col('topping.type'))

df_mul_json_final = df_mul_json_2.drop("batters","topping")
df_mul_json_final.show()

