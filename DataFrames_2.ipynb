{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fc765e-f3c6-4a5e-8d22-a5690876cd58",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##1.handle yy date format in pyspark for before 2000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f85c7def-bf65-4ea4-ac5d-59422b99b82f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yy_df = spark.read.format('csv').option('header',True).option('sep','|').option('inferSchema',True).option('nullValue','null').load('/content/emp_pipe_yy.txt')\n",
    "\n",
    "yy_df.show()\n",
    "yy_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc2eaf21-0650-48cf-9a80-0be0efc4d549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Spark defaulty chooses year after 2000\n",
    "\n",
    "# we need to set spark sql legcy timeparserpolicy to legacy -- Useful for less columns data only ,if more data means need ask source system set proper date format\n",
    "\n",
    "spark.conf.set('spark.sql.legacy.timeParserPolicy','LEGACY')\n",
    "\n",
    "yy_df.withColumn('DATE',to_date('UPDATED_DATE','dd-mm-yy')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d0c9981-c5c0-4546-9cbe-eb13760b871a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##2.get number of null in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8423561-8d60-4846-ba33-c5461295f724",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_csv.select([count(when(col(i).isNull(),i)).alias(i) for i in df_csv.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "337b0be4-70a8-46c0-bfed-13fa1cdd5337",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##3.word count in single line - Find No.of Occurences of single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29cb5021-f81c-490e-95ef-7967850f76b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RDD = spark.sparkContext.textFile('/content/word.txt').flatMap(lambda x : x.split(' ')).map(lambda x:(x,1)).reduceByKey(lambda x,y : x+y)\n",
    "\n",
    "RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48785f8c-ce4b-4549-9b9d-2dff87bb7f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##4.Skip rows in given file ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae057233-2013-49f6-bdc4-22918c863556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "r1 = spark.sparkContext.textFile('/content/emp_pipe_skip.txt').zipWithIndex().filter(lambda a : a[1]>2).map(lambda a : a[0].split('|'))\n",
    "\n",
    "r1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5966b7d-79e2-4c29-be59-1d82456e7932",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = r1.collect()[0]\n",
    "skipline = r1.collect()[0]\n",
    "print(columns)\n",
    "print(skipline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aff577f1-8671-45c4-ac4f-4287682cb330",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sk_df = r1.filter(lambda a : a!= skipline).toDF(columns)\n",
    "\n",
    "sk_df.show(10)\n",
    "print(sk_df.count())\n",
    "sk_df.printSchema()\n",
    "print(type(sk_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1673f6f1-a2da-48f2-b5e1-7f7ff3182327",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##5.get 53 th weak years from last 50 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c91066d-38ae-4b5c-9c38-0cc35a82c1d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "we_df = spark.createDataFrame([(i,\"01/01/\"+str(i)+\"\")for i in range(1990,2050)],['id','year'])\n",
    "\n",
    "we_df = we_df.withColumn('date',to_date('year','dd/mm/yyyy')).withColumn('week',weekofyear('date')).filter('week= 53')\n",
    "we_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "689bcda1-3961-497e-a7d3-cf866f58f778",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##6.Creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b29d87-afb8-4540-83ef-14328da9ba18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cr_df = spark.createDataFrame([(i,\"02/01/\"+str(i)+\"\") for i in range (1999,2050)],['id','year'])\n",
    "\n",
    "cr_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87c1c23a-9434-4d14-8918-163615aab1e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##7.handle or how to read variable/dynamic no.of columns data file ( with Out Header File)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36b527dc-a831-4017-87f4-bdd83c9e4052",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format('text').load('/content/emp_without_header.txt').withColumn('New',split('value',','))\n",
    "\n",
    "# To find max No.of coulumns in a table\n",
    "col_size = df.select(max(size(col('New'))))\n",
    "\n",
    "df.show(5,truncate=True)\n",
    "col_size.show()\n",
    "print(type(df))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1958d5eb-24ba-41a0-a3c6-e505d0ebca1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(col_size.collect()[0][0]):\n",
    "  df = df.withColumn('col'+str(i),df['New'][i])\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29e9becd-fcd0-44fe-8ed2-c05acd76d4ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df.drop('value').drop('New')\n",
    "df1.show(truncate = False)\n",
    "df1.printSchema()\n",
    "print(type(df1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2568384f-4c86-4dc5-9908-abd8b64a9ab2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##8.udf register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94dec5db-7ffe-42c5-aa51-1008465d1cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# UDF\n",
    "\n",
    "def f1(x):\n",
    "  return ((x*x)-x)\n",
    "\n",
    "spark.udf.register('fun',f1,IntegerType())\n",
    "fun = udf(f1,IntegerType())\n",
    "\n",
    "print(f1(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8183571-0ec9-4d2e-96af-f4caeab195dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5.select('ENAME',sqrt('SAL'),fun('SAL')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3df84a00-a581-4738-8ad6-d0308fb089cb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##9.how to handle bad data\n",
    "\n",
    "Spark read Mode:\n",
    "1.PERMISSIVE - allows bad data - it's spark default mode \\\n",
    "2.FAILFAST - won't allows bad data -it raise expection - it won't process further \\\n",
    "3.DROPMALFORMED - drops bad records based on schema -it won't save bad records \\\n",
    "4.badrecordsPath - save good data in table and saves bad it another path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10747baf-030e-43b5-893f-d60b4a4f6a5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bad = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/channels.csv')\n",
    "\n",
    "bad.show()\n",
    "\n",
    "bad.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbee5a85-24c1-4411-aa6c-7b0982f92516",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# need to add _corrupt_record column string type in custom made schema\n",
    "schema  = StructType([StructField('CHANNEL_ID', IntegerType(), True),\n",
    "                      StructField('CHANNEL_DESC', StringType(), True),\n",
    "                      StructField('CHANNEL_CLASS', StringType(), True),\n",
    "                      StructField('CHANNEL_CLASS_ID', IntegerType(), True),\n",
    "                      StructField('CHANNEL_TOTAL', StringType(), True),\n",
    "                      StructField('CHANNEL_TOTAL_ID', IntegerType(), True),\n",
    "                      StructField(\"BadData\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1caa7c98-2323-45a5-bbd1-3aaf6f036a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#save bad Records Using mode - PERMISSIVE and _corrupt_record ,columnNameofCorrputRecord\n",
    "\n",
    "bad1 = spark.read.format('csv').schema(schema).option('Mode','PERMISSIVE').option('ColumnNameOfCorruptRecord','BadData').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
    "bad1.show()\n",
    "\n",
    "# filter good records\n",
    "goodData = bad1.filter('BadData is Null').drop('BAdData')\n",
    "goodData.show()\n",
    "\n",
    "# filter corrupt records\n",
    "bad3 = bad1.filter('BadData is Not Null')\n",
    "bad3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c54e0bbc-0670-4a92-b1ce-e5eab6515b16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#mode - FAILFAST\n",
    "\n",
    "bad = spark.read.format('csv').schema(schema).option('mode','FAILFAST').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
    "bad.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56d92f20-5c01-40da-be56-2ae77860861d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DROPMALFORMED\n",
    "\n",
    "bad = spark.read.format('csv').schema(schema).option('mode','DROPMALFORMED').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
    "bad.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b6a7178-800e-4bd9-9d1e-a566976cf45f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##10.Creating Data Frame from REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "661d9896-a8e0-4602-8e35-c7d0b897cdf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#REST API -Accessing the data over internet through Urls\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "api = requests.request('GET','https://api.github.com/users/hadley/orgs')\n",
    "\n",
    "data = api.json()\n",
    "\n",
    "file = open('/content/sample_data/apidata.json','a')\n",
    "\n",
    "for record in data:\n",
    "  file.write(\"%s\\n\" %record)\n",
    "\n",
    "api_df = spark.read.format('json').load('/content/sample_data/apidata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcc65c2f-f397-4212-b9af-f883cfaed070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(type(api.json()))\n",
    "print(len(api.json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c3c6405-c8a6-4b92-a678-cb49bf64ce7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "api_df.show(10)\n",
    "api_df.printSchema()\n",
    "api_df.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame_2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
