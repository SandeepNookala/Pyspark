{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a43d2d-b9ea-4e12-9960-e3b78ccf75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Creating Pyspark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7b6dfb-0d02-4603-b733-19c1f0591a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Sandeep.bbrouter:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "\n",
    "#Create SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7faa8657-c6c6-4db7-991c-a555e4a1b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#2.creating lists\n",
    "list1 = [ i for i in range(1,10)]\n",
    "\n",
    "\n",
    "print(list1)\n",
    "print(type(list1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f5ea0b-08b9-4ecd-9c0f-b1896f5f87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Creating RDD from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5462060-5585-4b11-af81-9902ab288d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallelize method is used to create a (RDD) from an existing or local collection (like a Python list or tuple) in the driver program.Used for testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a88a3aa-85b8-4bed-807e-9d5b340b1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "rdd_type <class 'pyspark.core.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(list1)\n",
    "print('rdd:',rdd.collect())\n",
    "print('rdd_type',type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfab9bc2-bca8-48e0-8c0b-cfe908fd8a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.core.rdd object:\n",
      "\n",
      "class RDD(typing.Generic)\n",
      " |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
      " |  \n",
      " |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n",
      " |  Represents an immutable, partitioned collection of elements that can be\n",
      " |  operated on in parallel.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RDD\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> (rdd + rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  __getnewargs__(self) -> NoReturn\n",
      " |  \n",
      " |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given combine functions and a neutral \"zero\n",
      " |      value.\"\n",
      " |      \n",
      " |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      The first function (seqOp) can return a different result type, U, than\n",
      " |      the type of this RDD. Thus, we need one operation for merging a T into\n",
      " |      an U and one operation for merging two U\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : U\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      seqOp : function\n",
      " |          a function used to accumulate results within a partition\n",
      " |      combOp : function\n",
      " |          an associative function used to combine results from different partitions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      U\n",
      " |          the aggregated result\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduce`\n",
      " |      :meth:`RDD.fold`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (10, 4)\n",
      " |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
      " |      (0, 0)\n",
      " |  \n",
      " |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x000001CB45A42200>) -> 'RDD[Tuple[K, U]]'\n",
      " |      Aggregate the values of each key, using given combine functions and a neutral\n",
      " |      \"zero value\". This function can return a different result type, U, than the type\n",
      " |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
      " |      a U and one operation for merging two U's, The former operation is used for merging\n",
      " |      values within a partition, and the latter is used for merging values between\n",
      " |      partitions. To avoid memory allocation, both of these functions are\n",
      " |      allowed to modify and return their first argument instead of creating a new U.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : U\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      seqFunc : function\n",
      " |          a function to merge a V into a U\n",
      " |      combFunc : function\n",
      " |          a function to combine two U's into a single one\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n",
      " |      >>> combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
      " |      >>> sorted(rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect())\n",
      " |      [('a', (3, 2)), ('b', (1, 1))]\n",
      " |  \n",
      " |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n",
      " |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n",
      " |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n",
      " |      entire stage and relaunch all tasks for this stage.\n",
      " |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n",
      " |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDDBarrier`\n",
      " |          instance that provides actions within a barrier stage.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :class:`pyspark.BarrierTaskContext`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For additional information see\n",
      " |      \n",
      " |      - `SPIP: Barrier Execution Mode <https://issues.apache.org/jira/browse/SPARK-24374>`_\n",
      " |      - `Design Doc <https://issues.apache.org/jira/browse/SPARK-24582>`_\n",
      " |      \n",
      " |      This API is experimental\n",
      " |  \n",
      " |  cache(self: 'RDD[T]') -> 'RDD[T]'\n",
      " |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          The same :class:`RDD` with storage level set to `MEMORY_ONLY`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.persist`\n",
      " |      :meth:`RDD.unpersist`\n",
      " |      :meth:`RDD.getStorageLevel`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd2 = rdd.cache()\n",
      " |      >>> rdd2 is rdd\n",
      " |      True\n",
      " |      >>> str(rdd.getStorageLevel())\n",
      " |      'Memory Serialized 1x Replicated'\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |  \n",
      " |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
      " |      Return the Cartesian product of this RDD and another one, that is, the\n",
      " |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n",
      " |      ``b`` is in `other`.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the Cartesian product of this :class:`RDD` and another one\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`pyspark.sql.DataFrame.crossJoin`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> sorted(rdd.cartesian(rdd).collect())\n",
      " |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      " |  \n",
      " |  checkpoint(self) -> None\n",
      " |      Mark this RDD for checkpointing. It will be saved to a file inside the\n",
      " |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n",
      " |      all references to its parent RDDs will be removed. This function must\n",
      " |      be called before any job has been executed on this RDD. It is strongly\n",
      " |      recommended that this RDD is persisted in memory, otherwise saving it\n",
      " |      on a file will require recomputation.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.isCheckpointed`\n",
      " |      :meth:`RDD.getCheckpointFile`\n",
      " |      :meth:`RDD.localCheckpoint`\n",
      " |      :meth:`SparkContext.setCheckpointDir`\n",
      " |      :meth:`SparkContext.getCheckpointDir`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.is_checkpointed\n",
      " |      False\n",
      " |      >>> rdd.getCheckpointFile() == None\n",
      " |      True\n",
      " |      \n",
      " |      >>> rdd.checkpoint()\n",
      " |      >>> rdd.is_checkpointed\n",
      " |      True\n",
      " |      >>> rdd.getCheckpointFile() == None\n",
      " |      True\n",
      " |      \n",
      " |      >>> rdd.count()\n",
      " |      5\n",
      " |      >>> rdd.is_checkpointed\n",
      " |      True\n",
      " |      >>> rdd.getCheckpointFile() == None\n",
      " |      False\n",
      " |  \n",
      " |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n",
      " |      Removes an RDD's shuffles and it's non-persisted ancestors.\n",
      " |      \n",
      " |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n",
      " |      If you use the RDD after this call, you should checkpoint and materialize it first.\n",
      " |      \n",
      " |      .. versionadded:: 3.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool, optional, default False\n",
      " |         whether to block on shuffle cleanup tasks\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is a developer API.\n",
      " |  \n",
      " |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n",
      " |      Return a new RDD that is reduced into `numPartitions` partitions.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      shuffle : bool, optional, default False\n",
      " |          whether to add a shuffle step\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` that is reduced into `numPartitions` partitions\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.repartition`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
      " |      [[1], [2, 3], [4, 5]]\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
      " |      [[1, 2, 3, 4, 5]]\n",
      " |  \n",
      " |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n",
      " |      For each key k in `self` or `other`, return a resulting RDD that\n",
      " |      contains a tuple with the list of values for that key in `self` as\n",
      " |      well as `other`.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and cogrouped values\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.groupWith`\n",
      " |      :meth:`RDD.join`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n",
      " |      [('a', ([1], [2])), ('b', ([4], []))]\n",
      " |  \n",
      " |  collect(self: 'RDD[T]') -> List[~T]\n",
      " |      Return a list that contains all the elements in this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          a list containing all the elements\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.toLocalIterator`\n",
      " |      :meth:`pyspark.sql.DataFrame.collect`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.range(5).collect()\n",
      " |      [0, 1, 2, 3, 4]\n",
      " |      >>> sc.parallelize([\"x\", \"y\", \"z\"]).collect()\n",
      " |      ['x', 'y', 'z']\n",
      " |  \n",
      " |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n",
      " |      Return the key-value pairs in this RDD to the master as a dictionary.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`dict`\n",
      " |          a dictionary of (key, value) pairs\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.countByValue`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting data is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
      " |      >>> m[1]\n",
      " |      2\n",
      " |      >>> m[3]\n",
      " |      4\n",
      " |  \n",
      " |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n",
      " |      When collect rdd, use this method to specify job group.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      .. deprecated:: 3.1.0\n",
      " |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      groupId : str\n",
      " |          The group ID to assign.\n",
      " |      description : str\n",
      " |          The description to set for the job group.\n",
      " |      interruptOnCancel : bool, optional, default False\n",
      " |          whether to interrupt jobs on job cancellation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          a list containing all the elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collect`\n",
      " |      :meth:`SparkContext.setJobGroup`\n",
      " |  \n",
      " |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x000001CB45A42200>) -> 'RDD[Tuple[K, U]]'\n",
      " |      Generic function to combine the elements for each key using a custom\n",
      " |      set of aggregation functions.\n",
      " |      \n",
      " |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
      " |      type\" C.\n",
      " |      \n",
      " |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n",
      " |      modify and return their first argument instead of creating a new C.\n",
      " |      \n",
      " |      In addition, users can control the partitioning of the output RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      createCombiner : function\n",
      " |          a function to turns a V into a C\n",
      " |      mergeValue : function\n",
      " |          a function to merge a V into a C\n",
      " |      mergeCombiners : function\n",
      " |          a function to combine two C's into a single one\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      V and C can be different -- for example, one might group an RDD of type\n",
      " |          (Int, Int) into an RDD of type (Int, List[Int]).\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      " |      >>> def to_list(a):\n",
      " |      ...     return [a]\n",
      " |      ...\n",
      " |      >>> def append(a, b):\n",
      " |      ...     a.append(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> def extend(a, b):\n",
      " |      ...     a.extend(b)\n",
      " |      ...     return a\n",
      " |      ...\n",
      " |      >>> sorted(rdd.combineByKey(to_list, append, extend).collect())\n",
      " |      [('a', [1, 2]), ('b', [1])]\n",
      " |  \n",
      " |  count(self) -> int\n",
      " |      Return the number of elements in this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          the number of elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.countApprox`\n",
      " |      :meth:`pyspark.sql.DataFrame.count`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4]).count()\n",
      " |      3\n",
      " |  \n",
      " |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n",
      " |      Approximate version of count() that returns a potentially incomplete\n",
      " |      result within a timeout, even if not all tasks have finished.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout : int\n",
      " |          maximum time to wait for the job, in milliseconds\n",
      " |      confidence : float\n",
      " |          the desired statistical confidence in the result\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          a potentially incomplete result, with error bounds\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.count`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> rdd.countApprox(1000, 1.0)\n",
      " |      1000\n",
      " |  \n",
      " |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n",
      " |      Return approximate number of distinct elements in the RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      relativeSD : float, optional\n",
      " |          Relative accuracy. Smaller values create\n",
      " |          counters that require more space.\n",
      " |          It must be greater than 0.000017.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          approximate number of distinct elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.distinct`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The algorithm used is based on streamlib's implementation of\n",
      " |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
      " |      of The Art Cardinality Estimation Algorithm\", available here\n",
      " |      <https://doi.org/10.1145/2452376.2452456>`_.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
      " |      >>> 900 < n < 1100\n",
      " |      True\n",
      " |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
      " |      >>> 16 < n < 24\n",
      " |      True\n",
      " |  \n",
      " |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n",
      " |      Count the number of elements for each key, and return the result to the\n",
      " |      master as a dictionary.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          a dictionary of (key, count) pairs\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collectAsMap`\n",
      " |      :meth:`RDD.countByValue`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.countByKey().items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n",
      " |      Return the count of each unique value in this RDD as a dictionary of\n",
      " |      (value, count) pairs.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          a dictionary of (value, count) pairs\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collectAsMap`\n",
      " |      :meth:`RDD.countByKey`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
      " |      [(1, 2), (2, 3)]\n",
      " |  \n",
      " |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Return a new RDD containing the distinct elements in this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` containing the distinct elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.countApproxDistinct`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n",
      " |      Return a new RDD containing only the elements that satisfy a predicate.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each element of the RDD\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each element\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
      " |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  first(self: 'RDD[T]') -> ~T\n",
      " |      Return the first element in this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the first element\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.take`\n",
      " |      :meth:`pyspark.sql.DataFrame.first`\n",
      " |      :meth:`pyspark.sql.DataFrame.head`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4]).first()\n",
      " |      2\n",
      " |      >>> sc.parallelize([]).first()\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: RDD is empty\n",
      " |  \n",
      " |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by first applying a function to all elements of this\n",
      " |      RDD, and then flattening the results.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to turn a T into a sequence of U\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |          function doesn't modify the keys\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([2, 3, 4])\n",
      " |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
      " |      [1, 1, 1, 2, 2, 3]\n",
      " |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
      " |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
      " |  \n",
      " |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n",
      " |      Pass each value in the key-value pair RDD through a flatMap function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |         a function to turn a V into a sequence of U\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the flat-mapped value\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapValues`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
      " |      >>> def f(x): return x\n",
      " |      ...\n",
      " |      >>> rdd.flatMapValues(f).collect()\n",
      " |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
      " |  \n",
      " |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n",
      " |      Aggregate the elements of each partition, and then the results for all\n",
      " |      the partitions, using a given associative function and a neutral \"zero value.\"\n",
      " |      \n",
      " |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n",
      " |      as its result value to avoid object allocation; however, it should not\n",
      " |      modify ``t2``.\n",
      " |      \n",
      " |      This behaves somewhat differently from fold operations implemented\n",
      " |      for non-distributed collections in functional languages like Scala.\n",
      " |      This fold operation may be applied to partitions individually, and then\n",
      " |      fold those results into the final result, rather than apply the fold\n",
      " |      to each element sequentially in some defined ordering. For functions\n",
      " |      that are not commutative, the result may differ from that of a fold\n",
      " |      applied to a non-distributed collection.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : T\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      op : function\n",
      " |          a function used to both accumulate results within a partition and combine\n",
      " |          results from different partitions\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the aggregated result\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduce`\n",
      " |      :meth:`RDD.aggregate`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
      " |      15\n",
      " |  \n",
      " |  foldByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~V, func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x000001CB45A42200>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Merge the values for each key using an associative function \"func\"\n",
      " |      and a neutral \"zeroValue\" which may be added to the result an\n",
      " |      arbitrary number of times, and must not change the result\n",
      " |      (e.g., 0 for addition, or 1 for multiplication.).\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : V\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      func : function\n",
      " |          a function to combine two V's into a single one\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> from operator import add\n",
      " |      >>> sorted(rdd.foldByKey(0, add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  foreach(self: 'RDD[T]', f: Callable[[~T], NoneType]) -> None\n",
      " |      Applies a function to all elements of this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function applied to each element\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.foreachPartition`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreach`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(x): print(x)\n",
      " |      ...\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
      " |  \n",
      " |  foreachPartition(self: 'RDD[T]', f: Callable[[Iterable[~T]], NoneType]) -> None\n",
      " |      Applies a function to each partition of this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function applied to each partition\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.foreach`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreach`\n",
      " |      :meth:`pyspark.sql.DataFrame.foreachPartition`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> def f(iterator):\n",
      " |      ...     for x in iterator:\n",
      " |      ...          print(x)\n",
      " |      ...\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
      " |  \n",
      " |  fullOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], Optional[U]]]]'\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Similarly, for each element (k, w) in `other`, the resulting RDD will\n",
      " |      either contain all pairs (k, (v, w)) for v in `self`, or the pair\n",
      " |      (k, (None, w)) if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.join`\n",
      " |      :meth:`RDD.leftOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
      " |      >>> sorted(rdd1.fullOuterJoin(rdd2).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
      " |  \n",
      " |  getCheckpointFile(self) -> Optional[str]\n",
      " |      Gets the name of the file to which this RDD was checkpointed\n",
      " |      \n",
      " |      Not defined if RDD is checkpointed locally.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          the name of the file to which this :class:`RDD` was checkpointed\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`SparkContext.setCheckpointDir`\n",
      " |      :meth:`SparkContext.getCheckpointDir`\n",
      " |  \n",
      " |  getNumPartitions(self) -> int\n",
      " |      Returns the number of partitions in RDD\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          number of partitions\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> rdd.getNumPartitions()\n",
      " |      2\n",
      " |  \n",
      " |  getResourceProfile(self) -> Optional[pyspark.resource.profile.ResourceProfile]\n",
      " |      Get the :class:`pyspark.resource.ResourceProfile` specified with this RDD or None\n",
      " |      if it wasn't specified.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      class:`pyspark.resource.ResourceProfile`\n",
      " |          The user specified profile or None if none were specified\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.withResources`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |  \n",
      " |  getStorageLevel(self) -> pyspark.storagelevel.StorageLevel\n",
      " |      Get the RDD's current storage level.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StorageLevel`\n",
      " |          current :class:`StorageLevel`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.name`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1,2])\n",
      " |      >>> rdd.getStorageLevel()\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> print(rdd.getStorageLevel())\n",
      " |      Serialized 1x Replicated\n",
      " |  \n",
      " |  glom(self: 'RDD[T]') -> 'RDD[List[T]]'\n",
      " |      Return an RDD created by coalescing all elements within each partition\n",
      " |      into a list.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` coalescing all elements within each partition into a list\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> sorted(rdd.glom().collect())\n",
      " |      [[1, 2], [3, 4]]\n",
      " |  \n",
      " |  groupBy(self: 'RDD[T]', f: Callable[[~T], ~K], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x000001CB45A42200>) -> 'RDD[Tuple[K, Iterable[T]]]'\n",
      " |      Return an RDD of grouped items.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to compute the key\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          a function to compute the partition index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` of grouped items\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |      :meth:`pyspark.sql.DataFrame.groupBy`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
      " |      >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
      " |      >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
      " |      [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
      " |  \n",
      " |  groupByKey(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x000001CB45A42200>) -> 'RDD[Tuple[K, Iterable[V]]]'\n",
      " |      Group the values for each key in the RDD into a single sequence.\n",
      " |      Hash-partitions the resulting RDD with numPartitions partitions.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the grouped result for each key\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you are grouping in order to perform an aggregation (such as a\n",
      " |      sum or average) over each key, using reduceByKey or aggregateByKey will\n",
      " |      provide much better performance.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(len).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |      >>> sorted(rdd.groupByKey().mapValues(list).collect())\n",
      " |      [('a', [1, 1]), ('b', [1])]\n",
      " |  \n",
      " |  groupWith(self: 'RDD[Tuple[Any, Any]]', other: 'RDD[Tuple[Any, Any]]', *others: 'RDD[Tuple[Any, Any]]') -> 'RDD[Tuple[Any, Tuple[ResultIterable[Any], ...]]]'\n",
      " |      Alias for cogroup but with support for multiple RDDs.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      others : :class:`RDD`\n",
      " |          other :class:`RDD`\\s\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and cogrouped values\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.cogroup`\n",
      " |      :meth:`RDD.join`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd3 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> rdd4 = sc.parallelize([(\"b\", 42)])\n",
      " |      >>> [(x, tuple(map(list, y))) for x, y in\n",
      " |      ...     sorted(list(rdd1.groupWith(rdd2, rdd3, rdd4).collect()))]\n",
      " |      [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
      " |  \n",
      " |  histogram(self: 'RDD[S]', buckets: Union[int, List[ForwardRef('S')], Tuple[ForwardRef('S'), ...]]) -> Tuple[Sequence[ForwardRef('S')], List[int]]\n",
      " |      Compute a histogram using the provided buckets. The buckets\n",
      " |      are all open to the right except for the last which is closed.\n",
      " |      e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
      " |      which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
      " |      and 50 we would have a histogram of 1,0,1.\n",
      " |      \n",
      " |      If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
      " |      this can be switched from an O(log n) insertion to O(1) per\n",
      " |      element (where n is the number of buckets).\n",
      " |      \n",
      " |      Buckets must be sorted, not contain any duplicates, and have\n",
      " |      at least two elements.\n",
      " |      \n",
      " |      If `buckets` is a number, it will generate buckets which are\n",
      " |      evenly spaced between the minimum and maximum of the RDD. For\n",
      " |      example, if the min value is 0 and the max is 100, given `buckets`\n",
      " |      as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
      " |      be at least 1. An exception is raised if the RDD contains infinity.\n",
      " |      If the elements in the RDD do not vary (max == min), a single bucket\n",
      " |      will be used.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      buckets : int, or list, or tuple\n",
      " |          if `buckets` is a number, it computes a histogram of the data using\n",
      " |          `buckets` number of buckets evenly, otherwise, `buckets` is the provided\n",
      " |          buckets to bin the data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tuple\n",
      " |          a tuple of buckets and histogram\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(51))\n",
      " |      >>> rdd.histogram(2)\n",
      " |      ([0, 25, 50], [25, 26])\n",
      " |      >>> rdd.histogram([0, 5, 25, 50])\n",
      " |      ([0, 5, 25, 50], [5, 20, 26])\n",
      " |      >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
      " |      ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
      " |      >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
      " |      >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
      " |      (('a', 'b', 'c'), [2, 2])\n",
      " |  \n",
      " |  id(self) -> int\n",
      " |      A unique ID for this RDD (within its SparkContext).\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          The unique ID for this :class:`RDD`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.id()  # doctest: +SKIP\n",
      " |      3\n",
      " |  \n",
      " |  intersection(self: 'RDD[T]', other: 'RDD[T]') -> 'RDD[T]'\n",
      " |      Return the intersection of this RDD and another one. The output will\n",
      " |      not contain any duplicate elements, even if the input RDDs did.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the intersection of this :class:`RDD` and another one\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`pyspark.sql.DataFrame.intersect`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method performs a shuffle internally.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
      " |      >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
      " |      >>> rdd1.intersection(rdd2).collect()\n",
      " |      [1, 2, 3]\n",
      " |  \n",
      " |  isCheckpointed(self) -> bool\n",
      " |      Return whether this RDD is checkpointed and materialized, either reliably or locally.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          whether this :class:`RDD` is checkpointed and materialized, either reliably or locally\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`RDD.getCheckpointFile`\n",
      " |      :meth:`SparkContext.setCheckpointDir`\n",
      " |      :meth:`SparkContext.getCheckpointDir`\n",
      " |  \n",
      " |  isEmpty(self) -> bool\n",
      " |      Returns true if and only if the RDD contains no elements at all.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          whether the :class:`RDD` is empty\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.first`\n",
      " |      :meth:`pyspark.sql.DataFrame.isEmpty`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      An RDD may be empty even when it has at least 1 partition.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([]).isEmpty()\n",
      " |      True\n",
      " |      >>> sc.parallelize([1]).isEmpty()\n",
      " |      False\n",
      " |  \n",
      " |  isLocallyCheckpointed(self) -> bool\n",
      " |      Return whether this RDD is marked for local checkpointing.\n",
      " |      \n",
      " |      Exposed for testing.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool\n",
      " |          whether this :class:`RDD` is marked for local checkpointing\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.localCheckpoint`\n",
      " |  \n",
      " |  join(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, U]]]'\n",
      " |      Return an RDD containing all pairs of elements with matching keys in\n",
      " |      `self` and `other`.\n",
      " |      \n",
      " |      Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
      " |      (k, v1) is in `self` and (k, v2) is in `other`.\n",
      " |      \n",
      " |      Performs a hash join across the cluster.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.leftOuterJoin`\n",
      " |      :meth:`RDD.rightOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`RDD.cogroup`\n",
      " |      :meth:`RDD.groupWith`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
      " |      >>> sorted(rdd1.join(rdd2).collect())\n",
      " |      [('a', (1, 2)), ('a', (1, 3))]\n",
      " |  \n",
      " |  keyBy(self: 'RDD[T]', f: Callable[[~T], ~K]) -> 'RDD[Tuple[K, T]]'\n",
      " |      Creates tuples of the elements in this RDD by applying `f`.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to compute the key\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with the elements from this that are not in `other`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.keys`\n",
      " |      :meth:`RDD.values`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
      " |      >>> rdd2 = sc.parallelize(zip(range(0,5), range(0,5)))\n",
      " |      >>> [(x, list(map(list, y))) for x, y in sorted(rdd1.cogroup(rdd2).collect())]\n",
      " |      [(0, [[0], [0]]), (1, [[1], [1]]), (2, [[], [2]]), (3, [[], [3]]), (4, [[2], [4]])]\n",
      " |  \n",
      " |  keys(self: 'RDD[Tuple[K, V]]') -> 'RDD[K]'\n",
      " |      Return an RDD with the keys of each tuple.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` only containing the keys\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.values`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
      " |      >>> rdd.collect()\n",
      " |      [1, 3]\n",
      " |  \n",
      " |  leftOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[V, Optional[U]]]]'\n",
      " |      Perform a left outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, v) in `self`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for w in `other`, or the pair\n",
      " |      (k, (v, None)) if no elements in `other` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.join`\n",
      " |      :meth:`RDD.rightOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(rdd1.leftOuterJoin(rdd2).collect())\n",
      " |      [('a', (1, 2)), ('b', (4, None))]\n",
      " |  \n",
      " |  localCheckpoint(self) -> None\n",
      " |      Mark this RDD for local checkpointing using Spark's existing caching layer.\n",
      " |      \n",
      " |      This method is for users who wish to truncate RDD lineages while skipping the expensive\n",
      " |      step of replicating the materialized data in a reliable distributed file system. This is\n",
      " |      useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).\n",
      " |      \n",
      " |      Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed\n",
      " |      data is written to ephemeral local storage in the executors instead of to a reliable,\n",
      " |      fault-tolerant storage. The effect is that if an executor fails during the computation,\n",
      " |      the checkpointed data may no longer be accessible, causing an irrecoverable job failure.\n",
      " |      \n",
      " |      This is NOT safe to use with dynamic allocation, which removes executors along\n",
      " |      with their cached blocks. If you must use both features, you are advised to set\n",
      " |      `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.\n",
      " |      \n",
      " |      The checkpoint directory set through :meth:`SparkContext.setCheckpointDir` is not used.\n",
      " |      \n",
      " |      .. versionadded:: 2.2.0\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.checkpoint`\n",
      " |      :meth:`RDD.isLocallyCheckpointed`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.isLocallyCheckpointed()\n",
      " |      False\n",
      " |      \n",
      " |      >>> rdd.localCheckpoint()\n",
      " |      >>> rdd.isLocallyCheckpointed()\n",
      " |      True\n",
      " |  \n",
      " |  lookup(self: 'RDD[Tuple[K, V]]', key: ~K) -> List[~V]\n",
      " |      Return the list of values in the RDD for key `key`. This operation\n",
      " |      is done efficiently if the RDD has a known partitioner by only\n",
      " |      searching the partition that the key maps to.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : K\n",
      " |          the key to look up\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the list of values in the :class:`RDD` for key `key`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> l = range(1000)\n",
      " |      >>> rdd = sc.parallelize(zip(l, l), 10)\n",
      " |      >>> rdd.lookup(42)  # slow\n",
      " |      [42]\n",
      " |      >>> sorted = rdd.sortByKey()\n",
      " |      >>> sorted.lookup(42)  # fast\n",
      " |      [42]\n",
      " |      >>> sorted.lookup(1024)\n",
      " |      []\n",
      " |      >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n",
      " |      >>> list(rdd2.lookup(('a', 'b'))[0])\n",
      " |      ['c']\n",
      " |  \n",
      " |  map(self: 'RDD[T]', f: Callable[[~T], ~U], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each element of this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each element of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |          function doesn't modify the keys\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
      " |      [('a', 1), ('b', 1), ('c', 1)]\n",
      " |  \n",
      " |  mapPartitions(self: 'RDD[T]', f: Callable[[Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each partition of this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each partition of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |          function doesn't modify the keys\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each partition\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |      :meth:`RDDBarrier.mapPartitions`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
      " |      >>> def f(iterator): yield sum(iterator)\n",
      " |      ...\n",
      " |      >>> rdd.mapPartitions(f).collect()\n",
      " |      [3, 7]\n",
      " |  \n",
      " |  mapPartitionsWithIndex(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each partition of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |          function doesn't modify the keys\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each partition\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithSplit`\n",
      " |      :meth:`RDDBarrier.mapPartitionsWithIndex`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      ...\n",
      " |      >>> rdd.mapPartitionsWithIndex(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapPartitionsWithSplit(self: 'RDD[T]', f: Callable[[int, Iterable[~T]], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n",
      " |      Return a new RDD by applying a function to each partition of this RDD,\n",
      " |      while tracking the index of the original partition.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      .. deprecated:: 0.9.0\n",
      " |          use meth:`RDD.mapPartitionsWithIndex` instead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          a function to run on each partition of the RDD\n",
      " |      preservesPartitioning : bool, optional, default False\n",
      " |          indicates whether the input function preserves the partitioner,\n",
      " |          which should be False unless this is a pair RDD and the input\n",
      " |          function doesn't modify the keys\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` by applying a function to each partition\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMap`\n",
      " |      :meth:`RDD.mapPartitions`\n",
      " |      :meth:`RDD.mapPartitionsWithIndex`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
      " |      >>> def f(splitIndex, iterator): yield splitIndex\n",
      " |      ...\n",
      " |      >>> rdd.mapPartitionsWithSplit(f).sum()\n",
      " |      6\n",
      " |  \n",
      " |  mapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], ~U]) -> 'RDD[Tuple[K, U]]'\n",
      " |      Pass each value in the key-value pair RDD through a map function\n",
      " |      without changing the keys; this also retains the original RDD's\n",
      " |      partitioning.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |         a function to turn a V into a U\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the mapped value\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.map`\n",
      " |      :meth:`RDD.flatMapValues`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
      " |      >>> def f(x): return len(x)\n",
      " |      ...\n",
      " |      >>> rdd.mapValues(f).collect()\n",
      " |      [('a', 3), ('b', 1)]\n",
      " |  \n",
      " |  max(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
      " |      Find the maximum item in this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : function, optional\n",
      " |          A function used to generate key for comparing\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the maximum item\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.min`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.max()\n",
      " |      43.0\n",
      " |      >>> rdd.max(key=str)\n",
      " |      5.0\n",
      " |  \n",
      " |  mean(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the mean of this RDD's elements.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the mean of all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.sum`\n",
      " |      :meth:`RDD.meanApprox`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).mean()\n",
      " |      2.0\n",
      " |  \n",
      " |  meanApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.core.rdd.BoundedFloat\n",
      " |      Approximate operation to return the mean within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout : int\n",
      " |          maximum time to wait for the job, in milliseconds\n",
      " |      confidence : float\n",
      " |          the desired statistical confidence in the result\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`BoundedFloat`\n",
      " |          a potentially incomplete result, with error bounds\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.mean`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000)) / 1000.0\n",
      " |      >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  min(self: 'RDD[T]', key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> ~T\n",
      " |      Find the minimum item in this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : function, optional\n",
      " |          A function used to generate key for comparing\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the minimum item\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.max`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
      " |      >>> rdd.min()\n",
      " |      2.0\n",
      " |      >>> rdd.min(key=str)\n",
      " |      10.0\n",
      " |  \n",
      " |  name(self) -> Optional[str]\n",
      " |      Return the name of this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          :class:`RDD` name\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.setName`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.name() == None\n",
      " |      True\n",
      " |  \n",
      " |  partitionBy(self: 'RDD[Tuple[K, V]]', numPartitions: Optional[int], partitionFunc: Callable[[~K], int] = <function portable_hash at 0x000001CB45A42200>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Return a copy of the RDD partitioned using the specified partitioner.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` partitioned using the specified partitioner\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.repartition`\n",
      " |      :meth:`RDD.repartitionAndSortWithinPartitions`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
      " |      >>> sets = pairs.partitionBy(2).glom().collect()\n",
      " |      >>> len(set(sets[0]).intersection(set(sets[1])))\n",
      " |      0\n",
      " |  \n",
      " |  persist(self: 'RDD[T]', storageLevel: pyspark.storagelevel.StorageLevel = StorageLevel(False, True, False, False, 1)) -> 'RDD[T]'\n",
      " |      Set this RDD's storage level to persist its values across operations\n",
      " |      after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the RDD does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (`MEMORY_ONLY`).\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      storageLevel : :class:`StorageLevel`, default `MEMORY_ONLY`\n",
      " |          the target storage level\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          The same :class:`RDD` with storage level set to `storageLevel`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.cache`\n",
      " |      :meth:`RDD.unpersist`\n",
      " |      :meth:`RDD.getStorageLevel`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
      " |      >>> rdd.persist().is_cached\n",
      " |      True\n",
      " |      >>> str(rdd.getStorageLevel())\n",
      " |      'Memory Serialized 1x Replicated'\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |      \n",
      " |      >>> from pyspark import StorageLevel\n",
      " |      >>> rdd2 = sc.range(5)\n",
      " |      >>> _ = rdd2.persist(StorageLevel.MEMORY_AND_DISK)\n",
      " |      >>> rdd2.is_cached\n",
      " |      True\n",
      " |      >>> str(rdd2.getStorageLevel())\n",
      " |      'Disk Memory Serialized 1x Replicated'\n",
      " |      \n",
      " |      Can not override existing storage level\n",
      " |      \n",
      " |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      py4j.protocol.Py4JJavaError: ...\n",
      " |      \n",
      " |      Assign another storage level after `unpersist`\n",
      " |      \n",
      " |      >>> _ = rdd2.unpersist()\n",
      " |      >>> rdd2.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd2.persist(StorageLevel.MEMORY_ONLY_2)\n",
      " |      >>> str(rdd2.getStorageLevel())\n",
      " |      'Memory Serialized 2x Replicated'\n",
      " |      >>> rdd2.is_cached\n",
      " |      True\n",
      " |      >>> _ = rdd2.unpersist()\n",
      " |  \n",
      " |  pipe(self, command: str, env: Optional[Dict[str, str]] = None, checkCode: bool = False) -> 'RDD[str]'\n",
      " |      Return an RDD created by piping elements to a forked external process.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      command : str\n",
      " |          command to run.\n",
      " |      env : dict, optional\n",
      " |          environment variables to set.\n",
      " |      checkCode : bool, optional\n",
      " |          whether to check the return value of the shell command.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` of strings\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
      " |      ['1', '2', '', '3']\n",
      " |  \n",
      " |  randomSplit(self: 'RDD[T]', weights: Sequence[Union[int, float]], seed: Optional[int] = None) -> 'List[RDD[T]]'\n",
      " |      Randomly splits this RDD with the provided weights.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      weights : list\n",
      " |          weights for splits, will be normalized if they don't sum to 1\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          split :class:`RDD`\\s in a list\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`pyspark.sql.DataFrame.randomSplit`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(500), 1)\n",
      " |      >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
      " |      >>> len(rdd1.collect() + rdd2.collect())\n",
      " |      500\n",
      " |      >>> 150 < rdd1.count() < 250\n",
      " |      True\n",
      " |      >>> 250 < rdd2.count() < 350\n",
      " |      True\n",
      " |  \n",
      " |  reduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T]) -> ~T\n",
      " |      Reduces the elements of this RDD using the specified commutative and\n",
      " |      associative binary operator. Currently reduces partitions locally.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          the reduce function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the aggregated result\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.treeReduce`\n",
      " |      :meth:`RDD.aggregate`\n",
      " |      :meth:`RDD.treeAggregate`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
      " |      15\n",
      " |      >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
      " |      10\n",
      " |      >>> sc.parallelize([]).reduce(add)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Can not reduce() empty RDD\n",
      " |  \n",
      " |  reduceByKey(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x000001CB45A42200>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Merge the values for each key using an associative and commutative reduce function.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      Output will be partitioned with `numPartitions` partitions, or\n",
      " |      the default parallelism level if `numPartitions` is not specified.\n",
      " |      Default partitioner is hash-partition.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          the reduce function\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          function to compute the partition index\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the keys and the aggregated result for each key\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKeyLocally`\n",
      " |      :meth:`RDD.combineByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      :meth:`RDD.foldByKey`\n",
      " |      :meth:`RDD.groupByKey`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKey(add).collect())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  reduceByKeyLocally(self: 'RDD[Tuple[K, V]]', func: Callable[[~V, ~V], ~V]) -> Dict[~K, ~V]\n",
      " |      Merge the values for each key using an associative and commutative reduce function, but\n",
      " |      return the results immediately to the master as a dictionary.\n",
      " |      \n",
      " |      This will also perform the merging locally on each mapper before\n",
      " |      sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : function\n",
      " |          the reduce function\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          a dict containing the keys and the aggregated result for each key\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduceByKey`\n",
      " |      :meth:`RDD.aggregateByKey`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> from operator import add\n",
      " |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
      " |      >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
      " |      [('a', 2), ('b', 1)]\n",
      " |  \n",
      " |  repartition(self: 'RDD[T]', numPartitions: int) -> 'RDD[T]'\n",
      " |       Return a new RDD that has exactly numPartitions partitions.\n",
      " |      \n",
      " |       Can increase or decrease the level of parallelism in this RDD.\n",
      " |       Internally, this uses a shuffle to redistribute data.\n",
      " |       If you are decreasing the number of partitions in this RDD, consider\n",
      " |       using `coalesce`, which can avoid performing a shuffle.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with exactly numPartitions partitions\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.coalesce`\n",
      " |      :meth:`RDD.partitionBy`\n",
      " |      :meth:`RDD.repartitionAndSortWithinPartitions`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |       >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
      " |       >>> sorted(rdd.glom().collect())\n",
      " |       [[1], [2, 3], [4, 5], [6, 7]]\n",
      " |       >>> len(rdd.repartition(2).glom().collect())\n",
      " |       2\n",
      " |       >>> len(rdd.repartition(10).glom().collect())\n",
      " |       10\n",
      " |  \n",
      " |  repartitionAndSortWithinPartitions(self: 'RDD[Tuple[Any, Any]]', numPartitions: Optional[int] = None, partitionFunc: Callable[[Any], int] = <function portable_hash at 0x000001CB45A42200>, ascending: bool = True, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x000001CB45A99480>) -> 'RDD[Tuple[Any, Any]]'\n",
      " |      Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
      " |      sort records by their keys.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      partitionFunc : function, optional, default `portable_hash`\n",
      " |          a function to compute the partition index\n",
      " |      ascending : bool, optional, default True\n",
      " |          sort the keys in ascending or descending order\n",
      " |      keyfunc : function, optional, default identity mapping\n",
      " |          a function to compute the key\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.repartition`\n",
      " |      :meth:`RDD.partitionBy`\n",
      " |      :meth:`RDD.sortBy`\n",
      " |      :meth:`RDD.sortByKey`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
      " |      >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\n",
      " |      >>> rdd2.glom().collect()\n",
      " |      [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
      " |  \n",
      " |  rightOuterJoin(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[Optional[V], U]]]'\n",
      " |      Perform a right outer join of `self` and `other`.\n",
      " |      \n",
      " |      For each element (k, w) in `other`, the resulting RDD will either\n",
      " |      contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
      " |      if no elements in `self` have key k.\n",
      " |      \n",
      " |      Hash-partitions the resulting RDD into the given number of partitions.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing all pairs of elements with matching keys\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.join`\n",
      " |      :meth:`RDD.leftOuterJoin`\n",
      " |      :meth:`RDD.fullOuterJoin`\n",
      " |      :meth:`pyspark.sql.DataFrame.join`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n",
      " |      >>> sorted(rdd2.rightOuterJoin(rdd1).collect())\n",
      " |      [('a', (2, 1)), ('b', (None, 4))]\n",
      " |  \n",
      " |  sample(self: 'RDD[T]', withReplacement: bool, fraction: float, seed: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Return a sampled subset of this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool\n",
      " |          can elements be sampled multiple times (replaced when sampled out)\n",
      " |      fraction : float\n",
      " |          expected size of the sample as a fraction of this RDD's size\n",
      " |          without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
      " |          with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
      " |      seed : int, optional\n",
      " |          seed for the random number generator\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD` containing a sampled subset of elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.takeSample`\n",
      " |      :meth:`RDD.sampleByKey`\n",
      " |      :meth:`pyspark.sql.DataFrame.sample`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |      count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(100), 4)\n",
      " |      >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
      " |      True\n",
      " |  \n",
      " |  sampleByKey(self: 'RDD[Tuple[K, V]]', withReplacement: bool, fractions: Dict[~K, Union[float, int]], seed: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
      " |      Return a subset of this RDD sampled by key (via stratified sampling).\n",
      " |      Create a sample of this RDD using variable sampling rates for\n",
      " |      different keys as specified by fractions, a key to sampling rate map.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool\n",
      " |          whether to sample with or without replacement\n",
      " |      fractions : dict\n",
      " |          map of specific keys to sampling rates\n",
      " |      seed : int, optional\n",
      " |          seed for the random number generator\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the stratified sampling result\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sample`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
      " |      >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
      " |      >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
      " |      >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
      " |      True\n",
      " |      >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
      " |      True\n",
      " |      >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
      " |      True\n",
      " |  \n",
      " |  sampleStdev(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the sample standard deviation of this RDD's elements (which\n",
      " |      corrects for bias in estimating the standard deviation by dividing by\n",
      " |      N-1 instead of N).\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the sample standard deviation of all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
      " |      1.0\n",
      " |  \n",
      " |  sampleVariance(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the sample variance of this RDD's elements (which corrects\n",
      " |      for bias in estimating the variance by dividing by N-1 instead of N).\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the sample variance of all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
      " |      1.0\n",
      " |  \n",
      " |  saveAsHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conf : dict\n",
      " |          Hadoop job configuration\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.hadoopRDD`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsHadoopDataset\") as d:\n",
      " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Create the conf for writing\n",
      " |      ...     write_conf = {\n",
      " |      ...         \"mapred.output.format.class\": output_format_class,\n",
      " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      " |      ...     }\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsHadoopDataset(conf=write_conf)\n",
      " |      ...\n",
      " |      ...     # Create the conf for reading\n",
      " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     loaded = sc.hadoopRDD(input_format_class, key_class, value_class, conf=read_conf)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      " |  \n",
      " |  saveAsHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None, compressionCodecClass: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      outputFormatClass : str\n",
      " |          fully qualified classname of Hadoop OutputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
      " |      keyClass : str, optional\n",
      " |          fully qualified classname of key Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      valueClass : str, optional\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      conf : dict, optional\n",
      " |          (None by default)\n",
      " |      compressionCodecClass : str\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.hadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapred.TextOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapred.TextInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsHadoopFile\") as d:\n",
      " |      ...     path = os.path.join(d, \"old_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsHadoopFile(path, output_format_class, key_class, value_class)\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     loaded = sc.hadoopFile(path, input_format_class, key_class, value_class)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(0, '1\\t'), (0, '1\\ta'), (0, '3\\tx')]\n",
      " |  \n",
      " |  saveAsNewAPIHadoopDataset(self: 'RDD[Tuple[K, V]]', conf: Dict[str, str], keyConverter: Optional[str] = None, valueConverter: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
      " |      converted for output using either user specified converters or, by default,\n",
      " |      \"org.apache.spark.api.python.JavaToWritableConverter\".\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      conf : dict\n",
      " |          Hadoop job configuration\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.newAPIHadoopRDD`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      " |      >>> input_format_class = \"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat\"\n",
      " |      >>> key_class = \"org.apache.hadoop.io.IntWritable\"\n",
      " |      >>> value_class = \"org.apache.hadoop.io.Text\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsNewAPIHadoopDataset\") as d:\n",
      " |      ...     path = os.path.join(d, \"new_hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Create the conf for writing\n",
      " |      ...     write_conf = {\n",
      " |      ...         \"mapreduce.job.outputformat.class\": (output_format_class),\n",
      " |      ...         \"mapreduce.job.output.key.class\": key_class,\n",
      " |      ...         \"mapreduce.job.output.value.class\": value_class,\n",
      " |      ...         \"mapreduce.output.fileoutputformat.outputdir\": path,\n",
      " |      ...     }\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsNewAPIHadoopDataset(conf=write_conf)\n",
      " |      ...\n",
      " |      ...     # Create the conf for reading\n",
      " |      ...     read_conf = {\"mapreduce.input.fileinputformat.inputdir\": path}\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     loaded = sc.newAPIHadoopRDD(input_format_class,\n",
      " |      ...         key_class, value_class, conf=read_conf)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      " |  \n",
      " |  saveAsNewAPIHadoopFile(self: 'RDD[Tuple[K, V]]', path: str, outputFormatClass: str, keyClass: Optional[str] = None, valueClass: Optional[str] = None, keyConverter: Optional[str] = None, valueConverter: Optional[str] = None, conf: Optional[Dict[str, str]] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
      " |      will be inferred if not specified. Keys and values are converted for output using either\n",
      " |      user specified converters or \"org.apache.spark.api.python.JavaToWritableConverter\". The\n",
      " |      `conf` is applied on top of the base Hadoop conf associated with the SparkContext\n",
      " |      of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to Hadoop file\n",
      " |      outputFormatClass : str\n",
      " |          fully qualified classname of Hadoop OutputFormat\n",
      " |          (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
      " |      keyClass : str, optional\n",
      " |          fully qualified classname of key Writable class\n",
      " |           (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
      " |      valueClass : str, optional\n",
      " |          fully qualified classname of value Writable class\n",
      " |          (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
      " |      keyConverter : str, optional\n",
      " |          fully qualified classname of key converter (None by default)\n",
      " |      valueConverter : str, optional\n",
      " |          fully qualified classname of value converter (None by default)\n",
      " |      conf : dict, optional\n",
      " |          Hadoop job configuration (None by default)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.newAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the class of output format\n",
      " |      \n",
      " |      >>> output_format_class = \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\"\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsNewAPIHadoopFile\") as d:\n",
      " |      ...     path = os.path.join(d, \"hadoop_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary Hadoop file\n",
      " |      ...     rdd = sc.parallelize([(1, {3.0: \"bb\"}), (2, {1.0: \"aa\"}), (3, {2.0: \"dd\"})])\n",
      " |      ...     rdd.saveAsNewAPIHadoopFile(path, output_format_class)\n",
      " |      ...\n",
      " |      ...     # Load this Hadoop file as an RDD\n",
      " |      ...     sorted(sc.sequenceFile(path).collect())\n",
      " |      [(1, {3.0: 'bb'}), (2, {1.0: 'aa'}), (3, {2.0: 'dd'})]\n",
      " |  \n",
      " |  saveAsPickleFile(self, path: str, batchSize: int = 10) -> None\n",
      " |      Save this RDD as a SequenceFile of serialized objects. The serializer\n",
      " |      used is :class:`pyspark.serializers.CPickleSerializer`, default batch size\n",
      " |      is 10.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to pickled file\n",
      " |      batchSize : int, optional, default 10\n",
      " |          the number of Python objects represented as a single Java object.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.pickleFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsPickleFile\") as d:\n",
      " |      ...     path = os.path.join(d, \"pickle_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary pickled file\n",
      " |      ...     sc.parallelize(range(10)).saveAsPickleFile(path, 3)\n",
      " |      ...\n",
      " |      ...     # Load picked file as an RDD\n",
      " |      ...     sorted(sc.pickleFile(path, 3).collect())\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  saveAsSequenceFile(self: 'RDD[Tuple[K, V]]', path: str, compressionCodecClass: Optional[str] = None) -> None\n",
      " |      Output a Python RDD of key-value pairs (of form ``RDD[(K, V)]``) to any Hadoop file\n",
      " |      system, using the \"org.apache.hadoop.io.Writable\" types that we convert from the\n",
      " |      RDD's key and value types. The mechanism is as follows:\n",
      " |      \n",
      " |          1. Pickle is used to convert pickled Python RDD into RDD of Java objects.\n",
      " |          2. Keys and values of this Java RDD are converted to Writables and written out.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to sequence file\n",
      " |      compressionCodecClass : str, optional\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.sequenceFile`\n",
      " |      :meth:`RDD.saveAsHadoopFile`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopFile`\n",
      " |      :meth:`RDD.saveAsHadoopDataset`\n",
      " |      :meth:`RDD.saveAsNewAPIHadoopDataset`\n",
      " |      :meth:`RDD.saveAsSequenceFile`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      \n",
      " |      Set the related classes\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsSequenceFile\") as d:\n",
      " |      ...     path = os.path.join(d, \"sequence_file\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary sequence file\n",
      " |      ...     rdd = sc.parallelize([(1, \"\"), (1, \"a\"), (3, \"x\")])\n",
      " |      ...     rdd.saveAsSequenceFile(path)\n",
      " |      ...\n",
      " |      ...     # Load this sequence file as an RDD\n",
      " |      ...     loaded = sc.sequenceFile(path)\n",
      " |      ...     sorted(loaded.collect())\n",
      " |      [(1, ''), (1, 'a'), (3, 'x')]\n",
      " |  \n",
      " |  saveAsTextFile(self, path: str, compressionCodecClass: Optional[str] = None) -> None\n",
      " |      Save this RDD as a text file, using string representations of elements.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : str\n",
      " |          path to text file\n",
      " |      compressionCodecClass : str, optional\n",
      " |          fully qualified classname of the compression codec class\n",
      " |          i.e. \"org.apache.hadoop.io.compress.GzipCodec\" (None by default)\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.textFile`\n",
      " |      :meth:`SparkContext.wholeTextFiles`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import os\n",
      " |      >>> import tempfile\n",
      " |      >>> from fileinput import input\n",
      " |      >>> from glob import glob\n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsTextFile1\") as d1:\n",
      " |      ...     path1 = os.path.join(d1, \"text_file1\")\n",
      " |      ...\n",
      " |      ...     # Write a temporary text file\n",
      " |      ...     sc.parallelize(range(10)).saveAsTextFile(path1)\n",
      " |      ...\n",
      " |      ...     # Load text file as an RDD\n",
      " |      ...     ''.join(sorted(input(glob(path1 + \"/part-0000*\"))))\n",
      " |      '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
      " |      \n",
      " |      Empty lines are tolerated when saving to text files.\n",
      " |      \n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsTextFile2\") as d2:\n",
      " |      ...     path2 = os.path.join(d2, \"text2_file2\")\n",
      " |      ...\n",
      " |      ...     # Write another temporary text file\n",
      " |      ...     sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(path2)\n",
      " |      ...\n",
      " |      ...     # Load text file as an RDD\n",
      " |      ...     ''.join(sorted(input(glob(path2 + \"/part-0000*\"))))\n",
      " |      '\\n\\n\\nbar\\nfoo\\n'\n",
      " |      \n",
      " |      Using compressionCodecClass\n",
      " |      \n",
      " |      >>> from fileinput import input, hook_compressed\n",
      " |      >>> with tempfile.TemporaryDirectory(prefix=\"saveAsTextFile3\") as d3:\n",
      " |      ...     path3 = os.path.join(d3, \"text3\")\n",
      " |      ...     codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
      " |      ...\n",
      " |      ...     # Write another temporary text file with specified codec\n",
      " |      ...     sc.parallelize(['foo', 'bar']).saveAsTextFile(path3, codec)\n",
      " |      ...\n",
      " |      ...     # Load text file as an RDD\n",
      " |      ...     result = sorted(input(glob(path3 + \"/part*.gz\"), openhook=hook_compressed))\n",
      " |      ...     ''.join([r.decode('utf-8') if isinstance(r, bytes) else r for r in result])\n",
      " |      'bar\\nfoo\\n'\n",
      " |  \n",
      " |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n",
      " |      Assign a name to this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      name : str\n",
      " |          new name\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the same :class:`RDD` with name updated\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.name`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 2])\n",
      " |      >>> rdd.setName('I am an RDD').name()\n",
      " |      'I am an RDD'\n",
      " |  \n",
      " |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Sorts this RDD by the given keyfunc\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keyfunc : function\n",
      " |          a function to compute the key\n",
      " |      ascending : bool, optional, default True\n",
      " |          sort the keys in ascending or descending order\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sortByKey`\n",
      " |      :meth:`pyspark.sql.DataFrame.sort`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
      " |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |  \n",
      " |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x000001CB45A995A0>) -> 'RDD[Tuple[K, V]]'\n",
      " |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ascending : bool, optional, default True\n",
      " |          sort the keys in ascending or descending order\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      keyfunc : function, optional, default identity mapping\n",
      " |          a function to compute the key\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a new :class:`RDD`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sortBy`\n",
      " |      :meth:`pyspark.sql.DataFrame.sort`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey().first()\n",
      " |      ('1', 3)\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
      " |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      " |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
      " |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
      " |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
      " |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
      " |  \n",
      " |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n",
      " |      Return a :class:`StatCounter` object that captures the mean, variance\n",
      " |      and count of the RDD's elements in one operation.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`StatCounter`\n",
      " |          a :class:`StatCounter` capturing the mean, variance and count of all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |      :meth:`RDD.histogram`\n",
      " |      :meth:`pyspark.sql.DataFrame.stat`\n",
      " |  \n",
      " |  stdev(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the standard deviation of this RDD's elements.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the standard deviation of all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |      :meth:`RDD.variance`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).stdev()\n",
      " |      0.816...\n",
      " |  \n",
      " |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n",
      " |      Return each value in `self` that is not contained in `other`.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with the elements from this that are not in `other`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.subtractByKey`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(rdd1.subtract(rdd2).collect())\n",
      " |      [('a', 1), ('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n",
      " |      Return each (key, value) pair in `self` that has no pair with matching\n",
      " |      key in `other`.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      numPartitions : int, optional\n",
      " |          the number of partitions in new :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` with the pairs from this whose keys are not in `other`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.subtract`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
      " |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
      " |      >>> sorted(rdd1.subtractByKey(rdd2).collect())\n",
      " |      [('b', 4), ('b', 5)]\n",
      " |  \n",
      " |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n",
      " |      Add up the elements in this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float, int, or complex\n",
      " |          the sum of all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.mean`\n",
      " |      :meth:`RDD.sumApprox`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
      " |      6.0\n",
      " |  \n",
      " |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.core.rdd.BoundedFloat\n",
      " |      Approximate operation to return the sum within a timeout\n",
      " |      or meet the confidence.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      timeout : int\n",
      " |          maximum time to wait for the job, in milliseconds\n",
      " |      confidence : float\n",
      " |          the desired statistical confidence in the result\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`BoundedFloat`\n",
      " |          a potentially incomplete result, with error bounds\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sum`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(1000), 10)\n",
      " |      >>> r = sum(range(1000))\n",
      " |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n",
      " |      True\n",
      " |  \n",
      " |  take(self: 'RDD[T]', num: int) -> List[~T]\n",
      " |      Take the first num elements of the RDD.\n",
      " |      \n",
      " |      It works by first scanning one partition, and use the results from\n",
      " |      that partition to estimate the number of additional partitions needed\n",
      " |      to satisfy the limit.\n",
      " |      \n",
      " |      Translated from the Scala implementation in RDD#take().\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          first number of elements\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the first `num` elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.first`\n",
      " |      :meth:`pyspark.sql.DataFrame.take`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
      " |      [2, 3]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
      " |      [2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
      " |      [91, 92, 93]\n",
      " |  \n",
      " |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
      " |      Get the N elements from an RDD ordered in ascending order or as\n",
      " |      specified by the optional key function.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          top N\n",
      " |      key : function, optional\n",
      " |          a function used to generate key for comparing\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the top N elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.top`\n",
      " |      :meth:`RDD.max`\n",
      " |      :meth:`RDD.min`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
      " |      [1, 2, 3, 4, 5, 6]\n",
      " |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
      " |      [10, 9, 7, 6, 5, 4]\n",
      " |      >>> sc.emptyRDD().takeOrdered(3)\n",
      " |      []\n",
      " |  \n",
      " |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n",
      " |      Return a fixed-size sampled subset of this RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      withReplacement : bool\n",
      " |          whether sampling is done with replacement\n",
      " |      num : int\n",
      " |          size of the returned sample\n",
      " |      seed : int, optional\n",
      " |          random seed\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          a fixed-size sampled subset of this :class:`RDD` in an array\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.sample`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> import sys\n",
      " |      >>> rdd = sc.parallelize(range(0, 10))\n",
      " |      >>> len(rdd.takeSample(True, 20, 1))\n",
      " |      20\n",
      " |      >>> len(rdd.takeSample(False, 5, 2))\n",
      " |      5\n",
      " |      >>> len(rdd.takeSample(False, 15, 3))\n",
      " |      10\n",
      " |      >>> sc.range(0, 10).takeSample(False, sys.maxsize)\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      ValueError: Sample size cannot be greater than ...\n",
      " |  \n",
      " |  toDF(self: 'RDD[Any]', schema: Optional[Any] = None, sampleRatio: Optional[float] = None) -> 'DataFrame'\n",
      " |  \n",
      " |  toDebugString(self) -> Optional[bytes]\n",
      " |      A description of this RDD and its recursive dependencies for debugging.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bytes\n",
      " |          debugging information of this :class:`RDD`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.toDebugString()\n",
      " |      b'...PythonRDD...ParallelCollectionRDD...'\n",
      " |  \n",
      " |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n",
      " |      Return an iterator that contains all of the elements in this RDD.\n",
      " |      The iterator will consume as much memory as the largest partition in this RDD.\n",
      " |      With prefetch it may consume up to the memory of the 2 largest partitions.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      prefetchPartitions : bool, optional\n",
      " |          If Spark should pre-fetch the next partition\n",
      " |          before it is needed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`collections.abc.Iterator`\n",
      " |          an iterator that contains all of the elements in this :class:`RDD`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.collect`\n",
      " |      :meth:`pyspark.sql.DataFrame.toLocalIterator`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize(range(10))\n",
      " |      >>> [x for x in rdd.toLocalIterator()]\n",
      " |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      " |  \n",
      " |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n",
      " |      Get the top N elements from an RDD.\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      num : int\n",
      " |          top N\n",
      " |      key : function, optional\n",
      " |          a function used to generate key for comparing\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list\n",
      " |          the top N elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.takeOrdered`\n",
      " |      :meth:`RDD.max`\n",
      " |      :meth:`RDD.min`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method should only be used if the resulting array is expected\n",
      " |      to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      It returns the list sorted in descending order.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
      " |      [12]\n",
      " |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
      " |      [6, 5]\n",
      " |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
      " |      [4, 3, 2]\n",
      " |  \n",
      " |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n",
      " |      Aggregates the elements of this RDD in a multi-level tree\n",
      " |      pattern.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      zeroValue : U\n",
      " |          the initial value for the accumulated result of each partition\n",
      " |      seqOp : function\n",
      " |          a function used to accumulate results within a partition\n",
      " |      combOp : function\n",
      " |          an associative function used to combine results from different partitions\n",
      " |      depth : int, optional, default 2\n",
      " |          suggested depth of the tree\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      U\n",
      " |          the aggregated result\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.aggregate`\n",
      " |      :meth:`RDD.treeReduce`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeAggregate(0, add, add)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeAggregate(0, add, add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n",
      " |      Reduces the elements of this RDD in a multi-level tree pattern.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      f : function\n",
      " |          the reduce function\n",
      " |      depth : int, optional, default 2\n",
      " |          suggested depth of the tree (default: 2)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T\n",
      " |          the aggregated result\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.reduce`\n",
      " |      :meth:`RDD.aggregate`\n",
      " |      :meth:`RDD.treeAggregate`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> add = lambda x, y: x + y\n",
      " |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
      " |      >>> rdd.treeReduce(add)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 1)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 2)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 5)\n",
      " |      -5\n",
      " |      >>> rdd.treeReduce(add, 10)\n",
      " |      -5\n",
      " |  \n",
      " |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n",
      " |      Return the union of this RDD and another one.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the union of this :class:`RDD` and another one\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`SparkContext.union`\n",
      " |      :meth:`pyspark.sql.DataFrame.union`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
      " |      >>> rdd.union(rdd).collect()\n",
      " |      [1, 1, 2, 3, 1, 1, 2, 3]\n",
      " |  \n",
      " |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n",
      " |      Mark the RDD as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      blocking : bool, optional, default False\n",
      " |          whether to block until all blocks are deleted\n",
      " |      \n",
      " |          .. versionadded:: 3.0.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          The same :class:`RDD`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.cache`\n",
      " |      :meth:`RDD.persist`\n",
      " |      :meth:`RDD.getStorageLevel`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd.cache()\n",
      " |      >>> rdd.is_cached\n",
      " |      True\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |      >>> rdd.is_cached\n",
      " |      False\n",
      " |      >>> _ = rdd.unpersist()\n",
      " |  \n",
      " |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n",
      " |      Return an RDD with the values of each tuple.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` only containing the values\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.keys`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n",
      " |      >>> rdd.collect()\n",
      " |      [2, 4]\n",
      " |  \n",
      " |  variance(self: 'RDD[NumberOrArray]') -> float\n",
      " |      Compute the variance of this RDD's elements.\n",
      " |      \n",
      " |      .. versionadded:: 0.9.1\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          the variance of all elements\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.stats`\n",
      " |      :meth:`RDD.sampleVariance`\n",
      " |      :meth:`RDD.stdev`\n",
      " |      :meth:`RDD.sampleStdev`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([1, 2, 3]).variance()\n",
      " |      0.666...\n",
      " |  \n",
      " |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n",
      " |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n",
      " |      This is only supported on certain cluster managers and currently requires dynamic\n",
      " |      allocation to be enabled. It will result in new executors with the resources specified\n",
      " |      being acquired to calculate the RDD.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      profile : :class:`pyspark.resource.ResourceProfile`\n",
      " |          a resource profile\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          the same :class:`RDD` with user specified profile\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.getResourceProfile`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This API is experimental\n",
      " |  \n",
      " |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n",
      " |      Zips this RDD with another one, returning key-value pairs with the\n",
      " |      first element in each RDD second element in each RDD, etc. Assumes\n",
      " |      that the two RDDs have the same number of partitions and the same\n",
      " |      number of elements in each partition (e.g. one was made through\n",
      " |      a map on the other).\n",
      " |      \n",
      " |      .. versionadded:: 1.0.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : :class:`RDD`\n",
      " |          another :class:`RDD`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the zipped key-value pairs\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.zipWithIndex`\n",
      " |      :meth:`RDD.zipWithUniqueId`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd1 = sc.parallelize(range(0,5))\n",
      " |      >>> rdd2 = sc.parallelize(range(1000, 1005))\n",
      " |      >>> rdd1.zip(rdd2).collect()\n",
      " |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
      " |  \n",
      " |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
      " |      Zips this RDD with its element indices.\n",
      " |      \n",
      " |      The ordering is first based on the partition index and then the\n",
      " |      ordering of items within each partition. So the first item in\n",
      " |      the first partition gets index 0, and the last item in the last\n",
      " |      partition receives the largest index.\n",
      " |      \n",
      " |      This method needs to trigger a spark job when this RDD contains\n",
      " |      more than one partitions.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the zipped key-index pairs\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.zip`\n",
      " |      :meth:`RDD.zipWithUniqueId`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
      " |  \n",
      " |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n",
      " |      Zips this RDD with generated unique Long ids.\n",
      " |      \n",
      " |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
      " |      n is the number of partitions. So there may exist gaps, but this\n",
      " |      method won't trigger a spark job, which is different from\n",
      " |      :meth:`zipWithIndex`.\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`RDD`\n",
      " |          a :class:`RDD` containing the zipped key-UniqueId pairs\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`RDD.zip`\n",
      " |      :meth:`RDD.zipWithIndex`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
      " |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  context\n",
      " |      The :class:`SparkContext` that this RDD was created on.\n",
      " |      \n",
      " |      .. versionadded:: 0.7.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`SparkContext`\n",
      " |          The :class:`SparkContext` that this RDD was created on\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> rdd = sc.range(5)\n",
      " |      >>> rdd.context\n",
      " |      <SparkContext ...>\n",
      " |      >>> rdd.context is sc\n",
      " |      True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  __parameters__ = (+T_co,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To know All methods in rdd\n",
    "dir(rdd)\n",
    "\n",
    "#Get Help\n",
    "help(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "442e1baa-56d5-4bda-9453-f18968017399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.RDD Actions\n",
    "#a.collect (convert RDD to in-memory list)\n",
    "#b.take() (prints first elements)\n",
    "#c.top(prints highest elements)\n",
    "#d.takeSample (take some sample random values from list, if it's true it will repeat same value again ,False means unique)\n",
    "#e.Aggregate functions - gives single output value\n",
    "#min,max,sum(),mean(),stdev\n",
    "#count( no.of elements)\n",
    "#stats- complete info about count,mean,stdev,max,min\n",
    "#f.reduce -gives single output value,reduce that aggregate data set(RDD) element using function\n",
    "#g.CountByValue,countByKey - count of same values\n",
    "#h.fold - aggregate the elements of each partition\n",
    "#i.variance (all n values variance)\n",
    "#j.sample variance - (n-1) values variance\n",
    "#k.saveAsTextFile   -- text file format\n",
    "#l.saveAsPickleFile -- binary file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7819d155-feb1-4af3-b4d5-fd66e5b53a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print('rdd:',rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb8185ce-bb52-47d2-8f14-5fb165e5c872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5 elements: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print('first 5 elements:',rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b23cc86-e864-43fc-aee9-e70e316b3f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top five elements: [9, 8, 7, 6, 5]\n"
     ]
    }
   ],
   "source": [
    "print('top five elements:',rdd.top(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "274b5044-23ee-4102-9276-ed2977334a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini element: 1\n"
     ]
    }
   ],
   "source": [
    "print('mini element:',rdd.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afab3b36-3175-4e3c-91f2-ffcaed34fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max element: 9\n"
     ]
    }
   ],
   "source": [
    "print('max element:',rdd.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f455c9b-eab6-493b-ab6d-321ea6bfd9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no of elements: 9\n"
     ]
    }
   ],
   "source": [
    "print('total no of elements:',rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d25f2f1a-5169-4920-99dd-1f718537af14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum of elements: 45\n"
     ]
    }
   ],
   "source": [
    "print('sum of elements:',rdd.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "030de488-6ac7-413c-81ce-e01f366469de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of elements: 5.0\n"
     ]
    }
   ],
   "source": [
    "print('mean of elements:',rdd.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff83e840-25d2-4868-bc66-4ab146fa3bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdev of elements: 2.581988897471611\n"
     ]
    }
   ],
   "source": [
    "print('stdev of elements:',rdd.stdev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bca12c4-a640-4bd7-be71-8459aeac7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random duplicate elements: [5, 9, 1, 5, 2]\n",
      "random unique elements: [6, 8, 7, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "print('random duplicate elements:',rdd.takeSample(True,5))\n",
    "print('random unique elements:',rdd.takeSample(False,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e67803a8-5ee8-484c-9812-637302cc8288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list statatics (count: 9, mean: 5.0, stdev: 2.581988897471611, max: 9, min: 1)\n"
     ]
    }
   ],
   "source": [
    "print('list statatics',rdd.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05f24acc-46c5-46b2-9f70-d38043472f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "population Variance: 6.666666666666667\n",
      "sample Variance 7.5\n"
     ]
    }
   ],
   "source": [
    "print('population Variance:',rdd.variance())\n",
    "print('sample Variance',rdd.sampleVariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004d2a58-a025-4d43-90ff-98e6291d9bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce list to one element by sum: 45\n",
      "reduce list to one element by subtraction: -43\n"
     ]
    }
   ],
   "source": [
    "print('reduce list to one element by sum:',rdd.reduce( lambda x,y:x+y))\n",
    "print('reduce list to one element by subtraction:',rdd.reduce(lambda x,y:x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e58a8f3a-8fb1-42bd-a846-bae2ffbcc763",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Print' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mPrint\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave as txt file\u001b[39m\u001b[38;5;124m'\u001b[39m,rdd\u001b[38;5;241m.\u001b[39msaveAsTextFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnooka\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave as binary file\u001b[39m\u001b[38;5;124m'\u001b[39m,rdd\u001b[38;5;241m.\u001b[39msaveAsPickleFile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mnooka\u001b[39m\u001b[38;5;130;01m\\b\u001b[39;00m\u001b[38;5;124minary\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Print' is not defined"
     ]
    }
   ],
   "source": [
    "Print('save as txt file',rdd.saveAsTextFile('C:\\\\Users\\\\nooka\\csv'))\n",
    "print('save as binary file',rdd.saveAsPickleFile('C:\\\\Users\\\\nooka\\binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5719ff8-4267-4453-93d3-429221e9e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 3), (1, 5), (1, 7), (1, 9), (3, 1), (3, 3), (3, 5), (3, 7), (3, 9)]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#creating list pair\n",
    "list_pair = [(i,j) for i in range(1,5,2) for j in range(1,10,2)]\n",
    "\n",
    "print(list_pair)\n",
    "print(type(list_pair))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "488ec167-c972-4df4-8cf9-95ae1745da71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (1, 3),\n",
       " (1, 5),\n",
       " (1, 7),\n",
       " (1, 9),\n",
       " (3, 1),\n",
       " (3, 3),\n",
       " (3, 5),\n",
       " (3, 7),\n",
       " (3, 9)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_pair= sc.parallelize(list_pair)\n",
    "rdd_pair.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "270f2ffb-8df6-4211-b340-e1cf202e9a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of values defaultdict(<class 'int'>, {(1, 1): 1, (1, 3): 1, (1, 5): 1, (1, 7): 1, (1, 9): 1, (3, 1): 1, (3, 3): 1, (3, 5): 1, (3, 7): 1, (3, 9): 1})\n"
     ]
    }
   ],
   "source": [
    "print('count of values',rdd_pair.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef55b45b-508b-413a-b21f-b8152673f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of keys defaultdict(<class 'int'>, {1: 5, 3: 5})\n"
     ]
    }
   ],
   "source": [
    "print('count of keys',rdd_pair.countByKey())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00e693e1-1f48-4a76-9d5e-1751ae9e25ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate the elements of each partition: 58\n"
     ]
    }
   ],
   "source": [
    "from operator import *\n",
    "print('aggregate the elements of each partition:',rdd.fold(1,add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd0de7bd-da59-4ca5-87f7-c0d64c131908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.RDD transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8df61231-47a6-474f-82b6-06597a81b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 1: \n",
    "data = [\"hello world\", \"pyspark map example\", \"flatMap is cool\"]\n",
    "rdd_data = sc.parallelize(data)\n",
    "#example 2: \n",
    "list2 = [1,2,3,4,5,5,4,3,2,0]\n",
    "rdd_list = sc.parallelize(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b8eb00b-07b0-4b7a-8999-34578a65f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.1.Map():Applies the function to each element of the RDD.Return a new distributed dataset.Output is still a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c4c9c63-d864-4f77-8904-c03438d9dcb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'world'], ['pyspark', 'map', 'example'], ['flatMap', 'is', 'cool']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list of list\n",
    "rdd_data.map(lambda x : x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce1a63b5-efd5-4c9a-9dab-add3681656e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello world', 11), ('pyspark map example', 19), ('flatMap is cool', 15)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return new rdd with item name  and it's length\n",
    "\n",
    "rdd_data.map(lambda x: (x,len(x))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d11bf2e9-b370-465b-8ebc-7fd0804fb667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[range(0, 1),\n",
       " range(0, 2),\n",
       " range(0, 3),\n",
       " range(0, 4),\n",
       " range(0, 5),\n",
       " range(0, 5),\n",
       " range(0, 4),\n",
       " range(0, 3),\n",
       " range(0, 2),\n",
       " range(0, 0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return item range list of list\n",
    "rdd_list.map(lambda x: range(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bf92d01-f5b3-478c-9eac-edb9a73eb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.2.flatMap():Applies the function to each element of the RDD.returns a single list of all results.big list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73bcc43c-1925-4f93-9e1c-6adcd69671a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', 'pyspark', 'map', 'example', 'flatMap', 'is', 'cool']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words final list\n",
    "\n",
    "rdd_data.flatMap(lambda x: x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d42cca0-70b3-493d-b155-be8857acf08f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#return item range final list\n",
    "rdd_list.flatMap(lambda x : range(x)).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37efe166-d2b1-437a-84a1-5e1f18ccc500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 5, 3]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.3.filter:\n",
    "\n",
    "even_rdd = rdd_list.filter(lambda x : x%2 == 0)\n",
    "odd_rdd = rdd_list.filter(lambda x: x%2 !=0)\n",
    "\n",
    "even_rdd.collect()\n",
    "odd_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "083e37c7-659b-4464-b38c-49d031316bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4], [5, 6], [7, 8, 9]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example 3: \n",
    "num = [1,2,3,4,5,6,7,8,9]\n",
    "rdd_num = sc.parallelize(num,4) #distribute list into 4 partitions\n",
    "rdd_num.glom().collect()    #glom shows no.of partition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a36c816c-866d-468a-b668-3a644647069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 7, 11, 24]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.4.mapPartitions - Similar to map but runs separetely on each partition\n",
    "\n",
    "#partition wise total sum\n",
    "\n",
    "def f(x) : yield sum(x)\n",
    "rdd_num.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca82bb51-d4c0-43c3-a2ec-8ab232e679da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#partition wise min value\n",
    "\n",
    "def f(x) :yield min(x)\n",
    "rdd_num.mapPartitions(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d477efb1-bb5e-4ff2-a5c7-a1be393c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.5.mapPartitionWIthIndex - Similar to Mappartition ,but also provides an integer value is index of partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "99da8786-4ace-4fe4-937f-aa788f86f02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3), (1, 7), (2, 11), (3, 24)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(index,x):yield (index,sum(x))\n",
    "rdd_num.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "372cdda7-20a4-48e7-87e6-499eea2df6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 3), (2, 5), (3, 7)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(index,x):yield (index,min(x))\n",
    "rdd_num.mapPartitionsWithIndex(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c01afa05-aeee-4143-9c1a-2b76e09f8c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 4, 4, 5, 5, 4, 2, 2, 0, 0]\n",
      "[1, 2, 3, 4, 5, 5, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "#5.6.sample - Sample a fraction fraction of the data,sample(withreplacement.fraction,seed)  ,if (with replacement) - true - will repeat same value ,else (withreplacement) -False - unique values\n",
    "\n",
    "rdd_repeat = rdd_list.sample(True,1)\n",
    "rdd_unique  = rdd_list.sample(False,1)\n",
    "print(rdd_repeat.collect())\n",
    "print(rdd_unique.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6dbf17d-5750-403a-ae2f-99bc44777934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 5, 4, 3, 2, 0, 1, 2, 2, 3, 4, 4, 5, 5, 4, 2, 2, 0, 0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.7.union -Return a new dataset that contains the union of the elements in the source dataset and the argument.\n",
    "\n",
    "rdd_union = rdd_unique.union(rdd_repeat)\n",
    "rdd_union.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5aa33fd6-817e-4ff2-a030-b00b0ef58d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.8.union -Return a new dataset that contains the union of the elements in the source dataset and the argument.\n",
    "rdd_union.distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0347903-2685-460d-9830-4716e6386171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.9.intersection-Return a new RDD that contains the intersection of elements in the source dataset and the argument.\n",
    "rdd_unique = sc.parallelize([1, 2, 3, 4])\n",
    "rdd_repeat = sc.parallelize([3, 4, 5, 6])\n",
    "\n",
    "\n",
    "rdd_unique.intersection(rdd_repeat).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3194b9d8-53ca-46a5-a55c-a56174209f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('apple', 1), ('banana', 2), ('apple', 3), ('orange', 4), ('banana', 5)]\n",
      "[('orange', [4]), ('banana', [2, 5]), ('apple', [1, 3])]\n"
     ]
    }
   ],
   "source": [
    "#5.10.GroupByKey\n",
    "data3 = [(\"apple\", 1), (\"banana\", 2), (\"apple\", 3), (\"orange\", 4), (\"banana\", 5)]\n",
    "rdd_data3 = sc.parallelize(data3) \n",
    "group_rdd = rdd_data3.groupByKey()\n",
    "result = group_rdd.mapValues(list).collect()\n",
    "print(rdd_data3.collect())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b1df5-b92e-4e55-a976-add4eaa948b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.Spark Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb937f90-e81f-4192-add5-93981bf37852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caching -caching is  used to save the data(RDD/Dataframe/Dataset) in a cluster-wide in memory,\n",
    "#cache() method default saves data in MEMORY_ONLY.\n",
    "#Used to store small amount od data. This is Very Useful for accessing repeated data .\n",
    "#such as querying a small data set or when running an iterative algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50743d-52c8-43c2-a748-8145ab96a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.Spark Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20436858-a7e6-4d05-9a39-0b2d228bc9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same like cahe ( stores data in memeory) but we can store large amout of  data\n",
    "# persist() method is used to store it to the user-defined storage levels like (memory only , disk only ,memory and disk only etc)\n",
    "\n",
    "rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "#rdd.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "\n",
    "#rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c79b3-2f84-4ae9-97af-545b71d44c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clears cache and persist data manually\n",
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4500d2-dd2a-4aa9-9354-d6a2223a896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.Broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073a999-d5db-4ecc-a0a2-934682168c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow programmer to keep a read only variable cached on each machine rather than shipping a copy of it with tasks\n",
    "# spark useses efficient broadcast algoritham to reduce communication cost\n",
    "\n",
    "\n",
    "broad = sc.broadcast([1,2,3,4,5,6])\n",
    "\n",
    "print(broad.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e3bdb-19a4-41ce-b59c-bcb4f7dd9f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the boarcast variables by using unpersist or destroy\n",
    "\n",
    "#broad.unpersist()\n",
    "#OR \n",
    "broad.destroy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25845740-fda1-48a0-bb17-f8c11f79b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y9.Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223a608-00e4-42f0-9363-8f1e99525bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import Accumulator\n",
    "\n",
    "accum=sc.accumulator(10)\n",
    "\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae529a30-0ee9-4a4b-b084-f51f584da01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23ea1d2-8c53-4f9c-b5e2-d73a03ab7a60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rdd_cal \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rdd' is not defined"
     ]
    }
   ],
   "source": [
    "rdd_cal = rdd.coalesce(3)\n",
    "rdd_cal.getNumPartitions()\n",
    "rdd_cal.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e798b-1cf8-43d2-a9a6-85428b0e8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11.repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecdfcdc-439b-4f4c-bf2c-632b5f3912f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_repart = rdd.repartition(3)\n",
    "rdd_repart.getNumPartitions()\n",
    "rdd_repart.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e42893-239d-44d6-a722-b069b53a2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12.Transformations - Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585bdf6f-5e5f-4571-8251-7afaf1e72b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "J1 = sc.parallelize([('A',2),('B',3),('C',4),('D',5),('E',6),('F',7)])\n",
    "\n",
    "J2 = sc.parallelize([('C',2),('D',3),('E',4),('F',5),('G',6),('H',7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838ce17-776b-4e78-a0c0-52f9ef9c61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#join/inner Join - takes two pair of rdd ,return only matched records from both RDD's\n",
    "\n",
    "innerjoin = J1.join(J2)\n",
    "innerjoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375a7f1-c33d-4705-b4b1-c5cb78856378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leftouterjoin  - returns matched records from both rdd and unmatched records from left rdd\n",
    "\n",
    "leftjoin = J1.leftOuterJoin(J2)\n",
    "leftjoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5f5fba-05e0-4519-bd80-a3a6a3aad75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RightOuterJoin - returns matched records from both rdd and unmatched from right rdd\n",
    "\n",
    "rightjoin = J1.rightOuterJoin(J2)\n",
    "rightjoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3baa119-d12b-452b-8965-254eb7ecc88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fullOuterJoin - returned all records from both RDD'S\n",
    "\n",
    "fulljoin = J1.fullOuterJoin(J2)\n",
    "fulljoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560a82f6-5192-4a4a-a974-07fbd2449e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cartesian join - cross product of both elements of RDD\n",
    "\n",
    "cartisanjoin = J1.cartesian(J2)\n",
    "cartisanjoin.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
