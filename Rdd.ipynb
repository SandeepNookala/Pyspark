{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ffa7575-d297-4765-b92e-4ba53709667d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#1. Installing Pyspark and Creating Pyspark Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bda6a73-7452-40e0-9917-0a727956b731",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "!pip install pyspark,py4j \\\n",
    "from pyspark import SparkContext \\\n",
    "sc = SparkContext.getOrCreate() \\\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41b45cb-6074-49c1-88f1-588b67e425ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#2.Creating RDD from lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23c7ad3a-5ccb-43aa-9afa-7fab5537b334",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "l = [ i for i in range(1,20)]\n",
    "\n",
    "l1 = [i for i in range(1,20,2)]\n",
    "\n",
    "l2 = [('s',1),('a',2),('n',3),('d',4),('e',5),('e',6),('p',7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55f45c0d-cd96-4bc7-9b5a-d4c73e688e8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\nOut[141]: pyspark.rdd.RDD"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(l)\n",
    "print(rdd.collect())\n",
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb85e99-333a-47e9-a008-aa01e77ef2f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\nOut[142]: pyspark.rdd.RDD"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize(l1)\n",
    "print(rdd1.collect())\n",
    "type(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2a1973f-74ce-4aa0-b369-66a5b2784952",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s', 1), ('a', 2), ('n', 3), ('d', 4), ('e', 5), ('e', 6), ('p', 7)]\nOut[143]: pyspark.rdd.RDD"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.parallelize(l2)\n",
    "print(rdd2.collect())\n",
    "type(rdd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fdc2ee-e389-469a-85bc-5e77bad17499",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#3.To know All methods in rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "706cc40d-d652-4bd8-be65-aa3ffcfdccfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[108]: ['__add__',\n '__class__',\n '__class_getitem__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getnewargs__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__orig_bases__',\n '__parameters__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__slots__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_computeFractionForSampleSize',\n '_defaultReducePartitions',\n '_id',\n '_is_barrier',\n '_is_protocol',\n '_jrdd',\n '_jrdd_deserializer',\n '_memory_limit',\n '_pickled',\n '_reserialize',\n '_to_java_object_rdd',\n 'aggregate',\n 'aggregateByKey',\n 'barrier',\n 'cache',\n 'cartesian',\n 'checkpoint',\n 'cleanShuffleDependencies',\n 'coalesce',\n 'cogroup',\n 'collect',\n 'collectAsMap',\n 'collectWithJobGroup',\n 'combineByKey',\n 'context',\n 'count',\n 'countApprox',\n 'countApproxDistinct',\n 'countByKey',\n 'countByValue',\n 'ctx',\n 'distinct',\n 'filter',\n 'first',\n 'flatMap',\n 'flatMapValues',\n 'fold',\n 'foldByKey',\n 'foreach',\n 'foreachPartition',\n 'fullOuterJoin',\n 'getCheckpointFile',\n 'getNumPartitions',\n 'getResourceProfile',\n 'getStorageLevel',\n 'glom',\n 'groupBy',\n 'groupByKey',\n 'groupWith',\n 'has_resource_profile',\n 'histogram',\n 'id',\n 'intersection',\n 'isCheckpointed',\n 'isEmpty',\n 'isLocallyCheckpointed',\n 'is_cached',\n 'is_checkpointed',\n 'join',\n 'keyBy',\n 'keys',\n 'leftOuterJoin',\n 'localCheckpoint',\n 'lookup',\n 'map',\n 'mapPartitions',\n 'mapPartitionsWithIndex',\n 'mapPartitionsWithSplit',\n 'mapValues',\n 'max',\n 'mean',\n 'meanApprox',\n 'min',\n 'name',\n 'partitionBy',\n 'partitioner',\n 'persist',\n 'pipe',\n 'randomSplit',\n 'reduce',\n 'reduceByKey',\n 'reduceByKeyLocally',\n 'repartition',\n 'repartitionAndSortWithinPartitions',\n 'rightOuterJoin',\n 'sample',\n 'sampleByKey',\n 'sampleStdev',\n 'sampleVariance',\n 'saveAsHadoopDataset',\n 'saveAsHadoopFile',\n 'saveAsNewAPIHadoopDataset',\n 'saveAsNewAPIHadoopFile',\n 'saveAsPickleFile',\n 'saveAsSequenceFile',\n 'saveAsTextFile',\n 'setName',\n 'sortBy',\n 'sortByKey',\n 'stats',\n 'stdev',\n 'subtract',\n 'subtractByKey',\n 'sum',\n 'sumApprox',\n 'take',\n 'takeOrdered',\n 'takeSample',\n 'toDF',\n 'toDebugString',\n 'toLocalIterator',\n 'top',\n 'treeAggregate',\n 'treeReduce',\n 'union',\n 'unpersist',\n 'values',\n 'variance',\n 'withResources',\n 'zip',\n 'zipWithIndex',\n 'zipWithUniqueId']"
     ]
    }
   ],
   "source": [
    "dir(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365f6a7b-b198-4ced-8c79-9c89fb1d89e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#4.Get Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2f42bc-40e5-4243-8708-b7f4764d40a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RDD in module pyspark.rdd object:\n\nclass RDD(typing.Generic)\n |  RDD(jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |  \n |  A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.\n |  Represents an immutable, partitioned collection of elements that can be\n |  operated on in parallel.\n |  \n |  Method resolution order:\n |      RDD\n |      typing.Generic\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __add__(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> (rdd + rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  __getnewargs__(self) -> NoReturn\n |  \n |  __init__(self, jrdd: 'JavaObject', ctx: 'SparkContext', jrdd_deserializer: pyspark.serializers.Serializer = AutoBatchedSerializer(CloudPickleSerializer()))\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  aggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U]) -> ~U\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given combine functions and a neutral \"zero\n |      value.\"\n |      \n |      The functions ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      The first function (seqOp) can return a different result type, U, than\n |      the type of this RDD. Thus, we need one operation for merging a T into\n |      an U and one operation for merging two U\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : U\n |          the initial value for the accumulated result of each partition\n |      seqOp : function\n |          a function used to accumulate results within a partition\n |      combOp : function\n |          an associative function used to combine results from different partitions\n |      \n |      Returns\n |      -------\n |      U\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduce`\n |      :meth:`RDD.fold`\n |      \n |      Examples\n |      --------\n |      >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n |      >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n |      >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n |      (10, 4)\n |      >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n |      (0, 0)\n |  \n |  aggregateByKey(self: 'RDD[Tuple[K, V]]', zeroValue: ~U, seqFunc: Callable[[~U, ~V], ~U], combFunc: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f1a12d74940>) -> 'RDD[Tuple[K, U]]'\n |      Aggregate the values of each key, using given combine functions and a neutral\n |      \"zero value\". This function can return a different result type, U, than the type\n |      of the values in this RDD, V. Thus, we need one operation for merging a V into\n |      a U and one operation for merging two U's, The former operation is used for merging\n |      values within a partition, and the latter is used for merging values between\n |      partitions. To avoid memory allocation, both of these functions are\n |      allowed to modify and return their first argument instead of creating a new U.\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : U\n |          the initial value for the accumulated result of each partition\n |      seqFunc : function\n |          a function to merge a V into a U\n |      combFunc : function\n |          a function to combine two U's into a single one\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      partitionFunc : function, optional, default `portable_hash`\n |          function to compute the partition index\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and the aggregated result for each key\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduceByKey`\n |      :meth:`RDD.combineByKey`\n |      :meth:`RDD.foldByKey`\n |      :meth:`RDD.groupByKey`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n |      >>> seqFunc = (lambda x, y: (x[0] + y, x[1] + 1))\n |      >>> combFunc = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n |      >>> sorted(rdd.aggregateByKey((0, 0), seqFunc, combFunc).collect())\n |      [('a', (3, 2)), ('b', (1, 1))]\n |  \n |  barrier(self: 'RDD[T]') -> 'RDDBarrier[T]'\n |      Marks the current stage as a barrier stage, where Spark must launch all tasks together.\n |      In case of a task failure, instead of only restarting the failed task, Spark will abort the\n |      entire stage and relaunch all tasks for this stage.\n |      The barrier execution mode feature is experimental and it only handles limited scenarios.\n |      Please read the linked SPIP and design docs to understand the limitations and future plans.\n |      \n |      .. versionadded:: 2.4.0\n |      \n |      Returns\n |      -------\n |      :class:`RDDBarrier`\n |          instance that provides actions within a barrier stage.\n |      \n |      See Also\n |      --------\n |      :class:`pyspark.BarrierTaskContext`\n |      \n |      Notes\n |      -----\n |      For additional information see\n |      \n |      - `SPIP: Barrier Execution Mode <http://jira.apache.org/jira/browse/SPARK-24374>`_\n |      - `Design Doc <https://jira.apache.org/jira/browse/SPARK-24582>`_\n |      \n |      This API is experimental\n |  \n |  cache(self: 'RDD[T]') -> 'RDD[T]'\n |      Persist this RDD with the default storage level (`MEMORY_ONLY`).\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          The same :class:`RDD` with storage level set to `MEMORY_ONLY`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.persist`\n |      :meth:`RDD.unpersist`\n |      :meth:`RDD.getStorageLevel`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd2 = rdd.cache()\n |      >>> rdd2 is rdd\n |      True\n |      >>> str(rdd.getStorageLevel())\n |      'Memory Serialized 1x Replicated'\n |      >>> _ = rdd.unpersist()\n |  \n |  cartesian(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Return the Cartesian product of this RDD and another one, that is, the\n |      RDD of all pairs of elements ``(a, b)`` where ``a`` is in `self` and\n |      ``b`` is in `other`.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the Cartesian product of this :class:`RDD` and another one\n |      \n |      See Also\n |      --------\n |      :meth:`pyspark.sql.DataFrame.crossJoin`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2])\n |      >>> sorted(rdd.cartesian(rdd).collect())\n |      [(1, 1), (1, 2), (2, 1), (2, 2)]\n |  \n |  checkpoint(self) -> None\n |      Mark this RDD for checkpointing. It will be saved to a file inside the\n |      checkpoint directory set with :meth:`SparkContext.setCheckpointDir` and\n |      all references to its parent RDDs will be removed. This function must\n |      be called before any job has been executed on this RDD. It is strongly\n |      recommended that this RDD is persisted in memory, otherwise saving it\n |      on a file will require recomputation.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.isCheckpointed`\n |      :meth:`RDD.getCheckpointFile`\n |      :meth:`RDD.localCheckpoint`\n |      :meth:`SparkContext.setCheckpointDir`\n |      :meth:`SparkContext.getCheckpointDir`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.is_checkpointed\n |      False\n |      >>> rdd.getCheckpointFile() == None\n |      True\n |      \n |      >>> rdd.checkpoint()\n |      >>> rdd.is_checkpointed\n |      True\n |      >>> rdd.getCheckpointFile() == None\n |      True\n |      \n |      >>> rdd.count()\n |      5\n |      >>> rdd.is_checkpointed\n |      True\n |      >>> rdd.getCheckpointFile() == None\n |      False\n |  \n |  cleanShuffleDependencies(self, blocking: bool = False) -> None\n |      Removes an RDD's shuffles and it's non-persisted ancestors.\n |      \n |      When running without a shuffle service, cleaning up shuffle files enables downscaling.\n |      If you use the RDD after this call, you should checkpoint and materialize it first.\n |      \n |      .. versionadded:: 3.3.0\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional, default False\n |         whether to block on shuffle cleanup tasks\n |      \n |      Notes\n |      -----\n |      This API is a developer API.\n |  \n |  coalesce(self: 'RDD[T]', numPartitions: int, shuffle: bool = False) -> 'RDD[T]'\n |      Return a new RDD that is reduced into `numPartitions` partitions.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      shuffle : bool, optional, default False\n |          whether to add a shuffle step\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` that is reduced into `numPartitions` partitions\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.repartition`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n |      [[1], [2, 3], [4, 5]]\n |      >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n |      [[1, 2, 3, 4, 5]]\n |  \n |  cogroup(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, U]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, Tuple[ResultIterable[V], ResultIterable[U]]]]'\n |      For each key k in `self` or `other`, return a resulting RDD that\n |      contains a tuple with the list of values for that key in `self` as\n |      well as `other`.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and cogrouped values\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.groupWith`\n |      :meth:`RDD.join`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n |      >>> rdd2 = sc.parallelize([(\"a\", 2)])\n |      >>> [(x, tuple(map(list, y))) for x, y in sorted(list(rdd1.cogroup(rdd2).collect()))]\n |      [('a', ([1], [2])), ('b', ([4], []))]\n |  \n |  collect(self: 'RDD[T]') -> List[~T]\n |      Return a list that contains all the elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      list\n |          a list containing all the elements\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.toLocalIterator`\n |      :meth:`pyspark.sql.DataFrame.collect`\n |      \n |      Examples\n |      --------\n |      >>> sc.range(5).collect()\n |      [0, 1, 2, 3, 4]\n |      >>> sc.parallelize([\"x\", \"y\", \"z\"]).collect()\n |      ['x', 'y', 'z']\n |  \n |  collectAsMap(self: 'RDD[Tuple[K, V]]') -> Dict[~K, ~V]\n |      Return the key-value pairs in this RDD to the master as a dictionary.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`dict`\n |          a dictionary of (key, value) pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.countByValue`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting data is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n |      >>> m[1]\n |      2\n |      >>> m[3]\n |      4\n |  \n |  collectWithJobGroup(self: 'RDD[T]', groupId: str, description: str, interruptOnCancel: bool = False) -> 'List[T]'\n |      When collect rdd, use this method to specify job group.\n |      \n |      .. versionadded:: 3.0.0\n |      \n |      .. deprecated:: 3.1.0\n |          Use :class:`pyspark.InheritableThread` with the pinned thread mode enabled.\n |      \n |      Parameters\n |      ----------\n |      groupId : str\n |          The group ID to assign.\n |      description : str\n |          The description to set for the job group.\n |      interruptOnCancel : bool, optional, default False\n |          whether to interrupt jobs on job cancellation.\n |      \n |      Returns\n |      -------\n |      list\n |          a list containing all the elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collect`\n |      :meth:`SparkContext.setJobGroup`\n |  \n |  combineByKey(self: 'RDD[Tuple[K, V]]', createCombiner: Callable[[~V], ~U], mergeValue: Callable[[~U, ~V], ~U], mergeCombiners: Callable[[~U, ~U], ~U], numPartitions: Optional[int] = None, partitionFunc: Callable[[~K], int] = <function portable_hash at 0x7f1a12d74940>) -> 'RDD[Tuple[K, U]]'\n |      Generic function to combine the elements for each key using a custom\n |      set of aggregation functions.\n |      \n |      Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n |      type\" C.\n |      \n |      To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\n |      modify and return their first argument instead of creating a new C.\n |      \n |      In addition, users can control the partitioning of the output RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      createCombiner : function\n |          a function to turns a V into a C\n |      mergeValue : function\n |          a function to merge a V into a C\n |      mergeCombiners : function\n |          a function to combine two C's into a single one\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      partitionFunc : function, optional, default `portable_hash`\n |          function to compute the partition index\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and the aggregated result for each key\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduceByKey`\n |      :meth:`RDD.aggregateByKey`\n |      :meth:`RDD.foldByKey`\n |      :meth:`RDD.groupByKey`\n |      \n |      Notes\n |      -----\n |      V and C can be different -- for example, one might group an RDD of type\n |          (Int, Int) into an RDD of type (Int, List[Int]).\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n |      >>> def to_list(a):\n |      ...     return [a]\n |      ...\n |      >>> def append(a, b):\n |      ...     a.append(b)\n |      ...     return a\n |      ...\n |      >>> def extend(a, b):\n |      ...     a.extend(b)\n |      ...     return a\n |      ...\n |      >>> sorted(rdd.combineByKey(to_list, append, extend).collect())\n |      [('a', [1, 2]), ('b', [1])]\n |  \n |  count(self) -> int\n |      Return the number of elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      int\n |          the number of elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.countApprox`\n |      :meth:`pyspark.sql.DataFrame.count`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).count()\n |      3\n |  \n |  countApprox(self, timeout: int, confidence: float = 0.95) -> int\n |      Approximate version of count() that returns a potentially incomplete\n |      result within a timeout, even if not all tasks have finished.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Parameters\n |      ----------\n |      timeout : int\n |          maximum time to wait for the job, in milliseconds\n |      confidence : float\n |          the desired statistical confidence in the result\n |      \n |      Returns\n |      -------\n |      int\n |          a potentially incomplete result, with error bounds\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.count`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> rdd.countApprox(1000, 1.0)\n |      1000\n |  \n |  countApproxDistinct(self: 'RDD[T]', relativeSD: float = 0.05) -> int\n |      Return approximate number of distinct elements in the RDD.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Parameters\n |      ----------\n |      relativeSD : float, optional\n |          Relative accuracy. Smaller values create\n |          counters that require more space.\n |          It must be greater than 0.000017.\n |      \n |      Returns\n |      -------\n |      int\n |          approximate number of distinct elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.distinct`\n |      \n |      Notes\n |      -----\n |      The algorithm used is based on streamlib's implementation of\n |      `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n |      of The Art Cardinality Estimation Algorithm\", available here\n |      <https://doi.org/10.1145/2452376.2452456>`_.\n |      \n |      Examples\n |      --------\n |      >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n |      >>> 900 < n < 1100\n |      True\n |      >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n |      >>> 16 < n < 24\n |      True\n |  \n |  countByKey(self: 'RDD[Tuple[K, V]]') -> Dict[~K, int]\n |      Count the number of elements for each key, and return the result to the\n |      master as a dictionary.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      dict\n |          a dictionary of (key, count) pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collectAsMap`\n |      :meth:`RDD.countByValue`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n |      >>> sorted(rdd.countByKey().items())\n |      [('a', 2), ('b', 1)]\n |  \n |  countByValue(self: 'RDD[K]') -> Dict[~K, int]\n |      Return the count of each unique value in this RDD as a dictionary of\n |      (value, count) pairs.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      dict\n |          a dictionary of (value, count) pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collectAsMap`\n |      :meth:`RDD.countByKey`\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n |      [(1, 2), (2, 3)]\n |  \n |  distinct(self: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return a new RDD containing the distinct elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD` containing the distinct elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.countApproxDistinct`\n |      \n |      Examples\n |      --------\n |      >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n |      [1, 2, 3]\n |  \n |  filter(self: 'RDD[T]', f: Callable[[~T], bool]) -> 'RDD[T]'\n |      Return a new RDD containing only the elements that satisfy a predicate.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |          a function to run on each element of the RDD\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD` by applying a function to each element\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.map`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n |      >>> rdd.filter(lambda x: x % 2 == 0).collect()\n |      [2, 4]\n |  \n |  first(self: 'RDD[T]') -> ~T\n |      Return the first element in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      T\n |          the first element\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.take`\n |      :meth:`pyspark.sql.DataFrame.first`\n |      :meth:`pyspark.sql.DataFrame.head`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4]).first()\n |      2\n |      >>> sc.parallelize([]).first()\n |      Traceback (most recent call last):\n |          ...\n |      ValueError: RDD is empty\n |  \n |  flatMap(self: 'RDD[T]', f: Callable[[~T], Iterable[~U]], preservesPartitioning: bool = False) -> 'RDD[U]'\n |      Return a new RDD by first applying a function to all elements of this\n |      RDD, and then flattening the results.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |          a function to turn a T into a sequence of U\n |      preservesPartitioning : bool, optional, default False\n |          indicates whether the input function preserves the partitioner,\n |          which should be False unless this is a pair RDD and the input\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD` by applying a function to all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.map`\n |      :meth:`RDD.mapPartitions`\n |      :meth:`RDD.mapPartitionsWithIndex`\n |      :meth:`RDD.mapPartitionsWithSplit`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([2, 3, 4])\n |      >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n |      [1, 1, 1, 2, 2, 3]\n |      >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n |      [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n |  \n |  flatMapValues(self: 'RDD[Tuple[K, V]]', f: Callable[[~V], Iterable[~U]]) -> 'RDD[Tuple[K, U]]'\n |      Pass each value in the key-value pair RDD through a flatMap function\n |      without changing the keys; this also retains the original RDD's\n |      partitioning.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |         a function to turn a V into a sequence of U\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the keys and the flat-mapped value\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.flatMap`\n |      :meth:`RDD.mapValues`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n |      >>> def f(x): return x\n |      >>> rdd.flatMapValues(f).collect()\n |      [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n |  \n |  fold(self: 'RDD[T]', zeroValue: ~T, op: Callable[[~T, ~T], ~T]) -> ~T\n |      Aggregate the elements of each partition, and then the results for all\n |      the partitions, using a given associative function and a neutral \"zero value.\"\n |      \n |      The function ``op(t1, t2)`` is allowed to modify ``t1`` and return it\n |      as its result value to avoid object allocation; however, it should not\n |      modify ``t2``.\n |      \n |      This behaves somewhat differently from fold operations implemented\n |      for non-distributed collections in functional languages like Scala.\n |      This fold operation may be applied to partitions individually, and then\n |      fold those results into the final result, rather than apply the fold\n |      to each element sequentially in some defined ordering. For functions\n |      that are not commutative, the result may differ from that of a fold\n |      applied to a non-distributed collection.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : T\n |          the initial value for the accumulated result of each partition\n |      op : function\n |          a function used to both accumulate results within a partition and combine\n |          results from different partitions\n |      \n |      Returns\n |      -------\n |      T\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduce`\n |      :meth:`RDD.aggregate`\n |      \n |      Examples\n |      --------\n |      >>> from operator import add\n |      >>> sc\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n r in result])\n |      'bar\\nfoo\\n'\n |  \n |  setName(self: 'RDD[T]', name: str) -> 'RDD[T]'\n |      Assign a name to this RDD.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      name : str\n |          new name\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the same :class:`RDD` with name updated\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.name`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 2])\n |      >>> rdd.setName('I am an RDD').name()\n |      'I am an RDD'\n |  \n |  sortBy(self: 'RDD[T]', keyfunc: Callable[[~T], ForwardRef('S')], ascending: bool = True, numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Sorts this RDD by the given keyfunc\n |      \n |      .. versionadded:: 1.1.0\n |      \n |      Parameters\n |      ----------\n |      keyfunc : function\n |          a function to compute the key\n |      ascending : bool, optional, default True\n |          sort the keys in ascending or descending order\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sortByKey`\n |      :meth:`pyspark.sql.DataFrame.sort`\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n |      [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |  \n |  sortByKey(self: 'RDD[Tuple[K, V]]', ascending: Optional[bool] = True, numPartitions: Optional[int] = None, keyfunc: Callable[[Any], Any] = <function RDD.<lambda> at 0x7f1a0ad9d4c0>) -> 'RDD[Tuple[K, V]]'\n |      Sorts this RDD, which is assumed to consist of (key, value) pairs.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      ascending : bool, optional, default True\n |          sort the keys in ascending or descending order\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      keyfunc : function, optional, default identity mapping\n |          a function to compute the key\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a new :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sortBy`\n |      :meth:`pyspark.sql.DataFrame.sort`\n |      \n |      Examples\n |      --------\n |      >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n |      >>> sc.parallelize(tmp).sortByKey().first()\n |      ('1', 3)\n |      >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n |      [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n |      >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n |      >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n |      >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n |      [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n |  \n |  stats(self: 'RDD[NumberOrArray]') -> pyspark.statcounter.StatCounter\n |      Return a :class:`StatCounter` object that captures the mean, variance\n |      and count of the RDD's elements in one operation.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Returns\n |      -------\n |      :class:`StatCounter`\n |          a :class:`StatCounter` capturing the mean, variance and count of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.stdev`\n |      :meth:`RDD.sampleStdev`\n |      :meth:`RDD.variance`\n |      :meth:`RDD.sampleVariance`\n |      :meth:`RDD.histogram`\n |      :meth:`pyspark.sql.DataFrame.stat`\n |  \n |  stdev(self: 'RDD[NumberOrArray]') -> float\n |      Compute the standard deviation of this RDD's elements.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Returns\n |      -------\n |      float\n |          the standard deviation of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.stats`\n |      :meth:`RDD.sampleStdev`\n |      :meth:`RDD.variance`\n |      :meth:`RDD.sampleVariance`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).stdev()\n |      0.816...\n |  \n |  subtract(self: 'RDD[T]', other: 'RDD[T]', numPartitions: Optional[int] = None) -> 'RDD[T]'\n |      Return each value in `self` that is not contained in `other`.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` with the elements from this that are not in `other`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.subtractByKey`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(rdd1.subtract(rdd2).collect())\n |      [('a', 1), ('b', 4), ('b', 5)]\n |  \n |  subtractByKey(self: 'RDD[Tuple[K, V]]', other: 'RDD[Tuple[K, Any]]', numPartitions: Optional[int] = None) -> 'RDD[Tuple[K, V]]'\n |      Return each (key, value) pair in `self` that has no pair with matching\n |      key in `other`.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      numPartitions : int, optional\n |          the number of partitions in new :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` with the pairs from this whose keys are not in `other`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.subtract`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n |      >>> rdd2 = sc.parallelize([(\"a\", 3), (\"c\", None)])\n |      >>> sorted(rdd1.subtractByKey(rdd2).collect())\n |      [('b', 4), ('b', 5)]\n |  \n |  sum(self: 'RDD[NumberOrArray]') -> 'NumberOrArray'\n |      Add up the elements in this RDD.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      float, int, or complex\n |          the sum of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.mean`\n |      :meth:`RDD.sumApprox`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n |      6.0\n |  \n |  sumApprox(self: 'RDD[Union[float, int]]', timeout: int, confidence: float = 0.95) -> pyspark.rdd.BoundedFloat\n |      Approximate operation to return the sum within a timeout\n |      or meet the confidence.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Parameters\n |      ----------\n |      timeout : int\n |          maximum time to wait for the job, in milliseconds\n |      confidence : float\n |          the desired statistical confidence in the result\n |      \n |      Returns\n |      -------\n |      :class:`BoundedFloat`\n |          a potentially incomplete result, with error bounds\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sum`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(1000), 10)\n |      >>> r = sum(range(1000))\n |      >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\n |      True\n |  \n |  take(self: 'RDD[T]', num: int) -> List[~T]\n |      Take the first num elements of the RDD.\n |      \n |      It works by first scanning one partition, and use the results from\n |      that partition to estimate the number of additional partitions needed\n |      to satisfy the limit.\n |      \n |      Translated from the Scala implementation in RDD#take().\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      num : int\n |          first number of elements\n |      \n |      Returns\n |      -------\n |      list\n |          the first `num` elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.first`\n |      :meth:`pyspark.sql.DataFrame.take`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n |      [2, 3]\n |      >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n |      [2, 3, 4, 5, 6]\n |      >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n |      [91, 92, 93]\n |  \n |  takeOrdered(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the N elements from an RDD ordered in ascending order or as\n |      specified by the optional key function.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      num : int\n |          top N\n |      key : function, optional\n |          a function used to generate key for comparing\n |      \n |      Returns\n |      -------\n |      list\n |          the top N elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.top`\n |      :meth:`RDD.max`\n |      :meth:`RDD.min`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n |      [1, 2, 3, 4, 5, 6]\n |      >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n |      [10, 9, 7, 6, 5, 4]\n |  \n |  takeSample(self: 'RDD[T]', withReplacement: bool, num: int, seed: Optional[int] = None) -> List[~T]\n |      Return a fixed-size sampled subset of this RDD.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      withReplacement : list\n |          whether sampling is done with replacement\n |      num : int\n |          size of the returned sample\n |      seed : int, optional\n |          random seed\n |      \n |      Returns\n |      -------\n |      list\n |          a fixed-size sampled subset of this :class:`RDD` in an array\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.sample`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(0, 10))\n |      >>> len(rdd.takeSample(True, 20, 1))\n |      20\n |      >>> len(rdd.takeSample(False, 5, 2))\n |      5\n |      >>> len(rdd.takeSample(False, 15, 3))\n |      10\n |  \n |  toDF(self, schema=None, sampleRatio=None)\n |      Converts current :class:`RDD` into a :class:`DataFrame`\n |      \n |      This is a shorthand for ``spark.createDataFrame(rdd, schema, sampleRatio)``\n |      \n |      Parameters\n |      ----------\n |      schema : :class:`pyspark.sql.types.DataType`, str or list, optional\n |          a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n |          column names, default is None.  The data type string format equals to\n |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n |      sampleRatio : float, optional\n |          the sample ratio of rows used for inferring\n |      \n |      Returns\n |      -------\n |      :class:`DataFrame`\n |      \n |      Examples\n |      --------\n |      >>> rdd = spark.range(1).rdd.map(lambda x: tuple(x))\n |      >>> rdd.collect()\n |      [(0,)]\n |      >>> rdd.toDF().show()\n |      +---+\n |      | _1|\n |      +---+\n |      |  0|\n |      +---+\n |  \n |  toDebugString(self) -> Optional[bytes]\n |      A description of this RDD and its recursive dependencies for debugging.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Returns\n |      -------\n |      bytes\n |          debugging information of this :class:`RDD`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.toDebugString()\n |      b'...PythonRDD...ParallelCollectionRDD...'\n |  \n |  toLocalIterator(self: 'RDD[T]', prefetchPartitions: bool = False) -> Iterator[~T]\n |      Return an iterator that contains all of the elements in this RDD.\n |      The iterator will consume as much memory as the largest partition in this RDD.\n |      With prefetch it may consume up to the memory of the 2 largest partitions.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      prefetchPartitions : bool, optional\n |          If Spark should pre-fetch the next partition\n |          before it is needed.\n |      \n |      Returns\n |      -------\n |      :class:`collections.abc.Iterator`\n |          an iterator that contains all of the elements in this :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.collect`\n |      :meth:`pyspark.sql.DataFrame.toLocalIterator`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize(range(10))\n |      >>> [x for x in rdd.toLocalIterator()]\n |      [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n |  \n |  top(self: 'RDD[T]', num: int, key: Optional[Callable[[~T], ForwardRef('S')]] = None) -> List[~T]\n |      Get the top N elements from an RDD.\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      num : int\n |          top N\n |      key : function, optional\n |          a function used to generate key for comparing\n |      \n |      Returns\n |      -------\n |      list\n |          the top N elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.takeOrdered`\n |      :meth:`RDD.max`\n |      :meth:`RDD.min`\n |      \n |      Notes\n |      -----\n |      This method should only be used if the resulting array is expected\n |      to be small, as all the data is loaded into the driver's memory.\n |      \n |      It returns the list sorted in descending order.\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n |      [12]\n |      >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n |      [6, 5]\n |      >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n |      [4, 3, 2]\n |  \n |  treeAggregate(self: 'RDD[T]', zeroValue: ~U, seqOp: Callable[[~U, ~T], ~U], combOp: Callable[[~U, ~U], ~U], depth: int = 2) -> ~U\n |      Aggregates the elements of this RDD in a multi-level tree\n |      pattern.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      zeroValue : U\n |          the initial value for the accumulated result of each partition\n |      seqOp : function\n |          a function used to accumulate results within a partition\n |      combOp : function\n |          an associative function used to combine results from different partitions\n |      depth : int, optional, default 2\n |          suggested depth of the tree\n |      \n |      Returns\n |      -------\n |      U\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.aggregate`\n |      :meth:`RDD.treeReduce`\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeAggregate(0, add, add)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 1)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 2)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 5)\n |      -5\n |      >>> rdd.treeAggregate(0, add, add, 10)\n |      -5\n |  \n |  treeReduce(self: 'RDD[T]', f: Callable[[~T, ~T], ~T], depth: int = 2) -> ~T\n |      Reduces the elements of this RDD in a multi-level tree pattern.\n |      \n |      .. versionadded:: 1.3.0\n |      \n |      Parameters\n |      ----------\n |      f : function\n |          the reduce function\n |      depth : int, optional, default 2\n |          suggested depth of the tree (default: 2)\n |      \n |      Returns\n |      -------\n |      T\n |          the aggregated result\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.reduce`\n |      :meth:`RDD.aggregate`\n |      :meth:`RDD.treeAggregate`\n |      \n |      Examples\n |      --------\n |      >>> add = lambda x, y: x + y\n |      >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n |      >>> rdd.treeReduce(add)\n |      -5\n |      >>> rdd.treeReduce(add, 1)\n |      -5\n |      >>> rdd.treeReduce(add, 2)\n |      -5\n |      >>> rdd.treeReduce(add, 5)\n |      -5\n |      >>> rdd.treeReduce(add, 10)\n |      -5\n |  \n |  union(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Union[T, U]]'\n |      Return the union of this RDD and another one.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the union of this :class:`RDD` and another one\n |      \n |      See Also\n |      --------\n |      :meth:`SparkContext.union`\n |      :meth:`pyspark.sql.DataFrame.union`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([1, 1, 2, 3])\n |      >>> rdd.union(rdd).collect()\n |      [1, 1, 2, 3, 1, 1, 2, 3]\n |  \n |  unpersist(self: 'RDD[T]', blocking: bool = False) -> 'RDD[T]'\n |      Mark the RDD as non-persistent, and remove all blocks for it from\n |      memory and disk.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Parameters\n |      ----------\n |      blocking : bool, optional, default False\n |          whether to block until all blocks are deleted\n |      \n |          .. versionadded:: 3.0.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          The same :class:`RDD`\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.cache`\n |      :meth:`RDD.persist`\n |      :meth:`RDD.getStorageLevel`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.is_cached\n |      False\n |      >>> _ = rdd.unpersist()\n |      >>> rdd.is_cached\n |      False\n |      >>> _ = rdd.cache()\n |      >>> rdd.is_cached\n |      True\n |      >>> _ = rdd.unpersist()\n |      >>> rdd.is_cached\n |      False\n |      >>> _ = rdd.unpersist()\n |  \n |  values(self: 'RDD[Tuple[K, V]]') -> 'RDD[V]'\n |      Return an RDD with the values of each tuple.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` only containing the values\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.keys`\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.parallelize([(1, 2), (3, 4)]).values()\n |      >>> rdd.collect()\n |      [2, 4]\n |  \n |  variance(self: 'RDD[NumberOrArray]') -> float\n |      Compute the variance of this RDD's elements.\n |      \n |      .. versionadded:: 0.9.1\n |      \n |      Returns\n |      -------\n |      float\n |          the variance of all elements\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.stats`\n |      :meth:`RDD.sampleVariance`\n |      :meth:`RDD.stdev`\n |      :meth:`RDD.sampleStdev`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([1, 2, 3]).variance()\n |      0.666...\n |  \n |  withResources(self: 'RDD[T]', profile: pyspark.resource.profile.ResourceProfile) -> 'RDD[T]'\n |      Specify a :class:`pyspark.resource.ResourceProfile` to use when calculating this RDD.\n |      This is only supported on certain cluster managers and currently requires dynamic\n |      allocation to be enabled. It will result in new executors with the resources specified\n |      being acquired to calculate the RDD.\n |      \n |      .. versionadded:: 3.1.0\n |      \n |      Parameters\n |      ----------\n |      profile : :class:`pyspark.resource.ResourceProfile`\n |          a resource profile\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          the same :class:`RDD` with user specified profile\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.getResourceProfile`\n |      \n |      Notes\n |      -----\n |      This API is experimental\n |  \n |  zip(self: 'RDD[T]', other: 'RDD[U]') -> 'RDD[Tuple[T, U]]'\n |      Zips this RDD with another one, returning key-value pairs with the\n |      first element in each RDD second element in each RDD, etc. Assumes\n |      that the two RDDs have the same number of partitions and the same\n |      number of elements in each partition (e.g. one was made through\n |      a map on the other).\n |      \n |      .. versionadded:: 1.0.0\n |      \n |      Parameters\n |      ----------\n |      other : :class:`RDD`\n |          another :class:`RDD`\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the zipped key-value pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.zipWithIndex`\n |      :meth:`RDD.zipWithUniqueId`\n |      \n |      Examples\n |      --------\n |      >>> rdd1 = sc.parallelize(range(0,5))\n |      >>> rdd2 = sc.parallelize(range(1000, 1005))\n |      >>> rdd1.zip(rdd2).collect()\n |      [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n |  \n |  zipWithIndex(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with its element indices.\n |      \n |      The ordering is first based on the partition index and then the\n |      ordering of items within each partition. So the first item in\n |      the first partition gets index 0, and the last item in the last\n |      partition receives the largest index.\n |      \n |      This method needs to trigger a spark job when this RDD contains\n |      more than one partitions.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the zipped key-index pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.zip`\n |      :meth:`RDD.zipWithUniqueId`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n |      [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n |  \n |  zipWithUniqueId(self: 'RDD[T]') -> 'RDD[Tuple[T, int]]'\n |      Zips this RDD with generated unique Long ids.\n |      \n |      Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n |      n is the number of partitions. So there may exist gaps, but this\n |      method won't trigger a spark job, which is different from\n |      :meth:`zipWithIndex`.\n |      \n |      .. versionadded:: 1.2.0\n |      \n |      Returns\n |      -------\n |      :class:`RDD`\n |          a :class:`RDD` containing the zipped key-UniqueId pairs\n |      \n |      See Also\n |      --------\n |      :meth:`RDD.zip`\n |      :meth:`RDD.zipWithIndex`\n |      \n |      Examples\n |      --------\n |      >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n |      [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  context\n |      The :class:`SparkContext` that this RDD was created on.\n |      \n |      .. versionadded:: 0.7.0\n |      \n |      Returns\n |      -------\n |      :class:`SparkContext`\n |          The :class:`SparkContext` that this RDD was created on\n |      \n |      Examples\n |      --------\n |      >>> rdd = sc.range(5)\n |      >>> rdd.context\n |      <SparkContext ...>\n |      >>> rdd.context is sc\n |      True\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __orig_bases__ = (typing.Generic[+T_co],)\n |  \n |  __parameters__ = (+T_co,)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from typing.Generic:\n |  \n |  __class_getitem__(params) from builtins.type\n |  \n |  __init_subclass__(*args, **kwargs) from builtins.type\n |      This method is called when a class is subclassed.\n |      \n |      The default implementation does nothing. It may be\n |      overridden to extend subclasses.\n\n"
     ]
    }
   ],
   "source": [
    "help(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ef5db0-0d82-4e15-9d85-af7b2da6bae4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#5.RDD Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9341f84-fec9-4ade-bd38-94c3011bf135",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n[('s', 1), ('a', 2), ('n', 3), ('d', 4), ('e', 5), ('e', 6), ('p', 7)]\n"
     ]
    }
   ],
   "source": [
    "#collect (convert RDD to in-memory list)\n",
    "print(rdd.collect())\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff86caf-95d3-412a-867b-f6283f460995",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[111]: [1, 2, 3, 4, 5]"
     ]
    }
   ],
   "source": [
    "#take() (prints first elements)\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6745174-993c-429a-8c85-3eda5c158d65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[112]: [9, 8, 7, 6, 5]"
     ]
    }
   ],
   "source": [
    "#top(prints top elements)\n",
    "rdd.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb95572-f1db-4f87-aacb-d70b55cef92c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 8, 6]\n[2, 7, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "#takeSample (take some sample random values from list, if it's true it will repeat same value again ,False means unique)\n",
    "print(rdd.takeSample(True,4))\n",
    "print(rdd.takeSample(False,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74878cb8-8996-45c8-93e0-997337ec9490",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n1\n9\n45\n5.0\n2.581988897471611\n(count: 9, mean: 5.0, stdev: 2.581988897471611, max: 9.0, min: 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Action   - Aggregate functions - gives single output value\n",
    "# Action   - min,max,sum(),mean(),stdev\n",
    "# Action   - count( no.of elements)\n",
    "# Actions - stats- complete info about count,mean,stdev,max,min\n",
    "print(rdd.count())\n",
    "print(rdd.min())\n",
    "print(rdd.max())\n",
    "print(rdd.sum())\n",
    "print(rdd.mean())\n",
    "print(rdd.stdev())\n",
    "print(rdd.stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a433c2-7098-4ef3-9004-90bb35094000",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n-25\n362880\n"
     ]
    }
   ],
   "source": [
    "# Actions - Aggregate functions - gives single output value\n",
    "# Actions- reduce that aggregates a data set(RDD) element using function\n",
    "\n",
    "print(rdd.reduce(lambda x,y : x+y))\n",
    "\n",
    "print( rdd.reduce( lambda x,y: x-y))\n",
    "\n",
    "print(rdd.reduce(lambda x,y : x*y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016c0a99-5d10-4445-9488-8bf64b88b10f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {('s', 1): 1, ('a', 2): 1, ('n', 3): 1, ('d', 4): 1, ('e', 5): 1, ('e', 6): 1, ('p', 7): 1})\ndefaultdict(<class 'int'>, {'s': 1, 'a': 1, 'n': 1, 'd': 1, 'e': 2, 'p': 1})\n"
     ]
    }
   ],
   "source": [
    "#Actions - CountByValue,countByKey - count of same values\n",
    "print(rdd2.countByValue())\n",
    "print(rdd2.countByKey()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e541ee59-e59c-453b-900f-74748515270e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[117]: 54"
     ]
    }
   ],
   "source": [
    "#Actions \n",
    "#fold - aggregate the elements of each partition\n",
    "from operator import *\n",
    "rdd.fold(1,add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaaf27d0-df47-46fd-8b2c-895678efaed1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.666666666666667\n7.5\n"
     ]
    }
   ],
   "source": [
    "#Actions\n",
    "#variance (all n values variance)\n",
    "#sample variance - (n-1) values variance\n",
    "print(rdd.variance())\n",
    "print(rdd.sampleVariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b654f9-9471-4dfb-a3ac-f6963462b16b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<div class=\"ansiout\">command-320156528101109:1: error: value rmR is not a member of com.databricks.dbutils_v1.DbfsUtils\ndbutils.fs.rmR(&quot;/FileStore/shared_uploads/nookala382@gmail.com/&quot;) // SAFE COMMAND FROM MACRO\n           ^\n</div>",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs\n",
    "\n",
    "rmR /FileStore/shared_uploads/nookala382@gmail.com/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530a9ecf-40be-4a1b-a59c-2cd0a37ec582",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-467523204737831>:5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Actions\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#saveAsTextFile   -- text file format\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#saveAsPickleFile -- binary file format\u001B[39;00m\n",
       "\u001B[0;32m----> 5\u001B[0m rdd\u001B[38;5;241m.\u001B[39msaveAsTextFile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/shared_uploads/nookala382@gmail.com/output/csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m rdd\u001B[38;5;241m.\u001B[39msaveAsPickleFile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/shared_uploads/nookala382@gmail.com/output/pickle\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/rdd.py:3432\u001B[0m, in \u001B[0;36mRDD.saveAsTextFile\u001B[0;34m(self, path, compressionCodecClass)\u001B[0m\n",
       "\u001B[1;32m   3430\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonRDD\u001B[38;5;241m.\u001B[39msaveAsTextFileImpl(keyed\u001B[38;5;241m.\u001B[39m_jrdd, path, compressionCodecClass)\n",
       "\u001B[1;32m   3431\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 3432\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTextFileImpl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
       "\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl.\n",
       ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/shared_uploads/nookala382@gmail.com/output/csv already exists\n",
       "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
       "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n",
       "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1650)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1650)\n",
       "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1636)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n",
       "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n",
       "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1636)\n",
       "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:573)\n",
       "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:572)\n",
       "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
       "\tat org.apache.spark.api.python.PythonRDD$._saveAsTextFile(PythonRDD.scala:913)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.saveAsTextFileImpl(PythonRDD.scala:881)\n",
       "\tat org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\nFile \u001B[0;32m<command-467523204737831>:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#Actions\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#saveAsTextFile   -- text file format\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#saveAsPickleFile -- binary file format\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m rdd\u001B[38;5;241m.\u001B[39msaveAsTextFile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/shared_uploads/nookala382@gmail.com/output/csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m rdd\u001B[38;5;241m.\u001B[39msaveAsPickleFile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/FileStore/shared_uploads/nookala382@gmail.com/output/pickle\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/rdd.py:3432\u001B[0m, in \u001B[0;36mRDD.saveAsTextFile\u001B[0;34m(self, path, compressionCodecClass)\u001B[0m\n\u001B[1;32m   3430\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mctx\u001B[38;5;241m.\u001B[39m_jvm\u001B[38;5;241m.\u001B[39mPythonRDD\u001B[38;5;241m.\u001B[39msaveAsTextFileImpl(keyed\u001B[38;5;241m.\u001B[39m_jrdd, path, compressionCodecClass)\n\u001B[1;32m   3431\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3432\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPythonRDD\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTextFileImpl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyed\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jrdd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:228\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    227\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 228\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    230\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/shared_uploads/nookala382@gmail.com/output/csv already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:303)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:75)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1650)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1650)\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1636)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:445)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1636)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:573)\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:572)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat org.apache.spark.api.python.PythonRDD$._saveAsTextFile(PythonRDD.scala:913)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsTextFileImpl(PythonRDD.scala:881)\n\tat org.apache.spark.api.python.PythonRDD.saveAsTextFileImpl(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/FileStore/shared_uploads/nookala382@gmail.com/output/csv already exists",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Actions\n",
    "#saveAsTextFile   -- text file format\n",
    "#saveAsPickleFile -- binary file format\n",
    "\n",
    "rdd.saveAsTextFile('/FileStore/shared_uploads/nookala382@gmail.com/output/csv')\n",
    "rdd.saveAsPickleFile('/FileStore/shared_uploads/nookala382@gmail.com/output/pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e7caadd-6817-43e7-8f56-613ed8437f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#6.RDD transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a846f2d3-ace1-488c-ad2a-f3a078edb75d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[124]: [(1, 3), (2, 6), (3, 9), (4, 12), (5, 15), (6, 18), (7, 21), (8, 24), (9, 27)]"
     ]
    }
   ],
   "source": [
    "# map - Return new distributed dataset formed by passing each element of source through a function\n",
    "\n",
    "x = rdd.map( lambda x: (x,x*3))\n",
    "x.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dedb9b7-5a70-4bbb-878e-38d721ce82bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[122]: [1, 3, 2, 6, 3, 9, 4, 12, 5, 15, 6, 18, 7, 21, 8, 24, 9, 27]"
     ]
    }
   ],
   "source": [
    "#flatMap -Similar to map ,but each input iteam can be mapped to 0 or more output items\n",
    "y = rdd.flatMap( lambda x: (x,x*3))\n",
    "y.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cab4bf-dfdc-4594-87e2-b3df3b0eacc5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[126]: [2, 4, 6, 8]"
     ]
    }
   ],
   "source": [
    "# filter - Return a new dataset formed by selecting those elements of source\n",
    "\n",
    "z = rdd.filter( lambda x : x %2 ==0)\n",
    "\n",
    "z.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e05e5c81-0a60-46b3-81d0-c6f8cde0accb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[129]: [[1, 2], [3, 4], [5, 6], [7, 8, 9]]"
     ]
    }
   ],
   "source": [
    "#Partition given list and print partition lists\n",
    "\n",
    "rdd_part = sc.parallelize(l,4)\n",
    "rdd_part.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c64531-763b-4adb-a24f-a625f134465c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[133]: [[3], [7], [11], [24]]"
     ]
    }
   ],
   "source": [
    "#mapPartitions - Similar to map but runs separetely on each partition\n",
    "#partition wise total sum\n",
    "def x(iterator) :yield sum(iterator)\n",
    "map_part = rdd_part.mapPartitions(x)\n",
    "map_part.glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a08cceae-e31e-42f7-b87c-b25cf4d62bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[134]: [[1], [3], [5], [7]]"
     ]
    }
   ],
   "source": [
    "#partition wise min value\n",
    "\n",
    "def y(iterator) : yield min(iterator)\n",
    "\n",
    "map_part2 = rdd_part.mapPartitions(y)\n",
    "map_part2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af1efd0-6b4e-41d9-9ab6-ffa462152a11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[136]: [[(0, 3)], [(1, 7)], [(2, 11)], [(3, 24)]]"
     ]
    }
   ],
   "source": [
    "#mapPartitionWIthIndex - Similar to Mappartition ,but also provides an integer value is index of partition\n",
    "\n",
    "def x1(index,iterator) : yield (index,sum(iterator))\n",
    "\n",
    "map_par_index = rdd_part.mapPartitionsWithIndex(x1)\n",
    "\n",
    "map_par_index.glom().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61e94ed-d4cc-4b9a-b62c-37a381c2da41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 3, 4, 5, 7, 8, 9, 9, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "#sample - a fraction of fraction data ,with or without replacement ,using a given random number\n",
    "\n",
    "#sample(withreplacement.fraction,seed)  ,if (with replacement) - true - will repeat same value ,else (withreplacement) -False - unique values\n",
    "\n",
    "rdd_sample = rdd.sample(True,1)\n",
    "print(rdd_sample.collect())\n",
    "\n",
    "rdd_sample1 = rdd.sample(False,1)\n",
    "\n",
    "print(rdd_sample1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc183a75-421e-4767-8fcf-e0fbb7a8ff8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19]\n"
     ]
    }
   ],
   "source": [
    "#union\n",
    "\n",
    "rdd_union = rdd.union(rdd1)\n",
    "print(rdd_union.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef18c25-92a7-48f7-8f1b-a34868411ef4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[150]: [1, 17, 3, 19, 5, 7, 9, 11, 13, 15]"
     ]
    }
   ],
   "source": [
    "#intersection\n",
    "\n",
    "rdd_intersection = rdd.intersection(rdd1)\n",
    "rdd_intersection.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f1bcce-cbe6-4400-a8c6-2fc1f0b80ee0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('s', [1]), ('a', [2]), ('e', [5, 6]), ('n', [3]), ('p', [7]), ('d', [4])]\n"
     ]
    }
   ],
   "source": [
    "#GroupByKey\n",
    "rdd_group = rdd2.groupByKey()\n",
    "print([(j[0],[i for i in j[1]]) for j in rdd_group.collect()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c7fc23-8fc8-4419-acdd-a6bf9973e5fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[157]: [8, 16, 1, 9, 17, 2, 10, 18, 3, 11, 19, 4, 12, 5, 13, 6, 14, 7, 15]"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "rdd_dist = rdd.distinct()\n",
    "rdd_dist.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33ef327-4f45-4df6-bb8e-65b224bba443",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "read file into rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e35a2dc-a6f5-4b68-9508-d360094a8395",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd3 = sc.textFile('dbfs:/FileStore/shared_uploads/nookala382@gmail.com/emp_pipe.txt')\n",
    "rdd3.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28820bc-58d6-489b-94f4-5fd008fb40be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#7.Spark Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee3c42e-b6e8-4f99-81a5-6efa99fc627b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[123]: ['EMPNO|ENAME|JOB|MGR|HIREDATE|SAL|DEPTNO|UPDATED_DATE',\n '7369|SMITH|CLERK|7902|17-12-1980|800|20|01-01-2022',\n '7499|ALLEN|SALESMAN|7698|20-02-1981|1600|30|02-01-2022',\n '7521|WARD|SALESMAN|7698|22-02-1981|1250|30|03-01-2022',\n '7566|JONES|MANAGER|null|04-02-1981|2975|20|04-01-2022',\n '7654|null|null|7698|21-09-1981|null|30|05-01-2022',\n '7698|SGR|MANAGER|7839|05-01-1981|2850|30|06-01-2022',\n '7782|RAVI|null|null|06-09-1981|2450|10|07-01-2022',\n '7788|SCOTT|ANALYST|7566|19-04-1987|3000|20|08-01-2022',\n '7839|null|PRESIDENT|null|null|5000|10|null']"
     ]
    }
   ],
   "source": [
    "#caching -caching is  used to save the data(RDD/Dataframe/Dataset) in a cluster-wide in memory,\n",
    "#cache() method default saves data in MEMORY_ONLY.\n",
    "#Used to store small amount od data. This is Very Useful for accessing repeated data .\n",
    "#such as querying a small data set or when running an iterative algorithm\n",
    "\n",
    "rdd3.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fdbb756-2e34-4698-9c30-e0913607d524",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#8.Spark Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bcf77b0-2f08-42d7-a0f3-a0260fdd68f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# same like cahe ( stores data in memeory) but we can store large amout of  data\n",
    "# persist() method is used to store it to the user-defined storage levels like (memory only , disk only ,memory and disk only etc)\n",
    "\n",
    "rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "#rdd.persist(pyspark.StorageLevel.DISK_ONLY)\n",
    "\n",
    "#rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a81b2cf-93ea-4754-9424-df0ed038fcc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clears cache and persist data manually\n",
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "426c57a7-89e6-423e-890e-f4435c339525",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#9.Broadcast variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eedc272-b133-4dba-b2fa-227b49138adc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# allow programmer to keep a read only variable cached on each machine rather than shipping a copy of it with tasks\n",
    "# spark useses efficient broadcast algoritham to reduce communication cost\n",
    "\n",
    "\n",
    "broad = sc.broadcast([1,2,3,4,5,6])\n",
    "\n",
    "print(broad.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edea9d73-1cb4-4464-9f92-f80efa0e639e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clear the boarcast variables by using unpersist or destroy\n",
    "\n",
    "#broad.unpersist()\n",
    "#OR \n",
    "broad.destroy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff35b18-a34f-4ea3-adc3-cdc58a7b097f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#10.Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209aa091-a81c-4304-ae04-e8e3669b2f3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "from pyspark.accumulators import Accumulator\n",
    "\n",
    "accum=sc.accumulator(10)\n",
    "\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca26e080-087a-4368-aab9-f5e100ecd38f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#11.coalesce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdb0889-b3d8-4fac-8c2a-e1247c2a09f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd_cal = rdd.coalesce(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da5d04fc-b81a-4024-aa12-9c820e69de94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[175]: 3"
     ]
    }
   ],
   "source": [
    "rdd_cal.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a82ed1e6-ecfe-4475-9b17-fb6a2253a2e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[189]: [[1, 2, 3, 4], [5, 6, 7, 8, 9, 10, 11, 12], [13, 14, 15, 16, 17, 18, 19]]"
     ]
    }
   ],
   "source": [
    "rdd_cal.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1113c4d8-59f2-4273-a720-34cd39087ef4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#12.repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19775871-1c6d-4ae0-87dc-df3b891bf5f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd_repart = rdd.repartition(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3734958c-27e0-47cd-a0cb-0bf4401981b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[187]: 3"
     ]
    }
   ],
   "source": [
    "rdd_repart.getNumPartitions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9547b1e-2f0b-4e45-b121-bb4875aa0f46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[188]: [[1, 2, 7, 8, 9, 10, 15, 16], [3, 4], [5, 6, 11, 12, 13, 14, 17, 18, 19]]"
     ]
    }
   ],
   "source": [
    "rdd_repart.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e2eecfb-0b00-4562-9fcb-9ec833173247",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#13.Transformations - Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c89f6a9-bc35-4f1a-adca-1e0410e098d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "J1 = sc.parallelize([('A',2),('B',3),('C',4),('D',5),('E',6),('F',7)])\n",
    "\n",
    "J2 = sc.parallelize([('C',2),('D',3),('E',4),('F',5),('G',6),('H',7)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "018eda81-1325-4243-8a40-b001d40442c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[193]: [('F', (7, 5)), ('D', (5, 3)), ('E', (6, 4)), ('C', (4, 2))]"
     ]
    }
   ],
   "source": [
    "#join/inner Join - takes two pair of rdd ,return only matched records from both RDD's\n",
    "\n",
    "innerjoin = J1.join(J2)\n",
    "innerjoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45638246-3500-45db-a58e-b11c792e2bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[196]: [('B', (3, None)),\n ('F', (7, 5)),\n ('D', (5, 3)),\n ('E', (6, 4)),\n ('C', (4, 2)),\n ('A', (2, None))]"
     ]
    }
   ],
   "source": [
    "# leftouterjoin  - returns matched records from both rdd and unmatched records from left rdd\n",
    "\n",
    "leftjoin = J1.leftOuterJoin(J2)\n",
    "leftjoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcbda3b-f17a-40a3-b02e-25a9cb997ec9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[197]: [('F', (7, 5)),\n ('D', (5, 3)),\n ('E', (6, 4)),\n ('G', (None, 6)),\n ('H', (None, 7)),\n ('C', (4, 2))]"
     ]
    }
   ],
   "source": [
    "#RightOuterJoin - returns matched records from both rdd and unmatched from right rdd\n",
    "\n",
    "rightjoin = J1.rightOuterJoin(J2)\n",
    "rightjoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7168e4-3b79-436a-932a-339ae3122c66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[198]: [('B', (3, None)),\n ('F', (7, 5)),\n ('D', (5, 3)),\n ('E', (6, 4)),\n ('G', (None, 6)),\n ('H', (None, 7)),\n ('C', (4, 2)),\n ('A', (2, None))]"
     ]
    }
   ],
   "source": [
    "#fullOuterJoin - returned all records from both RDD'S\n",
    "\n",
    "fulljoin = J1.fullOuterJoin(J2)\n",
    "fulljoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4661a49-894d-44e4-b695-0c7a7847e356",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[199]: [(('A', 2), ('C', 2)),\n (('A', 2), ('D', 3)),\n (('A', 2), ('E', 4)),\n (('A', 2), ('F', 5)),\n (('A', 2), ('G', 6)),\n (('A', 2), ('H', 7)),\n (('B', 3), ('C', 2)),\n (('B', 3), ('D', 3)),\n (('B', 3), ('E', 4)),\n (('B', 3), ('F', 5)),\n (('B', 3), ('G', 6)),\n (('B', 3), ('H', 7)),\n (('C', 4), ('C', 2)),\n (('C', 4), ('D', 3)),\n (('C', 4), ('E', 4)),\n (('C', 4), ('F', 5)),\n (('C', 4), ('G', 6)),\n (('C', 4), ('H', 7)),\n (('D', 5), ('C', 2)),\n (('D', 5), ('D', 3)),\n (('D', 5), ('E', 4)),\n (('D', 5), ('F', 5)),\n (('D', 5), ('G', 6)),\n (('D', 5), ('H', 7)),\n (('E', 6), ('C', 2)),\n (('E', 6), ('D', 3)),\n (('E', 6), ('E', 4)),\n (('E', 6), ('F', 5)),\n (('E', 6), ('G', 6)),\n (('E', 6), ('H', 7)),\n (('F', 7), ('C', 2)),\n (('F', 7), ('D', 3)),\n (('F', 7), ('E', 4)),\n (('F', 7), ('F', 5)),\n (('F', 7), ('G', 6)),\n (('F', 7), ('H', 7))]"
     ]
    }
   ],
   "source": [
    "#cartesian join - cross product of both elements of RDD\n",
    "\n",
    "cartisanjoin = J1.cartesian(J2)\n",
    "cartisanjoin.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24ca272c-0363-452d-b03b-ea510403d9a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 320156528101109,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "RDD",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
