{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3in-2lvQJZb2",
        "HQTXxbqnJYFw",
        "3NkJYUG5MbwO",
        "mQM_-Rq31OHy",
        "u22VawEEFCLg",
        "9sVH_dl3jk2m",
        "AEXYbd4AjHe5",
        "MPp15hFA-b5g",
        "ANG4ONGDTCRw",
        "1d7XuxL5X50r",
        "Egb13y1nnydU",
        "6KOrxwloiaUn",
        "BAuEVPpOuRmG",
        "pERd85OnPkP0",
        "SogxNiM-NzVp",
        "AiXskdNP3leY",
        "Af6xhc4-Px-3",
        "_SoyRZHsFe3D",
        "bAzJc7yDRGeP",
        "1b2vS5BA6BwK",
        "CyDwbLPUMKAw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pyspark"
      ],
      "metadata": {
        "id": "3in-2lvQJZb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQbx377bOs0o",
        "outputId": "11b9703e-b4a1-4eb2-f3bd-d7c3ac8b2bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=eed8af924a50154117fcadbc5828cc23311665e690199b39a93ff43fd28d9df1\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating spark Session"
      ],
      "metadata": {
        "id": "HQTXxbqnJYFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('usecases').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "aF9MfnxdPLam",
        "outputId": "6df7e25f-4a34-42c9-ff4b-12c9c69967a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f66c77c7c70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7459bf318473:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>usecases</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# read csv file data"
      ],
      "metadata": {
        "id": "3NkJYUG5MbwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv = spark.read.format('csv').option('header',True).option('inferSchema',True).load('/content/employee.csv')\n",
        "\n",
        "df_csv.show(10)\n",
        "df_csv.printSchema()\n",
        "df_csv.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovCxp3ClMa24",
        "outputId": "88c3926c-8490-4597-b77c-b040d7f0c28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------+------+----------+----+------+------------+\n",
            "|EMPNO| ENAME|      JOB|Manger|  HIREDATE| SAL|DEPTNO|UPDATED_DATE|\n",
            "+-----+------+---------+------+----------+----+------+------------+\n",
            "| 7369| SMITH|    CLERK|  7902|17-12-1980| 800|    20|  01-01-2022|\n",
            "| 7499| ALLEN| SALESMAN|  7698|20-02-1981|1600|    30|  02-01-2022|\n",
            "| 7521|  WARD| SALESMAN|  7698|22-02-1981|1250|    30|  03-01-2022|\n",
            "| 7566| JONES|  MANAGER|  7839|04-02-1981|2975|    20|  04-01-2022|\n",
            "| 7654|MARTIN| SALESMAN|  7698|21-09-1981|1250|    30|  05-01-2022|\n",
            "| 7698|   SGR|  MANAGER|  7839|05-01-1981|2850|    30|  06-01-2022|\n",
            "| 7782|  RAVI|  MANAGER|  7839|06-09-1981|2450|    10|  07-01-2022|\n",
            "| 7788| SCOTT|  ANALYST|  7566|19-04-1987|3000|    20|  08-01-2022|\n",
            "| 7839|  KING|PRESIDENT|  null|      null|5000|    10|        null|\n",
            "| 7844|TURNER| SALESMAN|  7698|09-08-1981|1500|    30|  01-02-2022|\n",
            "+-----+------+---------+------+----------+----+------+------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "root\n",
            " |-- EMPNO: string (nullable = true)\n",
            " |-- ENAME: string (nullable = true)\n",
            " |-- JOB: string (nullable = true)\n",
            " |-- Manger: string (nullable = true)\n",
            " |-- HIREDATE: string (nullable = true)\n",
            " |-- SAL: string (nullable = true)\n",
            " |-- DEPTNO: string (nullable = true)\n",
            " |-- UPDATED_DATE: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwzVY2UUNTLg",
        "outputId": "c25fdaf7-9848-47a4-d0f4-07ec4a59af0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------+------+----------+----+------+------------+------------+----------------+\n",
            "|EMPNO| ENAME|      JOB|Manger|  HIREDATE| SAL|DEPTNO|UPDATED_DATE|New_HIREDATE|New_UPDATED_DATE|\n",
            "+-----+------+---------+------+----------+----+------+------------+------------+----------------+\n",
            "| 7369| SMITH|    CLERK|  7902|17-12-1980| 800|    20|  01-01-2022|  1980-01-17|      2022-01-01|\n",
            "| 7499| ALLEN| SALESMAN|  7698|20-02-1981|1600|    30|  02-01-2022|  1981-01-20|      2022-01-02|\n",
            "| 7521|  WARD| SALESMAN|  7698|22-02-1981|1250|    30|  03-01-2022|  1981-01-22|      2022-01-03|\n",
            "| 7566| JONES|  MANAGER|  7839|04-02-1981|2975|    20|  04-01-2022|  1981-01-04|      2022-01-04|\n",
            "| 7654|MARTIN| SALESMAN|  7698|21-09-1981|1250|    30|  05-01-2022|  1981-01-21|      2022-01-05|\n",
            "| 7698|   SGR|  MANAGER|  7839|05-01-1981|2850|    30|  06-01-2022|  1981-01-05|      2022-01-06|\n",
            "| 7782|  RAVI|  MANAGER|  7839|06-09-1981|2450|    10|  07-01-2022|  1981-01-06|      2022-01-07|\n",
            "| 7788| SCOTT|  ANALYST|  7566|19-04-1987|3000|    20|  08-01-2022|  1987-01-19|      2022-01-08|\n",
            "| 7839|  KING|PRESIDENT|  null|      null|5000|    10|        null|        null|            null|\n",
            "| 7844|TURNER| SALESMAN|  7698|09-08-1981|1500|    30|  01-02-2022|  1981-01-09|      2022-01-01|\n",
            "| 7876| ADAMS|    CLERK|  7788|23-05-1987|1100|    20|  02-02-2022|  1987-01-23|      2022-01-02|\n",
            "| 7900| JAMES|    CLERK|  7698|12-03-1981| 950|    30|  03-02-2022|  1981-01-12|      2022-01-03|\n",
            "| 7902|  FORD|  ANALYST|  7566|12-03-1981|3000|    20|  04-02-2022|  1981-01-12|      2022-01-04|\n",
            "| 7934|MILLER|    CLERK|  7782|01-03-1982|1300|    10|  05-02-2022|  1982-01-01|      2022-01-05|\n",
            "| 1234|SEKHAR|   doctor|  7777|      null| 667|    80|  06-02-2022|        null|      2022-01-06|\n",
            "| 7369| SMITH|    CLERK|  7902|17-12-1980| 800|    20|  07-02-2022|  1980-01-17|      2022-01-07|\n",
            "| 7499| ALLEN| SALESMAN|  7698|20-02-1981|1600|    30|  08-02-2022|  1981-01-20|      2022-01-08|\n",
            "| 7521|  WARD| SALESMAN|  7698|22-02-1981|1250|    30|        null|  1981-01-22|            null|\n",
            "| 7566| JONES|  MANAGER|  7839|04-02-1981|2975|    20|  01-02-2021|  1981-01-04|      2021-01-01|\n",
            "| 7654|MARTIN| SALESMAN|  7698|21-09-1981|1250|    30|  02-02-2021|  1981-01-21|      2021-01-02|\n",
            "+-----+------+---------+------+----------+----+------+------------+------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# convert from String (dd-mm-yyyy) date format to spark date format (yyyy-mm-dd)"
      ],
      "metadata": {
        "id": "mQM_-Rq31OHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# date format in Traditional databases is dd-mm-yyyy\n",
        "# date format in spark is yyyy-mm-dd\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "df_csv1 = df_csv.withColumn('New_HIREDATE',to_date('HIREDATE','dd-mm-yyyy')).withColumn('New_UPDATED_DATE',to_date('UPDATED_DATE','dd-mm-yyyy')).drop('HIREDATE','UPDATED_DATE')\n",
        "\n",
        "df_csv1.show(10)\n",
        "df_csv1.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxu3d5Aq33FU",
        "outputId": "71ef5a85-f006-4b6c-d76b-5771e953752d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------+------+----+------+------------+----------------+\n",
            "|EMPNO| ENAME|      JOB|Manger| SAL|DEPTNO|New_HIREDATE|New_UPDATED_DATE|\n",
            "+-----+------+---------+------+----+------+------------+----------------+\n",
            "| 7369| SMITH|    CLERK|  7902| 800|    20|  1980-01-17|      2022-01-01|\n",
            "| 7499| ALLEN| SALESMAN|  7698|1600|    30|  1981-01-20|      2022-01-02|\n",
            "| 7521|  WARD| SALESMAN|  7698|1250|    30|  1981-01-22|      2022-01-03|\n",
            "| 7566| JONES|  MANAGER|  7839|2975|    20|  1981-01-04|      2022-01-04|\n",
            "| 7654|MARTIN| SALESMAN|  7698|1250|    30|  1981-01-21|      2022-01-05|\n",
            "| 7698|   SGR|  MANAGER|  7839|2850|    30|  1981-01-05|      2022-01-06|\n",
            "| 7782|  RAVI|  MANAGER|  7839|2450|    10|  1981-01-06|      2022-01-07|\n",
            "| 7788| SCOTT|  ANALYST|  7566|3000|    20|  1987-01-19|      2022-01-08|\n",
            "| 7839|  KING|PRESIDENT|  null|5000|    10|        null|            null|\n",
            "| 7844|TURNER| SALESMAN|  7698|1500|    30|  1981-01-09|      2022-01-01|\n",
            "+-----+------+---------+------+----+------+------------+----------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "root\n",
            " |-- EMPNO: string (nullable = true)\n",
            " |-- ENAME: string (nullable = true)\n",
            " |-- JOB: string (nullable = true)\n",
            " |-- Manger: string (nullable = true)\n",
            " |-- SAL: string (nullable = true)\n",
            " |-- DEPTNO: string (nullable = true)\n",
            " |-- New_HIREDATE: date (nullable = true)\n",
            " |-- New_UPDATED_DATE: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4CUCVvnWVcqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding new columns with different values and drop null values"
      ],
      "metadata": {
        "id": "u22VawEEFCLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv2 = df_csv1.withColumn('Position',when( col('JOB') == 'CLERK' ,'Level3').when( col('JOB') == 'SALESMAN' ,'Level4')\n",
        ".when(col('JOB') == 'MANAGER','Level2').when( col('JOB') == 'PRESIDENT','Level1') ).dropna()\n",
        "\n",
        "df_csv2.show(10)\n",
        "df_csv2.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_jtJEyJqBhB",
        "outputId": "e2becba8-ad8f-4b2f-ae93-f8d0ca63cb39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------+------+----+------+------------+----------------+--------+\n",
            "|EMPNO| ENAME|     JOB|Manger| SAL|DEPTNO|New_HIREDATE|New_UPDATED_DATE|Position|\n",
            "+-----+------+--------+------+----+------+------------+----------------+--------+\n",
            "| 7369| SMITH|   CLERK|  7902| 800|    20|  1980-01-17|      2022-01-01|  Level3|\n",
            "| 7499| ALLEN|SALESMAN|  7698|1600|    30|  1981-01-20|      2022-01-02|  Level4|\n",
            "| 7521|  WARD|SALESMAN|  7698|1250|    30|  1981-01-22|      2022-01-03|  Level4|\n",
            "| 7566| JONES| MANAGER|  7839|2975|    20|  1981-01-04|      2022-01-04|  Level2|\n",
            "| 7654|MARTIN|SALESMAN|  7698|1250|    30|  1981-01-21|      2022-01-05|  Level4|\n",
            "| 7698|   SGR| MANAGER|  7839|2850|    30|  1981-01-05|      2022-01-06|  Level2|\n",
            "| 7782|  RAVI| MANAGER|  7839|2450|    10|  1981-01-06|      2022-01-07|  Level2|\n",
            "| 7844|TURNER|SALESMAN|  7698|1500|    30|  1981-01-09|      2022-01-01|  Level4|\n",
            "| 7876| ADAMS|   CLERK|  7788|1100|    20|  1987-01-23|      2022-01-02|  Level3|\n",
            "| 7900| JAMES|   CLERK|  7698| 950|    30|  1981-01-12|      2022-01-03|  Level3|\n",
            "+-----+------+--------+------+----+------+------------+----------------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Dataframe to Hive table"
      ],
      "metadata": {
        "id": "XC-rxv57hRCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv2.write.partitionBy('JOB').mode('append').saveAsTable('Employee1')"
      ],
      "metadata": {
        "id": "7kZsmzghhTiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# write spark sql query"
      ],
      "metadata": {
        "id": "9sVH_dl3jk2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select * from Employee1').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpjoJUx5jogA",
        "outputId": "5e79408b-a321-4704-cc10-8d8a2a04f866"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+----+------+------------+----------------+--------+--------+\n",
            "|EMPNO| ENAME|Manger| SAL|DEPTNO|New_HIREDATE|New_UPDATED_DATE|Position|     JOB|\n",
            "+-----+------+------+----+------+------------+----------------+--------+--------+\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-01|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2022-01-02|  Level3|   CLERK|\n",
            "| 7900| JAMES|  7698| 950|    30|  1981-01-12|      2022-01-03|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2022-01-05|  Level3|   CLERK|\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-07|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2021-01-08|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2021-01-02|  Level3|   CLERK|\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-01|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2022-01-02|  Level3|   CLERK|\n",
            "| 7900| JAMES|  7698| 950|    30|  1981-01-12|      2022-01-03|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2022-01-05|  Level3|   CLERK|\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-07|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2021-01-08|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2021-01-02|  Level3|   CLERK|\n",
            "| 7499| ALLEN|  7698|1600|    30|  1981-01-20|      2022-01-02|  Level4|SALESMAN|\n",
            "| 7521|  WARD|  7698|1250|    30|  1981-01-22|      2022-01-03|  Level4|SALESMAN|\n",
            "| 7654|MARTIN|  7698|1250|    30|  1981-01-21|      2022-01-05|  Level4|SALESMAN|\n",
            "| 7844|TURNER|  7698|1500|    30|  1981-01-09|      2022-01-01|  Level4|SALESMAN|\n",
            "| 7499| ALLEN|  7698|1600|    30|  1981-01-20|      2022-01-08|  Level4|SALESMAN|\n",
            "| 7654|MARTIN|  7698|1250|    30|  1981-01-21|      2021-01-02|  Level4|SALESMAN|\n",
            "+-----+------+------+----+------+------------+----------------+--------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Dataframe from Hive table"
      ],
      "metadata": {
        "id": "AEXYbd4AjHe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hive = spark.read.table('Employee1')\n",
        "\n",
        "df_hive.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv-O52LCicXB",
        "outputId": "9b6bd4bd-cbf6-4b4c-c4dc-e33ce74d4f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+----+------+------------+----------------+--------+--------+\n",
            "|EMPNO| ENAME|Manger| SAL|DEPTNO|New_HIREDATE|New_UPDATED_DATE|Position|     JOB|\n",
            "+-----+------+------+----+------+------------+----------------+--------+--------+\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-01|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2022-01-02|  Level3|   CLERK|\n",
            "| 7900| JAMES|  7698| 950|    30|  1981-01-12|      2022-01-03|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2022-01-05|  Level3|   CLERK|\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-07|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2021-01-08|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2021-01-02|  Level3|   CLERK|\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-01|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2022-01-02|  Level3|   CLERK|\n",
            "| 7900| JAMES|  7698| 950|    30|  1981-01-12|      2022-01-03|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2022-01-05|  Level3|   CLERK|\n",
            "| 7369| SMITH|  7902| 800|    20|  1980-01-17|      2022-01-07|  Level3|   CLERK|\n",
            "| 7876| ADAMS|  7788|1100|    20|  1987-01-23|      2021-01-08|  Level3|   CLERK|\n",
            "| 7934|MILLER|  7782|1300|    10|  1982-01-01|      2021-01-02|  Level3|   CLERK|\n",
            "| 7499| ALLEN|  7698|1600|    30|  1981-01-20|      2022-01-02|  Level4|SALESMAN|\n",
            "| 7521|  WARD|  7698|1250|    30|  1981-01-22|      2022-01-03|  Level4|SALESMAN|\n",
            "| 7654|MARTIN|  7698|1250|    30|  1981-01-21|      2022-01-05|  Level4|SALESMAN|\n",
            "| 7844|TURNER|  7698|1500|    30|  1981-01-09|      2022-01-01|  Level4|SALESMAN|\n",
            "| 7499| ALLEN|  7698|1600|    30|  1981-01-20|      2022-01-08|  Level4|SALESMAN|\n",
            "| 7654|MARTIN|  7698|1250|    30|  1981-01-21|      2021-01-02|  Level4|SALESMAN|\n",
            "+-----+------+------+----+------+------------+----------------+--------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding current timestamp to dataframe"
      ],
      "metadata": {
        "id": "h63qo9lnPaPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hive1 = df_hive.withColumn('Date',current_timestamp())\n",
        "\n",
        "df_hive1.show(truncate = False)\n"
      ],
      "metadata": {
        "id": "DVccCiUwPYeZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50072171-7ea7-4b86-808f-3d55aa610cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+------+----+------+------------+----------------+--------+--------+-------------------------+\n",
            "|EMPNO|ENAME |Manger|SAL |DEPTNO|New_HIREDATE|New_UPDATED_DATE|Position|JOB     |Date                     |\n",
            "+-----+------+------+----+------+------------+----------------+--------+--------+-------------------------+\n",
            "|7369 |SMITH |7902  |800 |20    |1980-01-17  |2022-01-01      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7876 |ADAMS |7788  |1100|20    |1987-01-23  |2022-01-02      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7900 |JAMES |7698  |950 |30    |1981-01-12  |2022-01-03      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7934 |MILLER|7782  |1300|10    |1982-01-01  |2022-01-05      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7369 |SMITH |7902  |800 |20    |1980-01-17  |2022-01-07      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7876 |ADAMS |7788  |1100|20    |1987-01-23  |2021-01-08      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7934 |MILLER|7782  |1300|10    |1982-01-01  |2021-01-02      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7369 |SMITH |7902  |800 |20    |1980-01-17  |2022-01-01      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7876 |ADAMS |7788  |1100|20    |1987-01-23  |2022-01-02      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7900 |JAMES |7698  |950 |30    |1981-01-12  |2022-01-03      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7934 |MILLER|7782  |1300|10    |1982-01-01  |2022-01-05      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7369 |SMITH |7902  |800 |20    |1980-01-17  |2022-01-07      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7876 |ADAMS |7788  |1100|20    |1987-01-23  |2021-01-08      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7934 |MILLER|7782  |1300|10    |1982-01-01  |2021-01-02      |Level3  |CLERK   |2023-06-22 10:39:09.58453|\n",
            "|7499 |ALLEN |7698  |1600|30    |1981-01-20  |2022-01-02      |Level4  |SALESMAN|2023-06-22 10:39:09.58453|\n",
            "|7521 |WARD  |7698  |1250|30    |1981-01-22  |2022-01-03      |Level4  |SALESMAN|2023-06-22 10:39:09.58453|\n",
            "|7654 |MARTIN|7698  |1250|30    |1981-01-21  |2022-01-05      |Level4  |SALESMAN|2023-06-22 10:39:09.58453|\n",
            "|7844 |TURNER|7698  |1500|30    |1981-01-09  |2022-01-01      |Level4  |SALESMAN|2023-06-22 10:39:09.58453|\n",
            "|7499 |ALLEN |7698  |1600|30    |1981-01-20  |2022-01-08      |Level4  |SALESMAN|2023-06-22 10:39:09.58453|\n",
            "|7654 |MARTIN|7698  |1250|30    |1981-01-21  |2021-01-02      |Level4  |SALESMAN|2023-06-22 10:39:09.58453|\n",
            "+-----+------+------+----+------+------------+----------------+--------+--------+-------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Frame from mysql table"
      ],
      "metadata": {
        "id": "MPp15hFA-b5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "df_mysql = spark.read.format('jdbc').\\\n",
        "           option('url','jdbc:mysql://localhost:3306').\\\n",
        "           option('driver','com.mysql.jdbc.Driver').\\\n",
        "           option('user','root').\\\n",
        "           option('password','sandeep').\\\n",
        "           option('query','select * from sandeep.emp_table').\\\n",
        "           load()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kfHXuuI-BKOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Json file Handling\n",
        "\n",
        "\n",
        "complex Data types\n",
        "\n",
        "1.struct - dict\n",
        "\n",
        "2.array -  list - To flattern complex datatype(array datatype ) we can you explode() function\n",
        "\n",
        "3.map\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ANG4ONGDTCRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrame from Json file\n",
        "\n",
        "data = spark.read.format('json').load('/content/emp.json')\n",
        "data.show()\n",
        "data.printSchema()\n",
        "data.count()\n"
      ],
      "metadata": {
        "id": "CmRO4ybgDPxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataFrame from multiLine Json file"
      ],
      "metadata": {
        "id": "1d7XuxL5X50r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if json file has  more no.of lines (nested data) .then we should use multiLine in options\n",
        "\n",
        "Mul_data = spark.read.format('json').option('multiline',True).option('inferSchema',True).option('nullValue','null').load('/content/nested_json.json')\n",
        "\n",
        "Mul_data.show(truncate = False)\n",
        "\n",
        "Mul_data.printSchema()\n",
        "Mul_data.count()"
      ],
      "metadata": {
        "id": "OwpKEkmDDbJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mul_data1 = Mul_data.withColumn('batters_exp',explode('batters.batter')) \\\n",
        "         .withColumn('batters_id',col('batters_exp.id')) \\\n",
        "         .withColumn('batters_type',col('batters_exp.type')) \\\n",
        "         .drop('batters','batters_exp') \\\n",
        "         .withColumn('topping_exp',explode('topping')) \\\n",
        "         .withColumn('topping_id',col('topping_exp.id')) \\\n",
        "         .withColumn('topping_type',col('topping_exp.type')) \\\n",
        "         .drop('topping','topping_exp') \\\n",
        "\n",
        "\n",
        "Mul_data1.show(10)\n",
        "\n",
        "Mul_data1.printSchema()\n",
        "\n",
        "Mul_data1.count()"
      ],
      "metadata": {
        "id": "sgba5VYV1jn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions in pyspark"
      ],
      "metadata": {
        "id": "Egb13y1nnydU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "fun = spark.sql('show functions')\n",
        "print(fun.count())\n",
        "print(fun.show())"
      ],
      "metadata": {
        "id": "R1C9_AAVn_JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(fun))\n",
        "\n",
        "# describe function details\n",
        "spark.sql('describe function aggregate').show(truncate=False)"
      ],
      "metadata": {
        "id": "XwurJt6fqA8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word count program step by step"
      ],
      "metadata": {
        "id": "6KOrxwloiaUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rd = spark.sparkContext.textFile(\"/content/word.txt\")\n",
        "\n",
        "print(type(rd))\n",
        "print(rd.collect())       # no.of lines\n",
        "print(rd.count())"
      ],
      "metadata": {
        "id": "SxIeaVvVjktl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rd1 = rd.map(lambda x : x.encode('utf-8'))\n",
        "\n",
        "print(rd1.collect())"
      ],
      "metadata": {
        "id": "ZGHpc8uit29t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map - gives Number of lists of strings\n",
        "\n",
        "rd2 = rd.map(lambda x:x.split(' '))\n",
        "print(type(rd2))\n",
        "print(rd2.collect())"
      ],
      "metadata": {
        "id": "VnO7twMLmKK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#flatmap - gives single list of strings\n",
        "\n",
        "rd3 = rd.flatMap(lambda x : x.split(' '))\n",
        "print(rd3.collect())"
      ],
      "metadata": {
        "id": "smr8WA0-mgeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assiging value to strings\n",
        "rd4 =  rd3.map(lambda x : (x,1))\n",
        "print(rd4.collect())"
      ],
      "metadata": {
        "id": "v5MLyj-KqTRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduceByKey -  groupping and aggregating same keys from rd4\n",
        "\n",
        "rd5 = rd4.reduceByKey(lambda x ,y : x+y)\n",
        "\n",
        "print(rd5.collect())"
      ],
      "metadata": {
        "id": "BHsbsr60r1I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word count in single line - Find No.of Occurences of single word"
      ],
      "metadata": {
        "id": "BAuEVPpOuRmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = spark.sparkContext.textFile('/content/word.txt').flatMap(lambda x : x.split(' ')).map(lambda x:(x,1)).reduceByKey(lambda x,y : x+y)\n",
        "\n",
        "RDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lQ_cEii5KDk",
        "outputId": "42e5e428-fab2-43a2-db25-c8ec54d98642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 1),\n",
              " ('forest', 1),\n",
              " ('raven', 3),\n",
              " ('also', 2),\n",
              " ('commonly', 1),\n",
              " ('known', 1),\n",
              " ('as', 3),\n",
              " ('the', 4),\n",
              " ('Tasmanian', 2),\n",
              " ('is', 1),\n",
              " ('a', 2),\n",
              " ('passerine', 1),\n",
              " ('bird', 1),\n",
              " ('in', 3),\n",
              " ('family', 1),\n",
              " ('Corvidae', 1),\n",
              " ('native', 1),\n",
              " ('to', 2),\n",
              " ('Tasmania', 1),\n",
              " ('and', 5),\n",
              " ('parts', 2),\n",
              " ('of', 3),\n",
              " ('southern', 1),\n",
              " ('Victoria', 1),\n",
              " ('such', 1),\n",
              " ('Wilsons', 1),\n",
              " ('Promontory', 1),\n",
              " ('Portland', 1),\n",
              " ('Populations', 1),\n",
              " ('are', 2),\n",
              " ('found', 1),\n",
              " ('New', 2),\n",
              " ('South', 2),\n",
              " ('Wales', 2),\n",
              " ('including', 1),\n",
              " ('Dorrigo', 1),\n",
              " ('Armidale', 1),\n",
              " ('it', 1),\n",
              " ('has', 1),\n",
              " ('allblack', 1),\n",
              " ('plumage', 1),\n",
              " ('beak', 1),\n",
              " ('legs', 1),\n",
              " ('As', 1),\n",
              " ('with', 2),\n",
              " ('other', 1),\n",
              " ('two', 1),\n",
              " ('species', 1),\n",
              " ('Australia,', 1),\n",
              " ('its', 1),\n",
              " ('black', 1),\n",
              " ('feathers', 1),\n",
              " ('have', 3),\n",
              " ('grey', 1),\n",
              " ('bases', 1),\n",
              " ('Adults', 1),\n",
              " ('white', 1),\n",
              " ('irises;', 1),\n",
              " ('younger', 1),\n",
              " ('birds', 1),\n",
              " ('dark', 1),\n",
              " ('brown', 1),\n",
              " ('then', 1),\n",
              " ('hazel', 1),\n",
              " ('irises', 1),\n",
              " ('an', 1),\n",
              " ('inner', 1),\n",
              " ('blue', 1),\n",
              " ('rim', 1),\n",
              " ('populations', 1),\n",
              " ('recognised', 1),\n",
              " ('separate', 1),\n",
              " ('subspecies', 2),\n",
              " ('C', 1),\n",
              " ('tasmanicus', 1),\n",
              " ('boreus', 1),\n",
              " ('but', 1),\n",
              " ('appear', 1),\n",
              " ('be', 1),\n",
              " ('nested', 1),\n",
              " ('within', 1),\n",
              " ('genetically', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FqvSfJiiF-Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fill missing data in textFile and convert into Dataframe"
      ],
      "metadata": {
        "id": "pERd85OnPkP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_miss = spark.read.format('csv').option('sep',' ').load('/content/fill missing.txt').fillna('no data')\n",
        "\n",
        "df_miss.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "AClRPWqjPuk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd5b688-c4e7-4c0c-bfd1-e7f3b02768f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|  _c0|    _c1|    _c2|    _c3|    _c4|    _c5|    _c6|    _c7|    _c8|    _c9|   _c10|   _c11|\n",
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#udf register"
      ],
      "metadata": {
        "id": "SogxNiM-NzVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# UDF\n",
        "\n",
        "def f1(x):\n",
        "  return ((x*x)-x)\n",
        "\n",
        "spark.udf.register('fun',f1,IntegerType())\n",
        "fun = udf(f1,IntegerType())\n",
        "\n",
        "print(f1(5))"
      ],
      "metadata": {
        "id": "ux58m9HqOdSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.select('ENAME',sqrt('SAL'),fun('SAL')).show()"
      ],
      "metadata": {
        "id": "k2sKsO_bPgeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle yy date format in pyspark for before 2000 data"
      ],
      "metadata": {
        "id": "AiXskdNP3leY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yy_df = spark.read.format('csv').option('header',True).option('sep','|').option('inferSchema',True).option('nullValue','null').load('/content/emp_pipe_yy.txt')\n",
        "\n",
        "yy_df.show()\n",
        "yy_df.printSchema()"
      ],
      "metadata": {
        "id": "0mvLKlJd4Amy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Spark defaulty chooses year after 2000\n",
        "\n",
        "# we need to set spark sql legcy timeparserpolicy to legacy -- Useful for less columns data only ,if more data means need ask source system set proper date format\n",
        "\n",
        "spark.conf.set('spark.sql.legacy.timeParserPolicy','LEGACY')\n",
        "\n",
        "yy_df.withColumn('DATE',to_date('UPDATED_DATE','dd-mm-yy')).show()"
      ],
      "metadata": {
        "id": "KZWmZ66P5Iqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle bad data\n",
        "\n"
      ],
      "metadata": {
        "id": "Af6xhc4-Px-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/channels.csv')\n",
        "\n",
        "bad.show()\n",
        "\n",
        "bad.schema"
      ],
      "metadata": {
        "id": "5ZEw-1NGPvoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark read Mode:\n",
        "\n",
        "1.PERMISSIVE - allows bad data - it's spark default mode\n",
        "\n",
        "2.FAILFAST - won't allows bad data -it raise expection - it won't process further\n",
        "\n",
        "3.DROPMALFORMED - drops bad records based on schema -it won't save bad records\n",
        "\n",
        "4.badrecordsPath - save good data in table and saves bad it another path"
      ],
      "metadata": {
        "id": "IOi9TvtAacB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# need to add _corrupt_record column string type in custom made schema\n",
        "schema  = StructType([StructField('CHANNEL_ID', IntegerType(), True),\n",
        "                      StructField('CHANNEL_DESC', StringType(), True),\n",
        "                      StructField('CHANNEL_CLASS', StringType(), True),\n",
        "                      StructField('CHANNEL_CLASS_ID', IntegerType(), True),\n",
        "                      StructField('CHANNEL_TOTAL', StringType(), True),\n",
        "                      StructField('CHANNEL_TOTAL_ID', IntegerType(), True),\n",
        "                      StructField(\"BadData\", StringType(), True)])"
      ],
      "metadata": {
        "id": "gj8coYWURVLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save bad Records Using mode - PERMISSIVE and _corrupt_record ,columnNameofCorrputRecord\n",
        "\n",
        "bad1 = spark.read.format('csv').schema(schema).option('Mode','PERMISSIVE').option('ColumnNameOfCorruptRecord','BadData').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad1.show()\n",
        "\n",
        "# filter good records\n",
        "goodData = bad1.filter('BadData is Null').drop('BAdData')\n",
        "goodData.show()\n",
        "\n",
        "# filter corrupt records\n",
        "bad3 = bad1.filter('BadData is Not Null')\n",
        "bad3.show()"
      ],
      "metadata": {
        "id": "2gGJjO8fidU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mode - FAILFAST\n",
        "\n",
        "bad = spark.read.format('csv').schema(schema).option('mode','FAILFAST').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad.show()\n"
      ],
      "metadata": {
        "id": "ArLjWoOOUdA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DROPMALFORMED\n",
        "\n",
        "bad = spark.read.format('csv').schema(schema).option('mode','DROPMALFORMED').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad.show()"
      ],
      "metadata": {
        "id": "Tek46tmobPMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference Between sort and order by\n",
        "\n",
        "\n",
        "\n",
        "1. Spark_sql : orderBy will do sorting an entire data ,sortby will do Partition wise sorting in sparksql .\n",
        "\n",
        "\n",
        "2. pyspark : orderBy and sort are same pyspark.sortwithinpartitions same as sortby ( it will do Partition wise sorting)\n",
        "\n"
      ],
      "metadata": {
        "id": "_SoyRZHsFe3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ta_df = spark.read.load('/content/spark-warehouse/oracle_db.db/emp_dept_table').repartition(4,'DEPTNO').withColumn('partition',spark_partition_id())\n",
        "\n",
        "ta_df.show()"
      ],
      "metadata": {
        "id": "256lRsObIJMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In Pyspark -orderBy and sort are same pyspark\n",
        "\n",
        "#orderBy\n",
        "\n",
        "ta_df.orderBy('SAL').show()\n"
      ],
      "metadata": {
        "id": "H1LZcRkmI83G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort\n",
        "ta_df.sort('SAL').show()"
      ],
      "metadata": {
        "id": "KxxocSEhMJwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sortWithinPartitions -sortwithinpartitions same as sortby ( it will do Partition wise sorting)\n",
        "\n",
        "ta_df.sortWithinPartitions('SAL').show()"
      ],
      "metadata": {
        "id": "z1Za9R5zK5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ta_df.createOrReplaceTempView('ta_df')"
      ],
      "metadata": {
        "id": "zUOvhVlRPBYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#order by - sort entire data\n",
        "\n",
        "spark.sql('select * from ta_df order by SAL').show()"
      ],
      "metadata": {
        "id": "zFLHWrMfMe2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort by - will do partition wise sorting\n",
        "spark.sql('select * from ta_df sort by SAL').show()"
      ],
      "metadata": {
        "id": "qD_3TvvfNRWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# coalesce and repartition in rdd\n",
        "\n",
        "coalesce : is a  Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.\n",
        "for increasing partitions we need provide another argument True ,then it will shuffle the data.\n",
        "\n",
        "repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions\n"
      ],
      "metadata": {
        "id": "bAzJc7yDRGeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "2Fw90GAERUs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(range(10),5)\n",
        "\n",
        "rdd1 = rdd.coalesce(2) # used to decrease no.of partitions ,No shuffle will happen\n",
        "\n",
        "rdd2 = rdd.coalesce(4,True)  # use True to increase no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd3 = rdd.repartition(2)    # use True to decrease no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd4 = rdd.repartition(6)   # use True to increase no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd.coalesce()\n",
        "\n",
        "rdd.repartition()\n",
        "\n",
        "print('original rdd', rdd.glom().collect())\n",
        "print('coalesce 2 ',rdd1.glom().collect())\n",
        "print('coalesce 4',rdd2.glom().collect())\n",
        "print('repartition 2',rdd3.glom().collect())\n",
        "print('repartition 6',rdd4.glom().collect())"
      ],
      "metadata": {
        "id": "_KX9V-ULWfAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fe192e-b278-4060-db6d-946254552f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original rdd [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
            "coalesce 2  [[0, 1, 2, 3], [4, 5, 6, 7, 8, 9]]\n",
            "coalesce 4 [[4, 5], [0, 1, 2, 3], [], [6, 7, 8, 9]]\n",
            "repartition 2 [[4, 5, 6, 7, 8, 9], [0, 1, 2, 3]]\n",
            "repartition 6 [[6, 7], [], [8, 9], [0, 1], [2, 3], [4, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# coalesce and repartition in dataframe\n",
        "\n",
        "coalesce : is a  Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.\n",
        "\n",
        "\n",
        "repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions,\n",
        "we can repartition based on column specific to increse the performence"
      ],
      "metadata": {
        "id": "1b2vS5BA6BwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df = spark.read.load('/content/spark-warehouse/oracle_db.db/emp_dept_table')\n",
        "print(cr_df.rdd.getNumPartitions())\n",
        "cr_df.show()"
      ],
      "metadata": {
        "id": "5hiHFFzr6HRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df1 = cr_df.repartition(4).withColumn('partition_id',spark_partition_id())\n",
        "cr_df1.show()"
      ],
      "metadata": {
        "id": "EYgmLeEI6bl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#repartition based on joining columns/filtering column to imporve performance\n",
        "cr_df2 = cr_df.repartition(4,'DEPTNO').withColumn('partition_id',spark_partition_id())\n",
        "cr_df2.show()"
      ],
      "metadata": {
        "id": "Ob07UMiAJRF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df3 = cr_df.coalesce(3).withColumn('partition_id',spark_partition_id())\n",
        "cr_df3.show()"
      ],
      "metadata": {
        "id": "ZpLHWRi8FwqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.coalesce()\n",
        "df.repartition()"
      ],
      "metadata": {
        "id": "j2LoRlHWGD8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Frame from REST API"
      ],
      "metadata": {
        "id": "CyDwbLPUMKAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REST API -Accessing the data over internet through Urls\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "api = requests.request('GET','https://api.github.com/users/hadley/orgs')\n",
        "\n",
        "data = api.json()\n",
        "\n",
        "file = open('/content/sample_data/apidata.json','a')\n",
        "\n",
        "for record in data:\n",
        "  file.write(\"%s\\n\" %record)\n",
        "\n",
        "api_df = spark.read.format('json').load('/content/sample_data/apidata.json')\n"
      ],
      "metadata": {
        "id": "WMI64DqaMVaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(api.json()))\n",
        "print(len(api.json()))"
      ],
      "metadata": {
        "id": "UE7H0go3YHPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea0bc86-e31d-451a-9c6e-697818e024d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_df.show(10)\n",
        "api_df.printSchema()\n",
        "api_df.count()"
      ],
      "metadata": {
        "id": "81TOTdwxVrT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e23a0d-4068-4fb3-f8ab-2e055751603b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "root\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}