
1.Creating Pyspark Context:

import findspark
findspark.init()
from pyspark import SparkContext

#Create SparkContext
sc = SparkContext.getOrCreate()
sc


#2.creating lists
list = [ i for i in range(1,10)]


print(list)
print(type(list))

#3.Creating RDD from lists
#parallelize method is used to create a (RDD) from an existing or local collection (like a Python list or tuple) in the driver program.Used for testing purpose

rdd = sc.parallelize(list)
print('rdd:',rdd.collect())
print('rdd_type',type(rdd))


#To know All methods in rdd
dir(rdd)

#Get Help
help(rdd)




#4.RDD Actions
#a.collect (convert RDD to in-memory list)
#b.take() (prints first elements)
#c.top(prints highest elements)
#d.takeSample (take some sample random values from list, if it's true it will repeat same value again ,False means unique)
#e.Aggregate functions - gives single output value
#min,max,sum(),mean(),stdev
#count( no.of elements)
#stats- complete info about count,mean,stdev,max,min
#f.reduce -gives single output value,reduce that aggregate data set(RDD) element using function
#g.CountByValue,countByKey - count of same values
#h.fold - aggregate the elements of each partition
#i.variance (all n values variance)
#j.sample variance - (n-1) values variance
#k.saveAsTextFile   -- text file format
#l.saveAsPickleFile -- binary file format

#Actions:
print('rdd:',rdd.collect())
print('first 5 elements:',rdd.take(5))
print('top five elements:',rdd.top(5))
print('mini element:',rdd.min())
print('max element:',rdd.max())
print('total no of elements:',rdd.count())
print('sum of elements:',rdd.sum())
print('mean of elements:',rdd.mean())
print('stdev of elements:',rdd.stdev())
print('random duplicate elements:',rdd.takeSample(True,5))
print('random unique elements:',rdd.takeSample(False,5))
print('list statatics',rdd.stats())
print('population Variance:',rdd.variance())
print('sample Variance',rdd.sampleVariance())
print('reduce list to one element by sum:',rdd.reduce( lambda x,y:x+y))
print('reduce list to one element by subtraction:',rdd.reduce(lambda x,y:x-y))
Print('save as txt file',rdd.saveAsTextFile('C:\\Users\\nooka\csv'))
print('save as binary file',rdd.saveAsPickleFile('C:\\Users\\nooka\binary'))

#creating list pair
list_pair = [(i,j) for i in range(1,5,2) for j in range(1,10,2)]
rdd_pair= sc.parallelize(list_pair)
rdd_pair.collect()
print('count of values',rdd_pair.countByValue())
print('count of keys',rdd_pair.countByKey())
print(list_pair)
print(type(list_pair))

#fold
from operator import *
print('aggregate the elements of each partition:',rdd.fold(1,add))



#5.RDD transformations



#5.1.Map():Applies the function to each element of the RDD.Return a new distributed dataset.Output is still a list of lists.

#example 1: 
data = ["hello world", "pyspark map example", "flatMap is cool"]
rdd_data = sc.parallelize(data)
rdd_data.map(lambda x : x.split(' ')).collect()

#example 2:  return new rdd with item name  and it's length
data2 = ["apple", "banana", "cherry"]
words = sc.parallelize(data2)
words.map(lambda x : (x,len(x))).collect()

#example 3:return iteam range
nums = sc.parallelize([1, 2, 3])
nums.map(lambda x: range(x)).collect()

#5.2.flatMap():Applies the function to each element of the RDD.returns a single list of all results.big list.
#example 1:words final list
data = ["hello world", "pyspark map example", "flatMap is cool"]
rdd_data = sc.parallelize(data)
rdd_data.flatMap(lambda x : x.split(' ')).collect()


#example 2:return item range final list
nums = sc.parallelize([1,2,3,4])
nums.flatMap(lambda x : range(x)).collect()

#5.3.filter:

nums = sc.parallelize([i for i in range(10)])

even_num = nums.filter( lambda x : x%2 == 0)
even_num.collect()

odd_num = nums.filter(lambda x: x%2 != 0)
odd_num.collect()


#example 3: 
num = [1,2,3,4,5,6,7,8,9]
rdd_num = sc.parallelize(num,4) #distribute list into 4 partitions
rdd_num.glom().collect()    #glom shows no.of partition


#5.4.mapPartitions - Similar to map but runs separetely on each partition

#partition wise total sum

def f(x) : yield sum(x)
rdd_num.mapPartitions(f).collect()


#5.4.mapPartitions - Similar to map but runs separetely on each partition

#partition wise total sum
def f(x) : yield sum(x)
rdd_num.mapPartitions(f).collect()


#5.5.mapPartitionWIthIndex - Similar to Mappartition ,but also provides an integer value is index of partition

def f(index,x):yield (index,sum(x))
rdd_num.mapPartitionsWithIndex(f).collect()


def f(index,x):yield (index,min(x))
rdd_num.mapPartitionsWithIndex(f).collect()


#5.6.sample - Sample a fraction fraction of the data,sample(withreplacement.fraction,seed)  ,if (with replacement) - true - will repeat same value ,else (withreplacement) -False - unique values

rdd_repeat = rdd_list.sample(True,1)
rdd_unique  = rdd_list.sample(False,1)
print(rdd_repeat.collect())


#5.7.union -Return a new dataset that contains the union of the elements in the source dataset and the argument.

rdd_union = rdd_unique.union(rdd_repeat)
rdd_union.collect()


#5.8.union -Return a new dataset that contains the union of the elements in the source dataset and the argument.

rdd_union.distinct().collect()


#5.9.intersection-Return a new RDD that contains the intersection of elements in the source dataset and the argument.

rdd_unique = sc.parallelize([1, 2, 3, 4])
rdd_repeat = sc.parallelize([3, 4, 5, 6])
rdd_unique.intersection(rdd_repeat).collect()


#5.10.GroupByKey

data3 = [("apple", 1), ("banana", 2), ("apple", 3), ("orange", 4), ("banana", 5)]
rdd_data3 = sc.parallelize(data3) 
group_rdd = rdd_data3.groupByKey()
result = group_rdd.mapValues(list).collect()
print(rdd_data3.collect())
print(result)
print(rdd_unique.collect())


#6.Spark Caching

#caching -caching is  used to save the data(RDD/Dataframe/Dataset) in a cluster-wide in memory,
#cache() method default saves data in MEMORY_ONLY.
#Used to store small amount od data. This is Very Useful for accessing repeated data .
#such as querying a small data set or when running an iterative algorithm


#7.Spark Persist
# same like cahe ( stores data in memeory) but we can store large amout of  data
# persist() method is used to store it to the user-defined storage levels like (memory only , disk only ,memory and disk only etc)

rdd.persist(pyspark.StorageLevel.MEMORY_ONLY)

#rdd.persist(pyspark.StorageLevel.DISK_ONLY)

#rdd.persist(pyspark.StorageLevel.MEMORY_AND_DISK)


# clears cache and persist data manually
rdd.unpersist()


#8.Broadcast variables

# allow programmer to keep a read only variable cached on each machine rather than shipping a copy of it with tasks
# spark useses efficient broadcast algoritham to reduce communication cost


broad = sc.broadcast([1,2,3,4,5,6])

print(broad.value)


# clear the boarcast variables by using unpersist or destroy

#broad.unpersist()
#OR 
broad.destroy


#9.Accumulators
from pyspark.accumulators import Accumulator

accum=sc.accumulator(10)

rdd.foreach(lambda x:accum.add(x))
print(accum.value)


#10.coalesce

rdd_cal = rdd.coalesce(3)
rdd_cal.getNumPartitions()
rdd_cal.glom().collect()


#11.repartition
rdd_repart = rdd.repartition(3)
rdd_repart.getNumPartitions()
rdd_repart.glom().collect()


#12.Transformations - Joins
J1 = sc.parallelize([('A',2),('B',3),('C',4),('D',5),('E',6),('F',7)])

J2 = sc.parallelize([('C',2),('D',3),('E',4),('F',5),('G',6),('H',7)])




#join/inner Join - takes two pair of rdd ,return only matched records from both RDD's

innerjoin = J1.join(J2)
innerjoin.collect()

# leftouterjoin  - returns matched records from both rdd and unmatched records from left rdd

leftjoin = J1.leftOuterJoin(J2)
leftjoin.collect()

#RightOuterJoin - returns matched records from both rdd and unmatched from right rdd

rightjoin = J1.rightOuterJoin(J2)
rightjoin.collect()

#fullOuterJoin - returned all records from both RDD'S

fulljoin = J1.fullOuterJoin(J2)
fulljoin.collect()

#cartesian join - cross product of both elements of RDD

cartisanjoin = J1.cartesian(J2)
cartisanjoin.collect()