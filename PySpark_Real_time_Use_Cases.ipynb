{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3in-2lvQJZb2",
        "HQTXxbqnJYFw",
        "Y-5-RjNwL94o",
        "3WPP8L_aSVR5",
        "J5r6xqsucfz7",
        "gXCdk97VaDqr",
        "2c5Q92FEYG80",
        "mQM_-Rq31OHy",
        "3hRGKHomx8eU",
        "LW8h33EY6NjH",
        "7uE0fJKN7_ok",
        "u22VawEEFCLg",
        "5GetXnFNT0RC",
        "vLLIpiKhTn-E",
        "a7oOqdJPT9Oj",
        "l8K25BYuQHdL",
        "ggQMF5lbVhKO",
        "MPp15hFA-b5g",
        "CyDwbLPUMKAw",
        "Egb13y1nnydU",
        "_SiY5y8qUm69",
        "NFRXq77I1Tez",
        "HPhKhuoiRwB6",
        "LWOMubDXT5eS",
        "6KOrxwloiaUn",
        "BAuEVPpOuRmG",
        "DJpxV8ZZGBG7",
        "pERd85OnPkP0",
        "SogxNiM-NzVp",
        "AiXskdNP3leY",
        "yRGG8fPJG4jq",
        "Af6xhc4-Px-3",
        "_SoyRZHsFe3D",
        "bAzJc7yDRGeP",
        "1b2vS5BA6BwK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pyspark"
      ],
      "metadata": {
        "id": "3in-2lvQJZb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPkp9Z8VGqFg",
        "outputId": "1df91980-b811-4e7a-9efd-33a43dbd96e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=27aef6c930ea8766f198b197cdf13c9f45e06ea338a5a116ae1e3be67c40b716\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating spark Session"
      ],
      "metadata": {
        "id": "HQTXxbqnJYFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Practice').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyg4V6Q1KSg_",
        "outputId": "3c833bb1-9b3e-4e9b-b033-47fb96b50e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7d705883d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://7e9bdb729725:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Practice</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating dataFrame from hdfs Location"
      ],
      "metadata": {
        "id": "Y-5-RjNwL94o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " reading data from hdfs remote server\n",
        "\n",
        "df = spark.read.format('csv').load('hdfs://172.16.38.131.8020/bigdata/cse/app_prod/cse.app_prod.csv')"
      ],
      "metadata": {
        "id": "eCozdXf8SrG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spark Defult parallelism"
      ],
      "metadata": {
        "id": "3WPP8L_aSVR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spark will read data from hdfs source as 10 partitions ,user can defined default parallelism in spark\n",
        "\n",
        "df = spark.read.format('csv').load('hdfs://172.16.38.131.8020/bigdata/cse/app_prod/cse.app_prod.csv',10)"
      ],
      "metadata": {
        "id": "ugVu-MqWSmVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Why Python slower then scala ?\n"
      ],
      "metadata": {
        "id": "J5r6xqsucfz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "python objects are converted into java (JVM) objects using  Py4j librery vice-versa .\n",
        "\n",
        "scala objects are  By defalut java (JVM) objects.\n",
        "\n",
        "Hence No convertion needed, so scala is faster than oython"
      ],
      "metadata": {
        "id": "NSTyFKdqcsbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark1 Vs Spark2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gXCdk97VaDqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spark1:**\n",
        "1.   permenent tables not created in spark 1 without hive integration but In\n",
        "2.   we can only create spark context\n",
        "\n",
        "**spark2:**\n",
        "\n",
        "1.   Spark2 we can create permenent table without hive integration \n",
        "2.   On Single Node Spark cluster from Spark 2 Data stored in Local file system  & MetaData will stored in embedded berby database\n",
        "3.   we can create both spark session and spark context\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7BK6uilzeEPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading DataFrame from csv"
      ],
      "metadata": {
        "id": "2c5Q92FEYG80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load(\"/content/emp.csv\")\n",
        "df.show(5)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsYMOVv9avRo",
        "outputId": "ae88f71f-ab35-4b1e-9cc7-5b68b463ca66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "|EMPNO| ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "| 7369| SMITH|   CLERK|7902|17-12-1980| 800| 300|    10|  01-01-2022|\n",
            "| 7499| ALLEN|SALESMAN|7698|20-02-1981|1600| 300|    20|  01-01-2022|\n",
            "| 7521|  WARD|SALESMAN|7698|22-02-1981|1250| 500|    30|  01-01-2022|\n",
            "| 7566| JONES| MANAGER|7839|04-02-1981|2975|null|    40|  05-01-2022|\n",
            "| 7654|MARTIN|SALESMAN|7698|21-09-1981|1250|1400|    50|  03-01-2022|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- EMPNO: integer (nullable = true)\n",
            " |-- ENAME: string (nullable = true)\n",
            " |-- JOB: string (nullable = true)\n",
            " |-- MGR: integer (nullable = true)\n",
            " |-- HIREDATE: string (nullable = true)\n",
            " |-- SAL: integer (nullable = true)\n",
            " |-- COMM: integer (nullable = true)\n",
            " |-- DEPTNO: integer (nullable = true)\n",
            " |-- UPDATED_DATE: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# convert from String (dd-mm-yyyy) date format to spark date format (yyyy-mm-dd)"
      ],
      "metadata": {
        "id": "mQM_-Rq31OHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "df1 = df.withColumn('HIREDATE',to_date('HIREDATE','dd-mm-yyyy')).withColumn('UPDATED_DATE',to_date('UPDATED_DATE','dd-mm-yyyy'))\n",
        "\n",
        "df1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxu3d5Aq33FU",
        "outputId": "fd92cc79-88a2-47bb-be2e-ed1b78881eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- EMPNO: integer (nullable = true)\n",
            " |-- ENAME: string (nullable = true)\n",
            " |-- JOB: string (nullable = true)\n",
            " |-- MGR: integer (nullable = true)\n",
            " |-- HIREDATE: date (nullable = true)\n",
            " |-- SAL: integer (nullable = true)\n",
            " |-- COMM: integer (nullable = true)\n",
            " |-- DEPTNO: integer (nullable = true)\n",
            " |-- UPDATED_DATE: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# filter or Where operations"
      ],
      "metadata": {
        "id": "3hRGKHomx8eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.where('ENAME=\"SMITH\"').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ec46kMUyCM4",
        "outputId": "06d48847-ee48-451b-83bc-cc6b3acdc8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+----+----------+---+----+------+------------+\n",
            "|EMPNO|ENAME|  JOB| MGR|  HIREDATE|SAL|COMM|DEPTNO|UPDATED_DATE|\n",
            "+-----+-----+-----+----+----------+---+----+------+------------+\n",
            "| 7369|SMITH|CLERK|7902|1980-01-17|800| 300|    10|  2022-01-01|\n",
            "| 7369|SMITH|CLERK|7902|1980-01-17|800|null|    40|  2022-01-04|\n",
            "+-----+-----+-----+----+----------+---+----+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1.filter('JOB = \"MANAGER\"').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWCIaO9gAUJX",
        "outputId": "597ef37b-5f09-4a36-88e9-774f554e3078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-------+----+----------+----+----+------+------------+\n",
            "|EMPNO|ENAME|    JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|\n",
            "+-----+-----+-------+----+----------+----+----+------+------------+\n",
            "| 7566|JONES|MANAGER|7839|1981-01-04|2975|null|    40|  2022-01-05|\n",
            "| 7782| RAVI|MANAGER|7839|1981-01-06|2450| 100|  null|  2022-01-02|\n",
            "| null| null|MANAGER|null|1981-01-01|null|null|  null|  2022-01-02|\n",
            "+-----+-----+-------+----+----------+----+----+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply Operations on Columns"
      ],
      "metadata": {
        "id": "LW8h33EY6NjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.select('SAL',col('SAL')/100).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-WcDxNW6ZPi",
        "outputId": "377d64f7-f0f7-4788-8206-36dd36dcee40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+\n",
            "| SAL|(SAL / 100)|\n",
            "+----+-----------+\n",
            "| 800|        8.0|\n",
            "|1600|       16.0|\n",
            "|1250|       12.5|\n",
            "|2975|      29.75|\n",
            "|1250|       12.5|\n",
            "|2850|       28.5|\n",
            "|2450|       24.5|\n",
            "|null|       null|\n",
            "|null|       null|\n",
            "|1500|       15.0|\n",
            "|1100|       11.0|\n",
            "| 950|        9.5|\n",
            "|3000|       30.0|\n",
            "| 800|        8.0|\n",
            "|1600|       16.0|\n",
            "|1250|       12.5|\n",
            "+----+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding new column"
      ],
      "metadata": {
        "id": "7uE0fJKN7_ok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df1.withColumn('NEW_SAL',col('SAL')*100)\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ6H-MO96box",
        "outputId": "56f8f473-52c5-413f-f3dc-0ec7af83ff06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------+----+----------+----+----+------+------------+-------+\n",
            "|EMPNO| ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+-------+\n",
            "| 7369| SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|\n",
            "| 7499| ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|\n",
            "| 7521|  WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|\n",
            "| 7566| JONES| MANAGER|7839|1981-01-04|2975|null|    40|  2022-01-05| 297500|\n",
            "| 7654|MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|\n",
            "| 7698|   SGR|    null|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|\n",
            "| 7782|  RAVI| MANAGER|7839|1981-01-06|2450| 100|  null|  2022-01-02| 245000|\n",
            "| 7788| SCOTT| ANALYST|7566|1987-01-19|null|null|    80|  2022-01-02|   null|\n",
            "| null|  null| MANAGER|null|1981-01-01|null|null|  null|  2022-01-02|   null|\n",
            "| 7844|TURNER|SALESMAN|7698|1981-01-09|1500|null|   100|  2022-01-02| 150000|\n",
            "| 7876| ADAMS|   CLERK|7788|1987-01-23|1100|null|    10|  2022-01-03| 110000|\n",
            "| 7900| JAMES|   CLERK|7698|1981-01-12| 950|null|    20|  2022-01-03|  95000|\n",
            "| 7902|  FORD| ANALYST|7566|1981-01-12|3000|null|    30|  2022-01-03| 300000|\n",
            "| 7369| SMITH|   CLERK|7902|1980-01-17| 800|null|    40|  2022-01-04|  80000|\n",
            "| 7499| ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|\n",
            "| 7521|  WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding new columns with different values"
      ],
      "metadata": {
        "id": "u22VawEEFCLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3 =df2.withColumn('CADER',when(col('JOB')=='CLERK','Lower').when(col('JOB')=='SALESMAN','lowest').when(col('JOB')=='MANAGER','Management').when(col('JOB')=='ANALYST','Middle'))\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zW3gEudFNEt",
        "outputId": "a55f58a7-0e38-4e28-8ab2-b8a7e0b22b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "|EMPNO| ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "| 7369| SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|\n",
            "| 7499| ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|\n",
            "| 7521|  WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|\n",
            "| 7566| JONES| MANAGER|7839|1981-01-04|2975|null|    40|  2022-01-05| 297500|Management|\n",
            "| 7654|MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|\n",
            "| 7698|   SGR|    null|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|      null|\n",
            "| 7782|  RAVI| MANAGER|7839|1981-01-06|2450| 100|  null|  2022-01-02| 245000|Management|\n",
            "| 7788| SCOTT| ANALYST|7566|1987-01-19|null|null|    80|  2022-01-02|   null|    Middle|\n",
            "| null|  null| MANAGER|null|1981-01-01|null|null|  null|  2022-01-02|   null|Management|\n",
            "| 7844|TURNER|SALESMAN|7698|1981-01-09|1500|null|   100|  2022-01-02| 150000|    lowest|\n",
            "| 7876| ADAMS|   CLERK|7788|1987-01-23|1100|null|    10|  2022-01-03| 110000|     Lower|\n",
            "| 7900| JAMES|   CLERK|7698|1981-01-12| 950|null|    20|  2022-01-03|  95000|     Lower|\n",
            "| 7902|  FORD| ANALYST|7566|1981-01-12|3000|null|    30|  2022-01-03| 300000|    Middle|\n",
            "| 7369| SMITH|   CLERK|7902|1980-01-17| 800|null|    40|  2022-01-04|  80000|     Lower|\n",
            "| 7499| ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|\n",
            "| 7521|  WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# filling null values "
      ],
      "metadata": {
        "id": "5GetXnFNT0RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df4 = df3.fillna(1).fillna('Missing',['ENAME','JOB'])\n",
        "df4.show()\n",
        "df4.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pQqQ3DZTzZy",
        "outputId": "24bad45b-1c6f-4961-e72d-0ce721097df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "|EMPNO|  ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|\n",
            "| 7566|  JONES| MANAGER|7839|1981-01-04|2975|   1|    40|  2022-01-05| 297500|Management|\n",
            "| 7654| MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|\n",
            "| 7698|    SGR| Missing|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|      null|\n",
            "| 7782|   RAVI| MANAGER|7839|1981-01-06|2450| 100|     1|  2022-01-02| 245000|Management|\n",
            "| 7788|  SCOTT| ANALYST|7566|1987-01-19|   1|   1|    80|  2022-01-02|      1|    Middle|\n",
            "|    1|Missing| MANAGER|   1|1981-01-01|   1|   1|     1|  2022-01-02|      1|Management|\n",
            "| 7844| TURNER|SALESMAN|7698|1981-01-09|1500|   1|   100|  2022-01-02| 150000|    lowest|\n",
            "| 7876|  ADAMS|   CLERK|7788|1987-01-23|1100|   1|    10|  2022-01-03| 110000|     Lower|\n",
            "| 7900|  JAMES|   CLERK|7698|1981-01-12| 950|   1|    20|  2022-01-03|  95000|     Lower|\n",
            "| 7902|   FORD| ANALYST|7566|1981-01-12|3000|   1|    30|  2022-01-03| 300000|    Middle|\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800|   1|    40|  2022-01-04|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dropping null values"
      ],
      "metadata": {
        "id": "vLLIpiKhTn-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df5 = df4.dropna()\n",
        "df5.show()\n",
        "df5.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cERUPPwqSnok",
        "outputId": "ffe7668e-cb23-406e-a140-90d763344229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "|EMPNO|  ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|\n",
            "| 7566|  JONES| MANAGER|7839|1981-01-04|2975|   1|    40|  2022-01-05| 297500|Management|\n",
            "| 7654| MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|\n",
            "| 7782|   RAVI| MANAGER|7839|1981-01-06|2450| 100|     1|  2022-01-02| 245000|Management|\n",
            "| 7788|  SCOTT| ANALYST|7566|1987-01-19|   1|   1|    80|  2022-01-02|      1|    Middle|\n",
            "|    1|Missing| MANAGER|   1|1981-01-01|   1|   1|     1|  2022-01-02|      1|Management|\n",
            "| 7844| TURNER|SALESMAN|7698|1981-01-09|1500|   1|   100|  2022-01-02| 150000|    lowest|\n",
            "| 7876|  ADAMS|   CLERK|7788|1987-01-23|1100|   1|    10|  2022-01-03| 110000|     Lower|\n",
            "| 7900|  JAMES|   CLERK|7698|1981-01-12| 950|   1|    20|  2022-01-03|  95000|     Lower|\n",
            "| 7902|   FORD| ANALYST|7566|1981-01-12|3000|   1|    30|  2022-01-03| 300000|    Middle|\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800|   1|    40|  2022-01-04|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Database operations"
      ],
      "metadata": {
        "id": "a7oOqdJPT9Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('create database oracle_db')\n",
        "spark.sql('show databases').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnCOjHzcUn06",
        "outputId": "ca9325b5-2beb-4812-c5d9-713e2cbc0ee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "|oracle_db|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Temp tables from dataframe"
      ],
      "metadata": {
        "id": "l8K25BYuQHdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#these are temporory tables ,dropped after spark session \n",
        "\n",
        "# temp tables doesn't belongs to any database\n",
        "\n",
        "df4.registerTempTable('T1')"
      ],
      "metadata": {
        "id": "3He2MZ11QQyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c4c45d-c161-4674-b6c0-f6329c48cc90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
            "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select * from T1').show()"
      ],
      "metadata": {
        "id": "iG2W_uZ_T8Su",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9dd9e8-677e-4f38-98d6-e1b263b471ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "|EMPNO|  ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|\n",
            "| 7566|  JONES| MANAGER|7839|1981-01-04|2975|   1|    40|  2022-01-05| 297500|Management|\n",
            "| 7654| MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|\n",
            "| 7698|    SGR| Missing|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|      null|\n",
            "| 7782|   RAVI| MANAGER|7839|1981-01-06|2450| 100|     1|  2022-01-02| 245000|Management|\n",
            "| 7788|  SCOTT| ANALYST|7566|1987-01-19|   1|   1|    80|  2022-01-02|      1|    Middle|\n",
            "|    1|Missing| MANAGER|   1|1981-01-01|   1|   1|     1|  2022-01-02|      1|Management|\n",
            "| 7844| TURNER|SALESMAN|7698|1981-01-09|1500|   1|   100|  2022-01-02| 150000|    lowest|\n",
            "| 7876|  ADAMS|   CLERK|7788|1987-01-23|1100|   1|    10|  2022-01-03| 110000|     Lower|\n",
            "| 7900|  JAMES|   CLERK|7698|1981-01-12| 950|   1|    20|  2022-01-03|  95000|     Lower|\n",
            "| 7902|   FORD| ANALYST|7566|1981-01-12|3000|   1|    30|  2022-01-03| 300000|    Middle|\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800|   1|    40|  2022-01-04|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('show tables').show()"
      ],
      "metadata": {
        "id": "R7SEPrXvUMIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462bf8e5-887b-46a7-a569-e76bb2b21e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|         |       t1|       true|\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Permenent tables from dataframe"
      ],
      "metadata": {
        "id": "ggQMF5lbVhKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#these tables will save in databases,we access later\n",
        "\n",
        "# Only perment tables belongs to Particular database\n",
        "\n",
        "spark.sql('use oracle_db')"
      ],
      "metadata": {
        "id": "POHjQzeKVqza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe282505-c4cf-4f25-a1d4-8a2b37da2291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"show tables\").show()"
      ],
      "metadata": {
        "id": "LrE7Ws-2XfqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f32bb8d-fc1f-4147-d457-ab4f37d25594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----------+\n",
            "|namespace|tableName|isTemporary|\n",
            "+---------+---------+-----------+\n",
            "|         |       t1|       true|\n",
            "+---------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spark.sql('drop table t1')\n",
        "#spark.sql('drop table Emp_data')"
      ],
      "metadata": {
        "id": "K0UhN35vY4Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4.write.saveAsTable('Emp_data')\n"
      ],
      "metadata": {
        "id": "2HzvvqglY-5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select * from Emp_data').show()"
      ],
      "metadata": {
        "id": "Pr-89eRmVwRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aecf1974-6464-4a04-be3e-196e76133278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "|EMPNO|  ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|\n",
            "| 7566|  JONES| MANAGER|7839|1981-01-04|2975|   1|    40|  2022-01-05| 297500|Management|\n",
            "| 7654| MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|\n",
            "| 7698|    SGR| Missing|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|      null|\n",
            "| 7782|   RAVI| MANAGER|7839|1981-01-06|2450| 100|     1|  2022-01-02| 245000|Management|\n",
            "| 7788|  SCOTT| ANALYST|7566|1987-01-19|   1|   1|    80|  2022-01-02|      1|    Middle|\n",
            "|    1|Missing| MANAGER|   1|1981-01-01|   1|   1|     1|  2022-01-02|      1|Management|\n",
            "| 7844| TURNER|SALESMAN|7698|1981-01-09|1500|   1|   100|  2022-01-02| 150000|    lowest|\n",
            "| 7876|  ADAMS|   CLERK|7788|1987-01-23|1100|   1|    10|  2022-01-03| 110000|     Lower|\n",
            "| 7900|  JAMES|   CLERK|7698|1981-01-12| 950|   1|    20|  2022-01-03|  95000|     Lower|\n",
            "| 7902|   FORD| ANALYST|7566|1981-01-12|3000|   1|    30|  2022-01-03| 300000|    Middle|\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800|   1|    40|  2022-01-04|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Frame from mysql table"
      ],
      "metadata": {
        "id": "MPp15hFA-b5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "df_mysql = spark.read.format('jdbc').\\\n",
        "           option('url','jdbc:mysql://localhost:3306').\\\n",
        "           option('driver','com.mysql.jdbc.Driver').\\\n",
        "           option('user','root').\\\n",
        "           option('password','sandeep').\\\n",
        "           option('query','select * from sandeep.emp_table').\\\n",
        "           load()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kfHXuuI-BKOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a9b06e-0abd-46c6-a2bf-086c0b0b3919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\ndf_mysql = spark.read.format('jdbc').           option('url','jdbc:mysql://localhost:3306').           option('driver','com.mysql.jdbc.Driver').           option('user','root').           option('password','sandeep').           option('query','select * from sandeep.emp_table').           load()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Json file Handling\n",
        "\n",
        "\n",
        "complex Data types\n",
        "\n",
        "1.struct - dict\n",
        "\n",
        "2.array -  list - To flattern complex datatype(array datatype ) we can you explode() function\n",
        "\n",
        "3.map\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ANG4ONGDTCRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrame from Json file\n",
        "\n",
        "data = spark.read.format('json').load('/content/emp.json')\n",
        "data.show()\n",
        "data.printSchema()\n",
        "data.count()"
      ],
      "metadata": {
        "id": "CmRO4ybgDPxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31c4c4c-f0ff-4a55-f819-b7c0e1eed7c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+-----+------+----------+---------+----+----+------------+\n",
            "|COMM|DEPTNO|EMPNO| ENAME|  HIREDATE|      JOB| MGR| SAL|UPDATED_DATE|\n",
            "+----+------+-----+------+----------+---------+----+----+------------+\n",
            "|  78|    80| 1234|SEKHAR|      null|   doctor|7777| 667|  2022-01-03|\n",
            "|null|    20| 7369| SMITH|17-12-1980|    CLERK|7902| 800|  2022-01-01|\n",
            "| 300|    30| 7499| ALLEN|20-02-1981| SALESMAN|7698|1600|  2022-01-01|\n",
            "| 500|    30| 7521|  WARD|22-02-1981| SALESMAN|7698|1250|  2022-01-01|\n",
            "|null|    20| 7566| JONES|04-02-1981|  MANAGER|7839|2975|  2022-01-05|\n",
            "|1400|    30| 7654|MARTIN|21-09-1981| SALESMAN|7698|1250|  2022-01-03|\n",
            "|null|    30| 7698|   SGR|05-01-1981|  MANAGER|7839|2850|  2022-01-04|\n",
            "|null|    10| 7782|  RAVI|06-09-1981|  MANAGER|7839|2450|  2022-01-02|\n",
            "|null|    20| 7788| SCOTT|19-04-1987|  ANALYST|7566|3000|  2022-01-02|\n",
            "|null|    10| 7839|  KING|01-11-1981|PRESIDENT|null|5000|  2022-01-02|\n",
            "|   0|    30| 7844|TURNER|09-08-1981| SALESMAN|7698|1500|  2022-01-02|\n",
            "|null|    20| 7876| ADAMS|23-05-1987|    CLERK|7788|1100|  2022-01-03|\n",
            "|null|    30| 7900| JAMES|12-03-1981|    CLERK|7698| 950|  2022-01-03|\n",
            "|null|    20| 7902|  FORD|12-03-1981|  ANALYST|7566|3000|  2022-01-03|\n",
            "|null|    10| 7934|MILLER|01-03-1982|    CLERK|7782|1300|  2022-01-03|\n",
            "+----+------+-----+------+----------+---------+----+----+------------+\n",
            "\n",
            "root\n",
            " |-- COMM: string (nullable = true)\n",
            " |-- DEPTNO: string (nullable = true)\n",
            " |-- EMPNO: string (nullable = true)\n",
            " |-- ENAME: string (nullable = true)\n",
            " |-- HIREDATE: string (nullable = true)\n",
            " |-- JOB: string (nullable = true)\n",
            " |-- MGR: string (nullable = true)\n",
            " |-- SAL: string (nullable = true)\n",
            " |-- UPDATED_DATE: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataFrame from multiLine Json file"
      ],
      "metadata": {
        "id": "1d7XuxL5X50r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if json file has  more no.of lines (nested data) .then we should use multiLine in options\n",
        "\n",
        "data2 = spark.read.format('json').option('multiLine',True).option('inferSchema',True).option('nullValue','null').load('/content/nested_json.json')\n",
        "data2.printSchema()\n",
        "data2.show(truncate = False)"
      ],
      "metadata": {
        "id": "OwpKEkmDDbJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8ed927-9dc6-420b-b673-3cb7a280a2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- batters: struct (nullable = true)\n",
            " |    |-- batter: array (nullable = true)\n",
            " |    |    |-- element: struct (containsNull = true)\n",
            " |    |    |    |-- id: string (nullable = true)\n",
            " |    |    |    |-- type: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- ppu: double (nullable = true)\n",
            " |-- topping: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- id: string (nullable = true)\n",
            " |    |    |-- type: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            "\n",
            "+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|batters                                                                        |id  |name|ppu |topping                                                                                                                                  |type |\n",
            "+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "|{[{1001, Regular}, {1002, Chocolate}, {1003, Blueberry}, {1004, Devil's Food}]}|0001|Cake|0.55|[{5001, None}, {5002, Glazed}, {5005, Sugar}, {5007, Powdered Sugar}, {5006, Chocolate with Sprinkles}, {5003, Chocolate}, {5004, Maple}]|donut|\n",
            "+-------------------------------------------------------------------------------+----+----+----+-----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = data2.withColumn('topping_explode',explode('topping'))\\\n",
        "     .withColumn('topping_id',col('topping_explode.id'))\\\n",
        "     .withColumn('topping_type',col('topping_explode.type'))\\\n",
        "     .drop('topping_explode','topping')\\\n",
        "     .withColumn('batters_explode',explode('batters.batter'))\\\n",
        "     .withColumn('batter_id',col('batters_explode.id'))\\\n",
        "     .withColumn('batter_type',col(\"batters_explode.type\"))\\\n",
        "     .drop('batters','batters_explode')\n",
        "\n",
        "data3.show(truncate=False)\n",
        "data3.count()"
      ],
      "metadata": {
        "id": "Uh55xURdG9c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e47398-e17d-4f9f-bb57-db8b2d6c76f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+-----+----------+------------------------+---------+------------+\n",
            "|id  |name|ppu |type |topping_id|topping_type            |batter_id|batter_type |\n",
            "+----+----+----+-----+----------+------------------------+---------+------------+\n",
            "|0001|Cake|0.55|donut|5001      |None                    |1001     |Regular     |\n",
            "|0001|Cake|0.55|donut|5001      |None                    |1002     |Chocolate   |\n",
            "|0001|Cake|0.55|donut|5001      |None                    |1003     |Blueberry   |\n",
            "|0001|Cake|0.55|donut|5001      |None                    |1004     |Devil's Food|\n",
            "|0001|Cake|0.55|donut|5002      |Glazed                  |1001     |Regular     |\n",
            "|0001|Cake|0.55|donut|5002      |Glazed                  |1002     |Chocolate   |\n",
            "|0001|Cake|0.55|donut|5002      |Glazed                  |1003     |Blueberry   |\n",
            "|0001|Cake|0.55|donut|5002      |Glazed                  |1004     |Devil's Food|\n",
            "|0001|Cake|0.55|donut|5005      |Sugar                   |1001     |Regular     |\n",
            "|0001|Cake|0.55|donut|5005      |Sugar                   |1002     |Chocolate   |\n",
            "|0001|Cake|0.55|donut|5005      |Sugar                   |1003     |Blueberry   |\n",
            "|0001|Cake|0.55|donut|5005      |Sugar                   |1004     |Devil's Food|\n",
            "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1001     |Regular     |\n",
            "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1002     |Chocolate   |\n",
            "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1003     |Blueberry   |\n",
            "|0001|Cake|0.55|donut|5007      |Powdered Sugar          |1004     |Devil's Food|\n",
            "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1001     |Regular     |\n",
            "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1002     |Chocolate   |\n",
            "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1003     |Blueberry   |\n",
            "|0001|Cake|0.55|donut|5006      |Chocolate with Sprinkles|1004     |Devil's Food|\n",
            "+----+----+----+-----+----------+------------------------+---------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Frame from REST API "
      ],
      "metadata": {
        "id": "CyDwbLPUMKAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REST API -Accessing the data over internet through Urls\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "api = requests.request('GET','https://api.github.com/users/hadley/orgs')\n",
        "\n",
        "data = api.json()\n",
        "\n",
        "file = open('/content/sample_data/apidata.json','a')\n",
        "\n",
        "for record in data:\n",
        "  file.write(\"%s\\n\" %record)\n",
        " \n",
        "api_df = spark.read.format('json').load('/content/sample_data/apidata.json')\n"
      ],
      "metadata": {
        "id": "WMI64DqaMVaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(api.json()))\n",
        "print(len(api.json()))"
      ],
      "metadata": {
        "id": "UE7H0go3YHPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dab8dad-8ca8-4a1c-daa8-309bad11b9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_df.show(10)\n",
        "api_df.printSchema()\n",
        "api_df.count()"
      ],
      "metadata": {
        "id": "81TOTdwxVrT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f26769-c05f-47b3-b37e-ee0ca992ac4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "root\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions in pyspark"
      ],
      "metadata": {
        "id": "Egb13y1nnydU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "fun = spark.sql('show functions')\n",
        "print(fun.count())\n",
        "print(fun.show(388))"
      ],
      "metadata": {
        "id": "R1C9_AAVn_JU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef61dcee-60ad-4342-ecc4-c3fd5458a544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "388\n",
            "+--------------------+\n",
            "|            function|\n",
            "+--------------------+\n",
            "|                   !|\n",
            "|                  !=|\n",
            "|                   %|\n",
            "|                   &|\n",
            "|                   *|\n",
            "|                   +|\n",
            "|                   -|\n",
            "|                   /|\n",
            "|                   <|\n",
            "|                  <=|\n",
            "|                 <=>|\n",
            "|                  <>|\n",
            "|                   =|\n",
            "|                  ==|\n",
            "|                   >|\n",
            "|                  >=|\n",
            "|                   ^|\n",
            "|                 abs|\n",
            "|                acos|\n",
            "|               acosh|\n",
            "|          add_months|\n",
            "|         aes_decrypt|\n",
            "|         aes_encrypt|\n",
            "|           aggregate|\n",
            "|                 and|\n",
            "|                 any|\n",
            "|approx_count_dist...|\n",
            "|   approx_percentile|\n",
            "|               array|\n",
            "|           array_agg|\n",
            "|      array_contains|\n",
            "|      array_distinct|\n",
            "|        array_except|\n",
            "|     array_intersect|\n",
            "|          array_join|\n",
            "|           array_max|\n",
            "|           array_min|\n",
            "|      array_position|\n",
            "|        array_remove|\n",
            "|        array_repeat|\n",
            "|          array_size|\n",
            "|          array_sort|\n",
            "|         array_union|\n",
            "|      arrays_overlap|\n",
            "|          arrays_zip|\n",
            "|               ascii|\n",
            "|                asin|\n",
            "|               asinh|\n",
            "|         assert_true|\n",
            "|                atan|\n",
            "|               atan2|\n",
            "|               atanh|\n",
            "|                 avg|\n",
            "|              base64|\n",
            "|             between|\n",
            "|              bigint|\n",
            "|                 bin|\n",
            "|              binary|\n",
            "|             bit_and|\n",
            "|           bit_count|\n",
            "|             bit_get|\n",
            "|          bit_length|\n",
            "|              bit_or|\n",
            "|             bit_xor|\n",
            "|            bool_and|\n",
            "|             bool_or|\n",
            "|             boolean|\n",
            "|              bround|\n",
            "|               btrim|\n",
            "|         cardinality|\n",
            "|                case|\n",
            "|                cast|\n",
            "|                cbrt|\n",
            "|                ceil|\n",
            "|             ceiling|\n",
            "|                char|\n",
            "|         char_length|\n",
            "|    character_length|\n",
            "|                 chr|\n",
            "|            coalesce|\n",
            "|        collect_list|\n",
            "|         collect_set|\n",
            "|              concat|\n",
            "|           concat_ws|\n",
            "|            contains|\n",
            "|                conv|\n",
            "|                corr|\n",
            "|                 cos|\n",
            "|                cosh|\n",
            "|                 cot|\n",
            "|               count|\n",
            "|            count_if|\n",
            "|    count_min_sketch|\n",
            "|           covar_pop|\n",
            "|          covar_samp|\n",
            "|               crc32|\n",
            "|                 csc|\n",
            "|           cume_dist|\n",
            "|     current_catalog|\n",
            "|    current_database|\n",
            "|        current_date|\n",
            "|   current_timestamp|\n",
            "|    current_timezone|\n",
            "|        current_user|\n",
            "|                date|\n",
            "|            date_add|\n",
            "|         date_format|\n",
            "| date_from_unix_date|\n",
            "|           date_part|\n",
            "|            date_sub|\n",
            "|          date_trunc|\n",
            "|            datediff|\n",
            "|                 day|\n",
            "|          dayofmonth|\n",
            "|           dayofweek|\n",
            "|           dayofyear|\n",
            "|             decimal|\n",
            "|              decode|\n",
            "|             degrees|\n",
            "|          dense_rank|\n",
            "|                 div|\n",
            "|              double|\n",
            "|                   e|\n",
            "|          element_at|\n",
            "|                 elt|\n",
            "|              encode|\n",
            "|            endswith|\n",
            "|               every|\n",
            "|              exists|\n",
            "|                 exp|\n",
            "|             explode|\n",
            "|       explode_outer|\n",
            "|               expm1|\n",
            "|             extract|\n",
            "|           factorial|\n",
            "|              filter|\n",
            "|         find_in_set|\n",
            "|               first|\n",
            "|         first_value|\n",
            "|             flatten|\n",
            "|               float|\n",
            "|               floor|\n",
            "|              forall|\n",
            "|       format_number|\n",
            "|       format_string|\n",
            "|            from_csv|\n",
            "|           from_json|\n",
            "|       from_unixtime|\n",
            "|  from_utc_timestamp|\n",
            "|     get_json_object|\n",
            "|              getbit|\n",
            "|            greatest|\n",
            "|            grouping|\n",
            "|         grouping_id|\n",
            "|                hash|\n",
            "|                 hex|\n",
            "|   histogram_numeric|\n",
            "|                hour|\n",
            "|               hypot|\n",
            "|                  if|\n",
            "|              ifnull|\n",
            "|               ilike|\n",
            "|                  in|\n",
            "|             initcap|\n",
            "|              inline|\n",
            "|        inline_outer|\n",
            "|input_file_block_...|\n",
            "|input_file_block_...|\n",
            "|     input_file_name|\n",
            "|               instr|\n",
            "|                 int|\n",
            "|               isnan|\n",
            "|           isnotnull|\n",
            "|              isnull|\n",
            "|         java_method|\n",
            "|   json_array_length|\n",
            "|    json_object_keys|\n",
            "|          json_tuple|\n",
            "|            kurtosis|\n",
            "|                 lag|\n",
            "|                last|\n",
            "|            last_day|\n",
            "|          last_value|\n",
            "|               lcase|\n",
            "|                lead|\n",
            "|               least|\n",
            "|                left|\n",
            "|              length|\n",
            "|         levenshtein|\n",
            "|                like|\n",
            "|                  ln|\n",
            "|              locate|\n",
            "|                 log|\n",
            "|               log10|\n",
            "|               log1p|\n",
            "|                log2|\n",
            "|               lower|\n",
            "|                lpad|\n",
            "|               ltrim|\n",
            "|           make_date|\n",
            "|    make_dt_interval|\n",
            "|       make_interval|\n",
            "|      make_timestamp|\n",
            "|    make_ym_interval|\n",
            "|                 map|\n",
            "|          map_concat|\n",
            "|    map_contains_key|\n",
            "|         map_entries|\n",
            "|          map_filter|\n",
            "|     map_from_arrays|\n",
            "|    map_from_entries|\n",
            "|            map_keys|\n",
            "|          map_values|\n",
            "|        map_zip_with|\n",
            "|                 max|\n",
            "|              max_by|\n",
            "|                 md5|\n",
            "|                mean|\n",
            "|                 min|\n",
            "|              min_by|\n",
            "|              minute|\n",
            "|                 mod|\n",
            "|monotonically_inc...|\n",
            "|               month|\n",
            "|      months_between|\n",
            "|        named_struct|\n",
            "|               nanvl|\n",
            "|            negative|\n",
            "|            next_day|\n",
            "|                 not|\n",
            "|                 now|\n",
            "|           nth_value|\n",
            "|               ntile|\n",
            "|              nullif|\n",
            "|                 nvl|\n",
            "|                nvl2|\n",
            "|        octet_length|\n",
            "|                  or|\n",
            "|             overlay|\n",
            "|           parse_url|\n",
            "|        percent_rank|\n",
            "|          percentile|\n",
            "|   percentile_approx|\n",
            "|                  pi|\n",
            "|                pmod|\n",
            "|          posexplode|\n",
            "|    posexplode_outer|\n",
            "|            position|\n",
            "|            positive|\n",
            "|                 pow|\n",
            "|               power|\n",
            "|              printf|\n",
            "|             quarter|\n",
            "|             radians|\n",
            "|         raise_error|\n",
            "|                rand|\n",
            "|               randn|\n",
            "|              random|\n",
            "|               range|\n",
            "|                rank|\n",
            "|             reflect|\n",
            "|              regexp|\n",
            "|      regexp_extract|\n",
            "|  regexp_extract_all|\n",
            "|         regexp_like|\n",
            "|      regexp_replace|\n",
            "|           regr_avgx|\n",
            "|           regr_avgy|\n",
            "|          regr_count|\n",
            "|             regr_r2|\n",
            "|              repeat|\n",
            "|             replace|\n",
            "|             reverse|\n",
            "|               right|\n",
            "|                rint|\n",
            "|               rlike|\n",
            "|               round|\n",
            "|          row_number|\n",
            "|                rpad|\n",
            "|               rtrim|\n",
            "|       schema_of_csv|\n",
            "|      schema_of_json|\n",
            "|                 sec|\n",
            "|              second|\n",
            "|           sentences|\n",
            "|            sequence|\n",
            "|      session_window|\n",
            "|                 sha|\n",
            "|                sha1|\n",
            "|                sha2|\n",
            "|           shiftleft|\n",
            "|          shiftright|\n",
            "|  shiftrightunsigned|\n",
            "|             shuffle|\n",
            "|                sign|\n",
            "|              signum|\n",
            "|                 sin|\n",
            "|                sinh|\n",
            "|                size|\n",
            "|            skewness|\n",
            "|               slice|\n",
            "|            smallint|\n",
            "|                some|\n",
            "|          sort_array|\n",
            "|             soundex|\n",
            "|               space|\n",
            "|  spark_partition_id|\n",
            "|               split|\n",
            "|          split_part|\n",
            "|                sqrt|\n",
            "|               stack|\n",
            "|          startswith|\n",
            "|                 std|\n",
            "|              stddev|\n",
            "|          stddev_pop|\n",
            "|         stddev_samp|\n",
            "|          str_to_map|\n",
            "|              string|\n",
            "|              struct|\n",
            "|              substr|\n",
            "|           substring|\n",
            "|     substring_index|\n",
            "|                 sum|\n",
            "|                 tan|\n",
            "|                tanh|\n",
            "|           timestamp|\n",
            "|    timestamp_micros|\n",
            "|    timestamp_millis|\n",
            "|   timestamp_seconds|\n",
            "|             tinyint|\n",
            "|           to_binary|\n",
            "|              to_csv|\n",
            "|             to_date|\n",
            "|             to_json|\n",
            "|           to_number|\n",
            "|        to_timestamp|\n",
            "|   to_unix_timestamp|\n",
            "|    to_utc_timestamp|\n",
            "|           transform|\n",
            "|      transform_keys|\n",
            "|    transform_values|\n",
            "|           translate|\n",
            "|                trim|\n",
            "|               trunc|\n",
            "|             try_add|\n",
            "|             try_avg|\n",
            "|          try_divide|\n",
            "|      try_element_at|\n",
            "|        try_multiply|\n",
            "|        try_subtract|\n",
            "|             try_sum|\n",
            "|       try_to_binary|\n",
            "|       try_to_number|\n",
            "|              typeof|\n",
            "|               ucase|\n",
            "|            unbase64|\n",
            "|               unhex|\n",
            "|           unix_date|\n",
            "|         unix_micros|\n",
            "|         unix_millis|\n",
            "|        unix_seconds|\n",
            "|      unix_timestamp|\n",
            "|               upper|\n",
            "|                uuid|\n",
            "|             var_pop|\n",
            "|            var_samp|\n",
            "|            variance|\n",
            "|             version|\n",
            "|             weekday|\n",
            "|          weekofyear|\n",
            "|                when|\n",
            "|        width_bucket|\n",
            "|              window|\n",
            "|               xpath|\n",
            "|       xpath_boolean|\n",
            "|        xpath_double|\n",
            "|         xpath_float|\n",
            "|           xpath_int|\n",
            "|          xpath_long|\n",
            "|        xpath_number|\n",
            "|         xpath_short|\n",
            "|        xpath_string|\n",
            "|            xxhash64|\n",
            "|                year|\n",
            "|            zip_with|\n",
            "|                   ||\n",
            "|                  |||\n",
            "|                   ~|\n",
            "+--------------------+\n",
            "\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(fun))\n",
        "\n",
        "# describe function details\n",
        "spark.sql('describe function aggregate').show(truncate=False)"
      ],
      "metadata": {
        "id": "XwurJt6fqA8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "214b3e38-9fc5-4672-f71b-8a7fb7423951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.dataframe.DataFrame'>\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|function_desc                                                                                                                                                                                                                                                                |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|Function: aggregate                                                                                                                                                                                                                                                          |\n",
            "|Class: org.apache.spark.sql.catalyst.expressions.ArrayAggregate                                                                                                                                                                                                              |\n",
            "|Usage: \\n      aggregate(expr, start, merge, finish) - Applies a binary operator to an initial state and all\\n      elements in the array, and reduces this to a single state. The final state is converted\\n      into the final result by applying a finish function.\\n    |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HIVE vs RDBMS"
      ],
      "metadata": {
        "id": "_SiY5y8qUm69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hive \n",
        "\n",
        "- Hive uses HQL/SQL langague \n",
        "- Hive uses HDFS  as storage \n",
        "- Hive used for Analysis\n",
        "- Metadata stored in Meta Store --- a database in RDBMS\n",
        "\n",
        "RDBMS\n",
        "\n",
        "- Oracle db uses Sql langague\n",
        "- oracle uses NTFS/EXT4 etc\n",
        "- used for real time transcations\n",
        "- metadata stored in RDBMS "
      ],
      "metadata": {
        "id": "yBNIajJbU1vV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating dataframe from Hive table\n",
        "\n"
      ],
      "metadata": {
        "id": "NFRXq77I1Tez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#DSL - Domain Specific language \n",
        "\n",
        "\n",
        "df_hive = spark.read.table('oracle_db.emp_data')\n",
        "\n",
        "print(df_hive.count())\n",
        "\n",
        "df_hive.show()\n"
      ],
      "metadata": {
        "id": "0gK-3qRS-Lw5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cb0d931-5438-4500-cc0a-c53f8098f91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "|EMPNO|  ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|\n",
            "| 7566|  JONES| MANAGER|7839|1981-01-04|2975|   1|    40|  2022-01-05| 297500|Management|\n",
            "| 7654| MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|\n",
            "| 7698|    SGR| Missing|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|      null|\n",
            "| 7782|   RAVI| MANAGER|7839|1981-01-06|2450| 100|     1|  2022-01-02| 245000|Management|\n",
            "| 7788|  SCOTT| ANALYST|7566|1987-01-19|   1|   1|    80|  2022-01-02|      1|    Middle|\n",
            "|    1|Missing| MANAGER|   1|1981-01-01|   1|   1|     1|  2022-01-02|      1|Management|\n",
            "| 7844| TURNER|SALESMAN|7698|1981-01-09|1500|   1|   100|  2022-01-02| 150000|    lowest|\n",
            "| 7876|  ADAMS|   CLERK|7788|1987-01-23|1100|   1|    10|  2022-01-03| 110000|     Lower|\n",
            "| 7900|  JAMES|   CLERK|7698|1981-01-12| 950|   1|    20|  2022-01-03|  95000|     Lower|\n",
            "| 7902|   FORD| ANALYST|7566|1981-01-12|3000|   1|    30|  2022-01-03| 300000|    Middle|\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800|   1|    40|  2022-01-04|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# pure sql language\n",
        "\n",
        "df_hive1 = spark.sql(\"select * from oracle_db.emp_data\")\n",
        "\n",
        "print(df_hive1.count())\n",
        "df_hive1.show()\n"
      ],
      "metadata": {
        "id": "UoBpkWvHOOHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46a669fb-312d-4bf9-8d7d-4ddfe5d02a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "|EMPNO|  ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|\n",
            "| 7566|  JONES| MANAGER|7839|1981-01-04|2975|   1|    40|  2022-01-05| 297500|Management|\n",
            "| 7654| MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|\n",
            "| 7698|    SGR| Missing|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|      null|\n",
            "| 7782|   RAVI| MANAGER|7839|1981-01-06|2450| 100|     1|  2022-01-02| 245000|Management|\n",
            "| 7788|  SCOTT| ANALYST|7566|1987-01-19|   1|   1|    80|  2022-01-02|      1|    Middle|\n",
            "|    1|Missing| MANAGER|   1|1981-01-01|   1|   1|     1|  2022-01-02|      1|Management|\n",
            "| 7844| TURNER|SALESMAN|7698|1981-01-09|1500|   1|   100|  2022-01-02| 150000|    lowest|\n",
            "| 7876|  ADAMS|   CLERK|7788|1987-01-23|1100|   1|    10|  2022-01-03| 110000|     Lower|\n",
            "| 7900|  JAMES|   CLERK|7698|1981-01-12| 950|   1|    20|  2022-01-03|  95000|     Lower|\n",
            "| 7902|   FORD| ANALYST|7566|1981-01-12|3000|   1|    30|  2022-01-03| 300000|    Middle|\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800|   1|    40|  2022-01-04|  80000|     Lower|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Dataframe to Hive"
      ],
      "metadata": {
        "id": "HPhKhuoiRwB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_hive.withColumn('Date',current_timestamp())\n",
        "\n",
        "df_final.write.partitionBy('Date').saveAsTable('oracle_db.new_emp_snap_view')\n"
      ],
      "metadata": {
        "id": "OCRyCtxZR1kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select * from new_emp_snap_view').show()"
      ],
      "metadata": {
        "id": "-jcx-9CPTBn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3a6c7f-3217-4aab-c9b7-6998dc5ec833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+--------------------+\n",
            "|EMPNO|  ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|NEW_SAL|     CADER|                Date|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+--------------------+\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|  80000|     Lower|2023-03-08 14:56:...|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01| 160000|    lowest|2023-03-08 14:56:...|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01| 125000|    lowest|2023-03-08 14:56:...|\n",
            "| 7566|  JONES| MANAGER|7839|1981-01-04|2975|   1|    40|  2022-01-05| 297500|Management|2023-03-08 14:56:...|\n",
            "| 7654| MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    50|  2022-01-03| 125000|    lowest|2023-03-08 14:56:...|\n",
            "| 7698|    SGR| Missing|7839|1981-01-05|2850|1600|    60|  2022-01-04| 285000|      null|2023-03-08 14:56:...|\n",
            "| 7782|   RAVI| MANAGER|7839|1981-01-06|2450| 100|     1|  2022-01-02| 245000|Management|2023-03-08 14:56:...|\n",
            "| 7788|  SCOTT| ANALYST|7566|1987-01-19|   1|   1|    80|  2022-01-02|      1|    Middle|2023-03-08 14:56:...|\n",
            "|    1|Missing| MANAGER|   1|1981-01-01|   1|   1|     1|  2022-01-02|      1|Management|2023-03-08 14:56:...|\n",
            "| 7844| TURNER|SALESMAN|7698|1981-01-09|1500|   1|   100|  2022-01-02| 150000|    lowest|2023-03-08 14:56:...|\n",
            "| 7876|  ADAMS|   CLERK|7788|1987-01-23|1100|   1|    10|  2022-01-03| 110000|     Lower|2023-03-08 14:56:...|\n",
            "| 7900|  JAMES|   CLERK|7698|1981-01-12| 950|   1|    20|  2022-01-03|  95000|     Lower|2023-03-08 14:56:...|\n",
            "| 7902|   FORD| ANALYST|7566|1981-01-12|3000|   1|    30|  2022-01-03| 300000|    Middle|2023-03-08 14:56:...|\n",
            "| 7369|  SMITH|   CLERK|7902|1980-01-17| 800|   1|    40|  2022-01-04|  80000|     Lower|2023-03-08 14:56:...|\n",
            "| 7499|  ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    50|  2022-01-04| 160000|    lowest|2023-03-08 14:56:...|\n",
            "| 7521|   WARD|SALESMAN|7698|1981-01-22|1250| 500|    60|  2022-01-04| 125000|    lowest|2023-03-08 14:56:...|\n",
            "+-----+-------+--------+----+----------+----+----+------+------------+-------+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  SPARK Input & OutPut formats\n",
        "\n"
      ],
      "metadata": {
        "id": "LWOMubDXT5eS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bydefault spark will read files in snappy.parquet\n",
        "\n",
        "Bydefault spark will save out files in snappy.parquet\n",
        "\n",
        "User need to define input and output formats By using format Function"
      ],
      "metadata": {
        "id": "2QC12wqHVGu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word count program step by step"
      ],
      "metadata": {
        "id": "6KOrxwloiaUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rd = spark.sparkContext.textFile(\"/content/word.txt\")\n",
        "\n",
        "print(type(rd))\n",
        "print(rd.collect())       # no.of lines\n",
        "print(rd.count())"
      ],
      "metadata": {
        "id": "SxIeaVvVjktl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd4c083-a9a9-4757-a3f7-9a28289db453"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "['The forest raven also commonly known as the Tasmanian raven', 'is a passerine bird in the family Corvidae native to Tasmania and parts of southern Victoria', 'such as Wilsons Promontory and Portland Populations are also found in parts of New South Wales', 'including Dorrigo and Armidale it has allblack plumage', 'beak and legs As with the other two species of raven in Australia, its black feathers have grey bases', 'Adults have white irises; younger birds have dark brown and then hazel irises with an inner blue rim', 'New South Wales populations are recognised as a separate subspecies C tasmanicus boreus', 'but appear to be nested within the Tasmanian subspecies genetically']\n",
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rd1 = rd.map(lambda x : x.encode('utf-8'))\n",
        "\n",
        "print(rd1.collect())"
      ],
      "metadata": {
        "id": "ZGHpc8uit29t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0e62df-9158-4336-a1c5-272a530c52c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'The forest raven also commonly known as the Tasmanian raven', b'is a passerine bird in the family Corvidae native to Tasmania and parts of southern Victoria', b'such as Wilsons Promontory and Portland Populations are also found in parts of New South Wales', b'including Dorrigo and Armidale it has allblack plumage', b'beak and legs As with the other two species of raven in Australia, its black feathers have grey bases', b'Adults have white irises; younger birds have dark brown and then hazel irises with an inner blue rim', b'New South Wales populations are recognised as a separate subspecies C tasmanicus boreus', b'but appear to be nested within the Tasmanian subspecies genetically']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# map - gives Number of lists of strings\n",
        "\n",
        "rd2 = rd.map(lambda x:x.split(' '))\n",
        "print(type(rd2))\n",
        "print(rd2.collect())"
      ],
      "metadata": {
        "id": "VnO7twMLmKK_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "face9e22-2d78-476c-9978-e65296de9690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.PipelinedRDD'>\n",
            "[['The', 'forest', 'raven', 'also', 'commonly', 'known', 'as', 'the', 'Tasmanian', 'raven'], ['is', 'a', 'passerine', 'bird', 'in', 'the', 'family', 'Corvidae', 'native', 'to', 'Tasmania', 'and', 'parts', 'of', 'southern', 'Victoria'], ['such', 'as', 'Wilsons', 'Promontory', 'and', 'Portland', 'Populations', 'are', 'also', 'found', 'in', 'parts', 'of', 'New', 'South', 'Wales'], ['including', 'Dorrigo', 'and', 'Armidale', 'it', 'has', 'allblack', 'plumage'], ['beak', 'and', 'legs', 'As', 'with', 'the', 'other', 'two', 'species', 'of', 'raven', 'in', 'Australia,', 'its', 'black', 'feathers', 'have', 'grey', 'bases'], ['Adults', 'have', 'white', 'irises;', 'younger', 'birds', 'have', 'dark', 'brown', 'and', 'then', 'hazel', 'irises', 'with', 'an', 'inner', 'blue', 'rim'], ['New', 'South', 'Wales', 'populations', 'are', 'recognised', 'as', 'a', 'separate', 'subspecies', 'C', 'tasmanicus', 'boreus'], ['but', 'appear', 'to', 'be', 'nested', 'within', 'the', 'Tasmanian', 'subspecies', 'genetically']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#flatmap - gives single list of strings\n",
        "\n",
        "rd3 = rd.flatMap(lambda x : x.split(' '))\n",
        "print(rd3.collect())"
      ],
      "metadata": {
        "id": "smr8WA0-mgeg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "452b57e0-c045-4716-8cc7-487a5fdc4595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'forest', 'raven', 'also', 'commonly', 'known', 'as', 'the', 'Tasmanian', 'raven', 'is', 'a', 'passerine', 'bird', 'in', 'the', 'family', 'Corvidae', 'native', 'to', 'Tasmania', 'and', 'parts', 'of', 'southern', 'Victoria', 'such', 'as', 'Wilsons', 'Promontory', 'and', 'Portland', 'Populations', 'are', 'also', 'found', 'in', 'parts', 'of', 'New', 'South', 'Wales', 'including', 'Dorrigo', 'and', 'Armidale', 'it', 'has', 'allblack', 'plumage', 'beak', 'and', 'legs', 'As', 'with', 'the', 'other', 'two', 'species', 'of', 'raven', 'in', 'Australia,', 'its', 'black', 'feathers', 'have', 'grey', 'bases', 'Adults', 'have', 'white', 'irises;', 'younger', 'birds', 'have', 'dark', 'brown', 'and', 'then', 'hazel', 'irises', 'with', 'an', 'inner', 'blue', 'rim', 'New', 'South', 'Wales', 'populations', 'are', 'recognised', 'as', 'a', 'separate', 'subspecies', 'C', 'tasmanicus', 'boreus', 'but', 'appear', 'to', 'be', 'nested', 'within', 'the', 'Tasmanian', 'subspecies', 'genetically']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assiging value to strings\n",
        "rd4 =  rd3.map(lambda x : (x,1))\n",
        "print(rd4.collect())"
      ],
      "metadata": {
        "id": "v5MLyj-KqTRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ac3b1c-82a2-48e2-8c99-7b0c1d73c2ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 1), ('forest', 1), ('raven', 1), ('also', 1), ('commonly', 1), ('known', 1), ('as', 1), ('the', 1), ('Tasmanian', 1), ('raven', 1), ('is', 1), ('a', 1), ('passerine', 1), ('bird', 1), ('in', 1), ('the', 1), ('family', 1), ('Corvidae', 1), ('native', 1), ('to', 1), ('Tasmania', 1), ('and', 1), ('parts', 1), ('of', 1), ('southern', 1), ('Victoria', 1), ('such', 1), ('as', 1), ('Wilsons', 1), ('Promontory', 1), ('and', 1), ('Portland', 1), ('Populations', 1), ('are', 1), ('also', 1), ('found', 1), ('in', 1), ('parts', 1), ('of', 1), ('New', 1), ('South', 1), ('Wales', 1), ('including', 1), ('Dorrigo', 1), ('and', 1), ('Armidale', 1), ('it', 1), ('has', 1), ('allblack', 1), ('plumage', 1), ('beak', 1), ('and', 1), ('legs', 1), ('As', 1), ('with', 1), ('the', 1), ('other', 1), ('two', 1), ('species', 1), ('of', 1), ('raven', 1), ('in', 1), ('Australia,', 1), ('its', 1), ('black', 1), ('feathers', 1), ('have', 1), ('grey', 1), ('bases', 1), ('Adults', 1), ('have', 1), ('white', 1), ('irises;', 1), ('younger', 1), ('birds', 1), ('have', 1), ('dark', 1), ('brown', 1), ('and', 1), ('then', 1), ('hazel', 1), ('irises', 1), ('with', 1), ('an', 1), ('inner', 1), ('blue', 1), ('rim', 1), ('New', 1), ('South', 1), ('Wales', 1), ('populations', 1), ('are', 1), ('recognised', 1), ('as', 1), ('a', 1), ('separate', 1), ('subspecies', 1), ('C', 1), ('tasmanicus', 1), ('boreus', 1), ('but', 1), ('appear', 1), ('to', 1), ('be', 1), ('nested', 1), ('within', 1), ('the', 1), ('Tasmanian', 1), ('subspecies', 1), ('genetically', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reduceByKey -  groupping and aggregating same keys from rd4\n",
        "\n",
        "rd5 = rd4.reduceByKey(lambda x ,y : x+y)\n",
        "\n",
        "print(rd5.collect())"
      ],
      "metadata": {
        "id": "BHsbsr60r1I2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4a0d14-51d8-4f58-a907-7c2b0245130e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 1), ('forest', 1), ('raven', 3), ('also', 2), ('commonly', 1), ('known', 1), ('as', 3), ('the', 4), ('Tasmanian', 2), ('is', 1), ('a', 2), ('passerine', 1), ('bird', 1), ('in', 3), ('family', 1), ('Corvidae', 1), ('native', 1), ('to', 2), ('Tasmania', 1), ('and', 5), ('parts', 2), ('of', 3), ('southern', 1), ('Victoria', 1), ('such', 1), ('Wilsons', 1), ('Promontory', 1), ('Portland', 1), ('Populations', 1), ('are', 2), ('found', 1), ('New', 2), ('South', 2), ('Wales', 2), ('including', 1), ('Dorrigo', 1), ('Armidale', 1), ('it', 1), ('has', 1), ('allblack', 1), ('plumage', 1), ('beak', 1), ('legs', 1), ('As', 1), ('with', 2), ('other', 1), ('two', 1), ('species', 1), ('Australia,', 1), ('its', 1), ('black', 1), ('feathers', 1), ('have', 3), ('grey', 1), ('bases', 1), ('Adults', 1), ('white', 1), ('irises;', 1), ('younger', 1), ('birds', 1), ('dark', 1), ('brown', 1), ('then', 1), ('hazel', 1), ('irises', 1), ('an', 1), ('inner', 1), ('blue', 1), ('rim', 1), ('populations', 1), ('recognised', 1), ('separate', 1), ('subspecies', 2), ('C', 1), ('tasmanicus', 1), ('boreus', 1), ('but', 1), ('appear', 1), ('be', 1), ('nested', 1), ('within', 1), ('genetically', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uaTnILfisYrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word count in single line - Find No.of Occurences of single word"
      ],
      "metadata": {
        "id": "BAuEVPpOuRmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = spark.sparkContext.textFile('/content/word.txt').flatMap(lambda x : x.split(' ')).map(lambda x: (x,1)).reduceByKey(lambda x ,y : x+y)\n",
        "R.collect()"
      ],
      "metadata": {
        "id": "z51tqB1CuQ6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9996bc92-b651-4b26-8577-de4a02332264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 1),\n",
              " ('forest', 1),\n",
              " ('raven', 3),\n",
              " ('also', 2),\n",
              " ('commonly', 1),\n",
              " ('known', 1),\n",
              " ('as', 3),\n",
              " ('the', 4),\n",
              " ('Tasmanian', 2),\n",
              " ('is', 1),\n",
              " ('a', 2),\n",
              " ('passerine', 1),\n",
              " ('bird', 1),\n",
              " ('in', 3),\n",
              " ('family', 1),\n",
              " ('Corvidae', 1),\n",
              " ('native', 1),\n",
              " ('to', 2),\n",
              " ('Tasmania', 1),\n",
              " ('and', 5),\n",
              " ('parts', 2),\n",
              " ('of', 3),\n",
              " ('southern', 1),\n",
              " ('Victoria', 1),\n",
              " ('such', 1),\n",
              " ('Wilsons', 1),\n",
              " ('Promontory', 1),\n",
              " ('Portland', 1),\n",
              " ('Populations', 1),\n",
              " ('are', 2),\n",
              " ('found', 1),\n",
              " ('New', 2),\n",
              " ('South', 2),\n",
              " ('Wales', 2),\n",
              " ('including', 1),\n",
              " ('Dorrigo', 1),\n",
              " ('Armidale', 1),\n",
              " ('it', 1),\n",
              " ('has', 1),\n",
              " ('allblack', 1),\n",
              " ('plumage', 1),\n",
              " ('beak', 1),\n",
              " ('legs', 1),\n",
              " ('As', 1),\n",
              " ('with', 2),\n",
              " ('other', 1),\n",
              " ('two', 1),\n",
              " ('species', 1),\n",
              " ('Australia,', 1),\n",
              " ('its', 1),\n",
              " ('black', 1),\n",
              " ('feathers', 1),\n",
              " ('have', 3),\n",
              " ('grey', 1),\n",
              " ('bases', 1),\n",
              " ('Adults', 1),\n",
              " ('white', 1),\n",
              " ('irises;', 1),\n",
              " ('younger', 1),\n",
              " ('birds', 1),\n",
              " ('dark', 1),\n",
              " ('brown', 1),\n",
              " ('then', 1),\n",
              " ('hazel', 1),\n",
              " ('irises', 1),\n",
              " ('an', 1),\n",
              " ('inner', 1),\n",
              " ('blue', 1),\n",
              " ('rim', 1),\n",
              " ('populations', 1),\n",
              " ('recognised', 1),\n",
              " ('separate', 1),\n",
              " ('subspecies', 2),\n",
              " ('C', 1),\n",
              " ('tasmanicus', 1),\n",
              " ('boreus', 1),\n",
              " ('but', 1),\n",
              " ('appear', 1),\n",
              " ('be', 1),\n",
              " ('nested', 1),\n",
              " ('within', 1),\n",
              " ('genetically', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FqvSfJiiF-Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark default Ram and Cores"
      ],
      "metadata": {
        "id": "DJpxV8ZZGBG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default ram is  - 1 GB\n",
        "# default cores are - local/local[1] - 1 core will be added/allocated\n",
        "#                   - local[n] - n cores will be added/allocated\n",
        "#                   - local[*] - all available cores in cluster \n",
        "\n",
        "# from pyspark.sql import sparkSession\n",
        "# spark = sparkSession.builder.master(local[*]).appName('demo').getOrCreate()"
      ],
      "metadata": {
        "id": "klbA2cxmHWbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fill missing data in textFile and convert into Dataframe"
      ],
      "metadata": {
        "id": "pERd85OnPkP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_miss = spark.read.format('csv').load('/content/fill missing.txt',sep = ' ').fillna('no data')\n",
        "\n",
        "df_miss.show()\n"
      ],
      "metadata": {
        "id": "AClRPWqjPuk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3faf7bef-582f-4236-f002-df2dd24e8d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|  _c0|    _c1|    _c2|    _c3|    _c4|    _c5|    _c6|    _c7|    _c8|    _c9|   _c10|   _c11|\n",
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#udf register"
      ],
      "metadata": {
        "id": "SogxNiM-NzVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# UDF \n",
        "\n",
        "def f1(x):\n",
        "  return ((x*x)-x)\n",
        "\n",
        "spark.udf.register('fun',f1,IntegerType())\n",
        "fun = udf(f1,IntegerType())\n",
        "\n",
        "print(f1(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ux58m9HqOdSV",
        "outputId": "ed359837-f63e-49b0-8f92-eeae697ff1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df5.select('ENAME',sqrt('SAL'),fun('SAL')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2sKsO_bPgeE",
        "outputId": "3624e10d-b118-4b68-e883-2fd637229f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-------+\n",
            "|  ENAME|         SQRT(SAL)|f1(SAL)|\n",
            "+-------+------------------+-------+\n",
            "|  SMITH|28.284271247461902| 639200|\n",
            "|  ALLEN|              40.0|2558400|\n",
            "|   WARD| 35.35533905932738|1561250|\n",
            "|  JONES|54.543560573178574|8847650|\n",
            "| MARTIN| 35.35533905932738|1561250|\n",
            "|   RAVI| 49.49747468305833|6000050|\n",
            "|  SCOTT|               1.0|      0|\n",
            "|Missing|               1.0|      0|\n",
            "| TURNER| 38.72983346207417|2248500|\n",
            "|  ADAMS|   33.166247903554|1208900|\n",
            "|  JAMES|30.822070014844883| 901550|\n",
            "|   FORD|54.772255750516614|8997000|\n",
            "|  SMITH|28.284271247461902| 639200|\n",
            "|  ALLEN|              40.0|2558400|\n",
            "|   WARD| 35.35533905932738|1561250|\n",
            "+-------+------------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle yy date format in pyspark for before 2000 data"
      ],
      "metadata": {
        "id": "AiXskdNP3leY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yy_df = spark.read.format('csv').option('header',True).option('sep','|').option('inferSchema',True).option('nullValue','null').load('/content/emp_pipe_yy.txt')\n",
        "\n",
        "yy_df.show()\n",
        "yy_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mvLKlJd4Amy",
        "outputId": "5084126d-166b-4ea5-c004-e40741000fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------+----+----+----+------+------------+\n",
            "|EMPNO| ENAME|      JOB| MGR| SAL|COMM|DEPTNO|UPDATED_DATE|\n",
            "+-----+------+---------+----+----+----+------+------------+\n",
            "| 7369| SMITH|    CLERK|7902| 800|null|    20|    01-01-98|\n",
            "| 7499| ALLEN| SALESMAN|7698|1600| 300|    30|    02-01-97|\n",
            "| 7521|  WARD| SALESMAN|7698|1250| 500|    30|    03-01-96|\n",
            "| 7566| JONES|  MANAGER|7839|2975|null|    20|    04-01-95|\n",
            "| 7654|MARTIN| SALESMAN|7698|1250|1400|    30|    05-01-98|\n",
            "| 7698|   SGR|  MANAGER|7839|2850|null|    30|    06-01-97|\n",
            "| 7782|  RAVI|  MANAGER|7839|2450|null|    10|    07-01-96|\n",
            "| 7788| SCOTT|  ANALYST|7566|3000|null|    20|    08-01-95|\n",
            "| 7839|  KING|PRESIDENT|null|5000|null|    10|        null|\n",
            "| 7844|TURNER| SALESMAN|7698|1500|   0|    30|    01-02-80|\n",
            "| 7876| ADAMS|    CLERK|7788|1100|null|    20|    02-02-81|\n",
            "| 7900| JAMES|    CLERK|7698| 950|null|    30|    03-02-82|\n",
            "| 7902|  FORD|  ANALYST|7566|3000|null|    20|    04-02-83|\n",
            "| 7934|MILLER|    CLERK|7782|1300|null|    10|    05-02-84|\n",
            "| 1234|SEKHAR|   doctor|7777| 667|  78|    80|    06-02-80|\n",
            "| 7369| SMITH|    CLERK|7902| 800|null|    20|    07-02-81|\n",
            "| 7499| ALLEN| SALESMAN|7698|1600| 300|    30|    08-02-82|\n",
            "| 7521|  WARD| SALESMAN|7698|1250| 500|    30|        null|\n",
            "| 7566| JONES|  MANAGER|7839|2975|null|    20|    01-02-83|\n",
            "+-----+------+---------+----+----+----+------+------------+\n",
            "\n",
            "root\n",
            " |-- EMPNO: integer (nullable = true)\n",
            " |-- ENAME: string (nullable = true)\n",
            " |-- JOB: string (nullable = true)\n",
            " |-- MGR: integer (nullable = true)\n",
            " |-- SAL: integer (nullable = true)\n",
            " |-- COMM: integer (nullable = true)\n",
            " |-- DEPTNO: integer (nullable = true)\n",
            " |-- UPDATED_DATE: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Spark defaulty chooses year after 2000 \n",
        "\n",
        "# we need to set spark sql legcy timeparserpolicy to legacy -- Useful for less columns data only ,if more data means need ask source system set proper date format\n",
        "\n",
        "spark.conf.set('spark.sql.legacy.timeParserPolicy','LEGACY')\n",
        "\n",
        "yy_df.withColumn('DATE',to_date('UPDATED_DATE','dd-mm-yy')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZWmZ66P5Iqy",
        "outputId": "5a1145f4-2636-4122-9e28-5f61cbcd617c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+---------+----+----+----+------+------------+----------+\n",
            "|EMPNO| ENAME|      JOB| MGR| SAL|COMM|DEPTNO|UPDATED_DATE|      DATE|\n",
            "+-----+------+---------+----+----+----+------+------------+----------+\n",
            "| 7369| SMITH|    CLERK|7902| 800|null|    20|    01-01-98|1998-01-01|\n",
            "| 7499| ALLEN| SALESMAN|7698|1600| 300|    30|    02-01-97|1997-01-02|\n",
            "| 7521|  WARD| SALESMAN|7698|1250| 500|    30|    03-01-96|1996-01-03|\n",
            "| 7566| JONES|  MANAGER|7839|2975|null|    20|    04-01-95|1995-01-04|\n",
            "| 7654|MARTIN| SALESMAN|7698|1250|1400|    30|    05-01-98|1998-01-05|\n",
            "| 7698|   SGR|  MANAGER|7839|2850|null|    30|    06-01-97|1997-01-06|\n",
            "| 7782|  RAVI|  MANAGER|7839|2450|null|    10|    07-01-96|1996-01-07|\n",
            "| 7788| SCOTT|  ANALYST|7566|3000|null|    20|    08-01-95|1995-01-08|\n",
            "| 7839|  KING|PRESIDENT|null|5000|null|    10|        null|      null|\n",
            "| 7844|TURNER| SALESMAN|7698|1500|   0|    30|    01-02-80|1980-01-01|\n",
            "| 7876| ADAMS|    CLERK|7788|1100|null|    20|    02-02-81|1981-01-02|\n",
            "| 7900| JAMES|    CLERK|7698| 950|null|    30|    03-02-82|1982-01-03|\n",
            "| 7902|  FORD|  ANALYST|7566|3000|null|    20|    04-02-83|1983-01-04|\n",
            "| 7934|MILLER|    CLERK|7782|1300|null|    10|    05-02-84|1984-01-05|\n",
            "| 1234|SEKHAR|   doctor|7777| 667|  78|    80|    06-02-80|1980-01-06|\n",
            "| 7369| SMITH|    CLERK|7902| 800|null|    20|    07-02-81|1981-01-07|\n",
            "| 7499| ALLEN| SALESMAN|7698|1600| 300|    30|    08-02-82|1982-01-08|\n",
            "| 7521|  WARD| SALESMAN|7698|1250| 500|    30|        null|      null|\n",
            "| 7566| JONES|  MANAGER|7839|2975|null|    20|    01-02-83|1983-01-01|\n",
            "+-----+------+---------+----+----+----+------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle duplicate column error"
      ],
      "metadata": {
        "id": "yRGG8fPJG4jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/dept.csv')\n",
        "emp = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/emp.csv')\n",
        "\n",
        "dept.show()\n",
        "emp.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R53nhCSJHE-A",
        "outputId": "a251f26b-248d-4ee1-9542-7cdb3d739e7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-----+\n",
            "|     DNAME|     LOC|depno|\n",
            "+----------+--------+-----+\n",
            "|ACCOUNTING|NEW YORK|   10|\n",
            "|  RESEARCH|  DALLAS|   20|\n",
            "|     SALES| CHICAGO|   30|\n",
            "|OPERATIONS|  BOSTON|   40|\n",
            "|ACCOUNTING|NEW YORK|   50|\n",
            "|  RESEARCH|  DALLAS|   60|\n",
            "|     SALES| CHICAGO|   70|\n",
            "|OPERATIONS|  BOSTON|   80|\n",
            "|ACCOUNTING|NEW YORK|   90|\n",
            "|  RESEARCH|  DALLAS|  100|\n",
            "|     SALES| CHICAGO|   10|\n",
            "|OPERATIONS|  BOSTON|   20|\n",
            "|ACCOUNTING|NEW YORK|   30|\n",
            "|  RESEARCH|  DALLAS|   40|\n",
            "|     SALES| CHICAGO|   50|\n",
            "|OPERATIONS|  BOSTON|   60|\n",
            "+----------+--------+-----+\n",
            "\n",
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "|EMPNO| ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "| 7369| SMITH|   CLERK|7902|17-12-1980| 800| 300|    10|  01-01-2022|\n",
            "| 7499| ALLEN|SALESMAN|7698|20-02-1981|1600| 300|    20|  01-01-2022|\n",
            "| 7521|  WARD|SALESMAN|7698|22-02-1981|1250| 500|    30|  01-01-2022|\n",
            "| 7566| JONES| MANAGER|7839|04-02-1981|2975|null|    40|  05-01-2022|\n",
            "| 7654|MARTIN|SALESMAN|7698|21-09-1981|1250|1400|    50|  03-01-2022|\n",
            "| 7698|   SGR|    null|7839|05-01-1981|2850|1600|    60|  04-01-2022|\n",
            "| 7782|  RAVI| MANAGER|7839|06-09-1981|2450| 100|  null|  02-01-2022|\n",
            "| 7788| SCOTT| ANALYST|7566|19-04-1987|null|null|    80|  02-01-2022|\n",
            "| null|  null| MANAGER|null|01-11-1981|null|null|  null|  02-01-2022|\n",
            "| 7844|TURNER|SALESMAN|7698|09-08-1981|1500|null|   100|  02-01-2022|\n",
            "| 7876| ADAMS|   CLERK|7788|23-05-1987|1100|null|    10|  03-01-2022|\n",
            "| 7900| JAMES|   CLERK|7698|12-03-1981| 950|null|    20|  03-01-2022|\n",
            "| 7902|  FORD| ANALYST|7566|12-03-1981|3000|null|    30|  03-01-2022|\n",
            "| 7369| SMITH|   CLERK|7902|17-12-1980| 800|null|    40|  04-01-2022|\n",
            "| 7499| ALLEN|SALESMAN|7698|20-02-1981|1600| 300|    50|  04-01-2022|\n",
            "| 7521|  WARD|SALESMAN|7698|22-02-1981|1250| 500|    60|  04-01-2022|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innerjoin \n",
        "\n",
        "emp_dept = emp.join(dept,emp['DEPTNO'] == dept['depno'],'inner').drop('depno')\n",
        "\n",
        "emp_dept.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS1DMkVyHScx",
        "outputId": "a99d7166-e538-43b1-d83b-6ed1bce90502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------+----+----------+----+----+------+------------+----------+--------+\n",
            "|EMPNO| ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|     DNAME|     LOC|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+----------+--------+\n",
            "| 7369| SMITH|   CLERK|7902|17-12-1980| 800| 300|    10|  01-01-2022|     SALES| CHICAGO|\n",
            "| 7369| SMITH|   CLERK|7902|17-12-1980| 800| 300|    10|  01-01-2022|ACCOUNTING|NEW YORK|\n",
            "| 7499| ALLEN|SALESMAN|7698|20-02-1981|1600| 300|    20|  01-01-2022|OPERATIONS|  BOSTON|\n",
            "| 7499| ALLEN|SALESMAN|7698|20-02-1981|1600| 300|    20|  01-01-2022|  RESEARCH|  DALLAS|\n",
            "| 7521|  WARD|SALESMAN|7698|22-02-1981|1250| 500|    30|  01-01-2022|ACCOUNTING|NEW YORK|\n",
            "| 7521|  WARD|SALESMAN|7698|22-02-1981|1250| 500|    30|  01-01-2022|     SALES| CHICAGO|\n",
            "| 7566| JONES| MANAGER|7839|04-02-1981|2975|null|    40|  05-01-2022|  RESEARCH|  DALLAS|\n",
            "| 7566| JONES| MANAGER|7839|04-02-1981|2975|null|    40|  05-01-2022|OPERATIONS|  BOSTON|\n",
            "| 7654|MARTIN|SALESMAN|7698|21-09-1981|1250|1400|    50|  03-01-2022|     SALES| CHICAGO|\n",
            "| 7654|MARTIN|SALESMAN|7698|21-09-1981|1250|1400|    50|  03-01-2022|ACCOUNTING|NEW YORK|\n",
            "| 7698|   SGR|    null|7839|05-01-1981|2850|1600|    60|  04-01-2022|OPERATIONS|  BOSTON|\n",
            "| 7698|   SGR|    null|7839|05-01-1981|2850|1600|    60|  04-01-2022|  RESEARCH|  DALLAS|\n",
            "| 7788| SCOTT| ANALYST|7566|19-04-1987|null|null|    80|  02-01-2022|OPERATIONS|  BOSTON|\n",
            "| 7844|TURNER|SALESMAN|7698|09-08-1981|1500|null|   100|  02-01-2022|  RESEARCH|  DALLAS|\n",
            "| 7876| ADAMS|   CLERK|7788|23-05-1987|1100|null|    10|  03-01-2022|     SALES| CHICAGO|\n",
            "| 7876| ADAMS|   CLERK|7788|23-05-1987|1100|null|    10|  03-01-2022|ACCOUNTING|NEW YORK|\n",
            "| 7900| JAMES|   CLERK|7698|12-03-1981| 950|null|    20|  03-01-2022|OPERATIONS|  BOSTON|\n",
            "| 7900| JAMES|   CLERK|7698|12-03-1981| 950|null|    20|  03-01-2022|  RESEARCH|  DALLAS|\n",
            "| 7902|  FORD| ANALYST|7566|12-03-1981|3000|null|    30|  03-01-2022|ACCOUNTING|NEW YORK|\n",
            "| 7902|  FORD| ANALYST|7566|12-03-1981|3000|null|    30|  03-01-2022|     SALES| CHICAGO|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# write into a delta table\n",
        "\n",
        "emp_dept.write.saveAsTable('emp_dept_table')"
      ],
      "metadata": {
        "id": "zS5Y4i8iLy8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select count(*) from emp_dept_table').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ls7yebjN8Rc",
        "outputId": "046af9ae-27fe-41e9-ce16-e12a429a36bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|      26|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle bad data\n",
        "\n"
      ],
      "metadata": {
        "id": "Af6xhc4-Px-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/channels.csv')\n",
        "\n",
        "bad.show()\n",
        "\n",
        "bad.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZEw-1NGPvoj",
        "outputId": "5c15de1b-0adc-40c4-9f43-35c3afed4fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+-------------+----------------+-------------+----------------+\n",
            "|          CHANNEL_ID|CHANNEL_DESC|CHANNEL_CLASS|CHANNEL_CLASS_ID|CHANNEL_TOTAL|CHANNEL_TOTAL_ID|\n",
            "+--------------------+------------+-------------+----------------+-------------+----------------+\n",
            "|                   3|Direct Sales|       Direct|              12|Channel total|               1|\n",
            "|                   9|  Tele Sales|       Direct|              12|Channel total|               1|\n",
            "|                   5|     Catalog|     Indirect|              13|Channel total|               1|\n",
            "|                   4|    Internet|     Indirect|              13|Channel total|               1|\n",
            "|              sample|    Partners|       Others|              14|Channel total|               1|\n",
            "|10 Partners Other...|        null|         null|            null|         null|            null|\n",
            "|11 Partners Other...|        null|         null|            null|         null|            null|\n",
            "+--------------------+------------+-------------+----------------+-------------+----------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('CHANNEL_ID', StringType(), True), StructField('CHANNEL_DESC', StringType(), True), StructField('CHANNEL_CLASS', StringType(), True), StructField('CHANNEL_CLASS_ID', IntegerType(), True), StructField('CHANNEL_TOTAL', StringType(), True), StructField('CHANNEL_TOTAL_ID', IntegerType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark read Mode:\n",
        "\n",
        "1.PERMISSIVE - allows bad data - it's spark default mode\n",
        "\n",
        "2.FAILFAST - won't allows bad data -it raise expection - it won't process further\n",
        "\n",
        "3.DROPMALFORMED - drops bad records based on schema -it won't save bad records\n",
        "\n",
        "4.badrecordsPath - save good data in table and saves bad it another path"
      ],
      "metadata": {
        "id": "IOi9TvtAacB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# need to add _corrupt_record column string type in custom made schema\n",
        "schema  = StructType([StructField('CHANNEL_ID', IntegerType(), True), \n",
        "                      StructField('CHANNEL_DESC', StringType(), True), \n",
        "                      StructField('CHANNEL_CLASS', StringType(), True), \n",
        "                      StructField('CHANNEL_CLASS_ID', IntegerType(), True), \n",
        "                      StructField('CHANNEL_TOTAL', StringType(), True), \n",
        "                      StructField('CHANNEL_TOTAL_ID', IntegerType(), True),\n",
        "                      StructField(\"BadData\", StringType(), True)])"
      ],
      "metadata": {
        "id": "gj8coYWURVLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save bad Records Using mode - PERMISSIVE and _corrupt_record ,columnNameofCorrputRecord\n",
        "\n",
        "bad1 = spark.read.format('csv').schema(schema).option('Mode','PERMISSIVE').option('ColumnNameOfCorruptRecord','BadData').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad1.show()\n",
        "\n",
        "# filter good records\n",
        "goodData = bad1.filter('BadData is Null').drop('BAdData')\n",
        "goodData.show()\n",
        "\n",
        "# filter corrupt records\n",
        "bad3 = bad1.filter('BadData is Not Null')\n",
        "bad3.show()"
      ],
      "metadata": {
        "id": "2gGJjO8fidU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf81d44c-22df-4f38-f286-5e8d760fc859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+----------------+-------------+----------------+--------------------+\n",
            "|CHANNEL_ID|CHANNEL_DESC|CHANNEL_CLASS|CHANNEL_CLASS_ID|CHANNEL_TOTAL|CHANNEL_TOTAL_ID|             BadData|\n",
            "+----------+------------+-------------+----------------+-------------+----------------+--------------------+\n",
            "|         3|Direct Sales|       Direct|              12|Channel total|               1|                null|\n",
            "|         9|  Tele Sales|       Direct|              12|Channel total|               1|                null|\n",
            "|         5|     Catalog|     Indirect|              13|Channel total|               1|                null|\n",
            "|         4|    Internet|     Indirect|              13|Channel total|               1|                null|\n",
            "|      null|    Partners|       Others|              14|Channel total|               1|sample,Partners,O...|\n",
            "|      null|        null|         null|            null|         null|            null|10 Partners Other...|\n",
            "|      null|        null|         null|            null|         null|            null|11 Partners Other...|\n",
            "+----------+------------+-------------+----------------+-------------+----------------+--------------------+\n",
            "\n",
            "+----------+------------+-------------+----------------+-------------+----------------+\n",
            "|CHANNEL_ID|CHANNEL_DESC|CHANNEL_CLASS|CHANNEL_CLASS_ID|CHANNEL_TOTAL|CHANNEL_TOTAL_ID|\n",
            "+----------+------------+-------------+----------------+-------------+----------------+\n",
            "|         3|Direct Sales|       Direct|              12|Channel total|               1|\n",
            "|         9|  Tele Sales|       Direct|              12|Channel total|               1|\n",
            "|         5|     Catalog|     Indirect|              13|Channel total|               1|\n",
            "|         4|    Internet|     Indirect|              13|Channel total|               1|\n",
            "+----------+------------+-------------+----------------+-------------+----------------+\n",
            "\n",
            "+----------+------------+-------------+----------------+-------------+----------------+--------------------+\n",
            "|CHANNEL_ID|CHANNEL_DESC|CHANNEL_CLASS|CHANNEL_CLASS_ID|CHANNEL_TOTAL|CHANNEL_TOTAL_ID|             BadData|\n",
            "+----------+------------+-------------+----------------+-------------+----------------+--------------------+\n",
            "|      null|    Partners|       Others|              14|Channel total|               1|sample,Partners,O...|\n",
            "|      null|        null|         null|            null|         null|            null|10 Partners Other...|\n",
            "|      null|        null|         null|            null|         null|            null|11 Partners Other...|\n",
            "+----------+------------+-------------+----------------+-------------+----------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mode - FAILFAST\n",
        "\n",
        "bad = spark.read.format('csv').schema(schema).option('mode','FAILFAST').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ArLjWoOOUdA7",
        "outputId": "317b6509-73dd-4daa-8269-c6cd8f9c55ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-956677a57fd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FAILFAST'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nullValue'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/channels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o350.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 85.0 failed 1 times, most recent failure: Lost task 0.0 in stage 85.0 (TID 76) (7e9bdb729725 executor driver): org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file file:///content/channels.csv. Details: \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:731)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 19 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 25 more\nCaused by: java.lang.RuntimeException: Malformed CSV record\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat jdk.internal.reflect.GeneratedMethodAccessor53.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.sql.execution.QueryExecutionException: Encountered error while reading file file:///content/channels.csv. Details: \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:731)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:68)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:421)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:274)\n\t... 19 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.RuntimeException: Malformed CSV record\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:330)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:275)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:417)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 25 more\nCaused by: java.lang.RuntimeException: Malformed CSV record\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedCSVRecordError(QueryExecutionErrors.scala:1222)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:298)\n\t... 28 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DROPMALFORMED\n",
        "\n",
        "bad = spark.read.format('csv').schema(schema).option('mode','DROPMALFORMED').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad.show()"
      ],
      "metadata": {
        "id": "Tek46tmobPMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference Between sort and order by\n",
        "\n",
        "\n",
        "\n",
        "1. Spark_sql : orderBy will do sorting an entire data ,sortby will do Partition wise sorting in sparksql .\n",
        "\n",
        "\n",
        "2. pyspark : orderBy and sort are same pyspark.sortwithinpartitions same as sortby ( it will do Partition wise sorting)\n",
        "\n"
      ],
      "metadata": {
        "id": "_SoyRZHsFe3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ta_df = spark.read.load('/content/spark-warehouse/oracle_db.db/emp_dept_table').repartition(4,'DEPTNO').withColumn('partition',spark_partition_id())\n",
        "\n",
        "ta_df.show()"
      ],
      "metadata": {
        "id": "256lRsObIJMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In Pyspark -orderBy and sort are same pyspark\n",
        "\n",
        "#orderBy\n",
        "\n",
        "ta_df.orderBy('SAL').show()"
      ],
      "metadata": {
        "id": "H1LZcRkmI83G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort\n",
        "ta_df.sort('SAL').show()"
      ],
      "metadata": {
        "id": "KxxocSEhMJwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sortWithinPartitions -sortwithinpartitions same as sortby ( it will do Partition wise sorting)\n",
        "\n",
        "ta_df.sortWithinPartitions('SAL').show()"
      ],
      "metadata": {
        "id": "z1Za9R5zK5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ta_df.createOrReplaceTempView('ta_df')"
      ],
      "metadata": {
        "id": "zUOvhVlRPBYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#order by - sort entire data \n",
        "\n",
        "spark.sql('select * from ta_df order by SAL').show()"
      ],
      "metadata": {
        "id": "zFLHWrMfMe2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort by - will do partition wise sorting\n",
        "spark.sql('select * from ta_df sort by SAL').show()"
      ],
      "metadata": {
        "id": "qD_3TvvfNRWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# coalesce and repartition in rdd \n",
        "\n",
        "coalesce : is a  Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.\n",
        "for increasing partitions we need provide another argument True ,then it will shuffle the data.\n",
        "\n",
        "repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions\n"
      ],
      "metadata": {
        "id": "bAzJc7yDRGeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "2Fw90GAERUs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(range(10),5)\n",
        "\n",
        "rdd1 = rdd.coalesce(2) # used to decrease no.of partitions ,No shuffle will happen\n",
        "\n",
        "rdd2 = rdd.coalesce(4,True)  # use True to increase no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd3 = rdd.repartition(2)    # use True to decrease no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd4 = rdd.repartition(6)   # use True to increase no.of partitions ,shuffle will happen\n",
        "\n",
        "\n",
        "print('original rdd', rdd.glom().collect())\n",
        "print('coalesce 2 ',rdd1.glom().collect())\n",
        "print('coalesce 4',rdd2.glom().collect())\n",
        "print('repartition 2',rdd3.glom().collect())\n",
        "print('repartition 6',rdd4.glom().collect())"
      ],
      "metadata": {
        "id": "_KX9V-ULWfAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# coalesce and repartition in dataframe\n",
        "\n",
        "coalesce : is a  Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.\n",
        "\n",
        "\n",
        "repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions,\n",
        "we can repartition based on column specific to increse the performence "
      ],
      "metadata": {
        "id": "1b2vS5BA6BwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df = spark.read.load('/content/spark-warehouse/oracle_db.db/emp_dept_table')\n",
        "print(cr_df.rdd.getNumPartitions())\n",
        "cr_df.show()"
      ],
      "metadata": {
        "id": "5hiHFFzr6HRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df1 = cr_df.repartition(4).withColumn('partition_id',spark_partition_id())\n",
        "cr_df1.show()"
      ],
      "metadata": {
        "id": "EYgmLeEI6bl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#repartition based on joining columns/filtering column to imporve performance\n",
        "cr_df2 = cr_df.repartition(4,'DEPTNO').withColumn('partition_id',spark_partition_id())\n",
        "cr_df2.show()"
      ],
      "metadata": {
        "id": "Ob07UMiAJRF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df3 = cr_df.coalesce(3).withColumn('partition_id',spark_partition_id())\n",
        "cr_df3.show()"
      ],
      "metadata": {
        "id": "ZpLHWRi8FwqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j2LoRlHWGD8S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}