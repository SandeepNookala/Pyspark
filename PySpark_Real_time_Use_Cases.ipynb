{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3in-2lvQJZb2",
        "HQTXxbqnJYFw",
        "mQM_-Rq31OHy",
        "u22VawEEFCLg",
        "MPp15hFA-b5g",
        "ANG4ONGDTCRw",
        "1d7XuxL5X50r",
        "Egb13y1nnydU",
        "NFRXq77I1Tez",
        "HPhKhuoiRwB6",
        "6KOrxwloiaUn",
        "BAuEVPpOuRmG",
        "pERd85OnPkP0",
        "SogxNiM-NzVp",
        "AiXskdNP3leY",
        "yRGG8fPJG4jq",
        "Af6xhc4-Px-3",
        "_SoyRZHsFe3D",
        "bAzJc7yDRGeP",
        "1b2vS5BA6BwK",
        "CyDwbLPUMKAw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pyspark"
      ],
      "metadata": {
        "id": "3in-2lvQJZb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQbx377bOs0o",
        "outputId": "a0a40445-d76b-44be-b1f9-0a77d5dedfa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.9/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# creating spark Session"
      ],
      "metadata": {
        "id": "HQTXxbqnJYFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('demo').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF9MfnxdPLam",
        "outputId": "f12ebdaf-9662-450b-e581-e0fa24a59791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f0dec56c4f0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://abbeaaac0d7c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>demo</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# convert from String (dd-mm-yyyy) date format to spark date format (yyyy-mm-dd)"
      ],
      "metadata": {
        "id": "mQM_-Rq31OHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# date format in Traditional databases is dd-mm-yyyy\n",
        "# date format in spark is yyyy-mm-dd\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "df1 = df.withColumn('HIREDATE',to_date('HIREDATE','dd-mm-yyyy')).withColumn('UPDATED_DATE',to_date('UPDATED_DATE','dd-mm-yyyy'))\n",
        "\n",
        "df1.show(5)\n",
        "df1.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxu3d5Aq33FU",
        "outputId": "dfa8e1ec-bb1c-4b9c-f90c-5d5bdfa5ca0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "|EMPNO| ENAME|     JOB| MGR|  HIREDATE| SAL|COMM|DEPTNO|UPDATED_DATE|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "| 7369| SMITH|   CLERK|7902|1980-01-17| 800| 300|    10|  2022-01-01|\n",
            "| 7499| ALLEN|SALESMAN|7698|1981-01-20|1600| 300|    20|  2022-01-01|\n",
            "| 7521|  WARD|SALESMAN|7698|1981-01-22|1250| 500|    30|  2022-01-01|\n",
            "| 7566| JONES| MANAGER|7839|1981-01-04|2975|null|    40|  2022-01-05|\n",
            "| 7654|MARTIN|SALESMAN|7698|1981-01-21|1250|1400|    10|  2022-01-03|\n",
            "+-----+------+--------+----+----------+----+----+------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- EMPNO: integer (nullable = true)\n",
            " |-- ENAME: string (nullable = true)\n",
            " |-- JOB: string (nullable = true)\n",
            " |-- MGR: integer (nullable = true)\n",
            " |-- HIREDATE: date (nullable = true)\n",
            " |-- SAL: integer (nullable = true)\n",
            " |-- COMM: integer (nullable = true)\n",
            " |-- DEPTNO: integer (nullable = true)\n",
            " |-- UPDATED_DATE: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Adding new columns with different values"
      ],
      "metadata": {
        "id": "u22VawEEFCLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2.withColumn('Levels',when(col('JOB')=='SALESMAN','Level3').when(col('JOB')=='CLERK','Level2').when(col('JOB')=='MANAGER','Level1')\n",
        "df3.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "I_jtJEyJqBhB",
        "outputId": "26addea9-42a3-4a75-fa2a-9d9480cbdaa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6612b3031bbd>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    df3.show()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Frame from mysql table"
      ],
      "metadata": {
        "id": "MPp15hFA-b5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "df_mysql = spark.read.format('jdbc').\\\n",
        "           option('url','jdbc:mysql://localhost:3306').\\\n",
        "           option('driver','com.mysql.jdbc.Driver').\\\n",
        "           option('user','root').\\\n",
        "           option('password','sandeep').\\\n",
        "           option('query','select * from sandeep.emp_table').\\\n",
        "           load()\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "kfHXuuI-BKOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Json file Handling\n",
        "\n",
        "\n",
        "complex Data types\n",
        "\n",
        "1.struct - dict\n",
        "\n",
        "2.array -  list - To flattern complex datatype(array datatype ) we can you explode() function\n",
        "\n",
        "3.map\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ANG4ONGDTCRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrame from Json file\n",
        "\n",
        "data = spark.read.format('json').load('/content/emp.json')\n",
        "data.show()\n",
        "data.printSchema()\n",
        "data.count()\n"
      ],
      "metadata": {
        "id": "CmRO4ybgDPxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating DataFrame from multiLine Json file"
      ],
      "metadata": {
        "id": "1d7XuxL5X50r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if json file has  more no.of lines (nested data) .then we should use multiLine in options\n",
        "\n",
        "Mul_data = spark.read.format('json').option('multiline',True).option('inferSchema',True).option('nullValue','null').load('/content/nested_json.json')\n",
        "\n",
        "Mul_data.show(truncate = False)\n",
        "\n",
        "Mul_data.printSchema()\n",
        "Mul_data.count()"
      ],
      "metadata": {
        "id": "OwpKEkmDDbJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mul_data1 = Mul_data.withColumn('batters_exp',explode('batters.batter')) \\\n",
        "         .withColumn('batters_id',col('batters_exp.id')) \\\n",
        "         .withColumn('batters_type',col('batters_exp.type')) \\\n",
        "         .drop('batters','batters_exp') \\\n",
        "         .withColumn('topping_exp',explode('topping')) \\\n",
        "         .withColumn('topping_id',col('topping_exp.id')) \\\n",
        "         .withColumn('topping_type',col('topping_exp.type')) \\\n",
        "         .drop('topping','topping_exp') \\\n",
        "\n",
        "\n",
        "Mul_data1.show(10)\n",
        "\n",
        "Mul_data1.printSchema()\n",
        "\n",
        "Mul_data1.count()"
      ],
      "metadata": {
        "id": "sgba5VYV1jn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions in pyspark"
      ],
      "metadata": {
        "id": "Egb13y1nnydU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "fun = spark.sql('show functions')\n",
        "print(fun.count())\n",
        "print(fun.show())"
      ],
      "metadata": {
        "id": "R1C9_AAVn_JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(fun))\n",
        "\n",
        "# describe function details\n",
        "spark.sql('describe function aggregate').show(truncate=False)"
      ],
      "metadata": {
        "id": "XwurJt6fqA8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating dataframe from Hive table\n",
        "\n"
      ],
      "metadata": {
        "id": "NFRXq77I1Tez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#DSL - Domain Specific language\n",
        "\n",
        "hive = spark.read.table('oracle_db.emp_data')\n",
        "hive.show(10)\n",
        "hive.printSchema()\n",
        "hive.count()\n"
      ],
      "metadata": {
        "id": "0gK-3qRS-Lw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding current timestamp to dataframe"
      ],
      "metadata": {
        "id": "h63qo9lnPaPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hive1 = hive.withColumn('Date',current_timestamp())\n",
        "hive1.show(truncate = False)"
      ],
      "metadata": {
        "id": "DVccCiUwPYeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Dataframe to Hive"
      ],
      "metadata": {
        "id": "HPhKhuoiRwB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hive1.write.partitionBy('HIREDATE').saveAsTable('Emp_Hive')\n"
      ],
      "metadata": {
        "id": "T5vvRub2OMPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select * from emp_hive').show()"
      ],
      "metadata": {
        "id": "-jcx-9CPTBn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word count program step by step"
      ],
      "metadata": {
        "id": "6KOrxwloiaUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rd = spark.sparkContext.textFile(\"/content/word.txt\")\n",
        "\n",
        "print(type(rd))\n",
        "print(rd.collect())       # no.of lines\n",
        "print(rd.count())"
      ],
      "metadata": {
        "id": "SxIeaVvVjktl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rd1 = rd.map(lambda x : x.encode('utf-8'))\n",
        "\n",
        "print(rd1.collect())"
      ],
      "metadata": {
        "id": "ZGHpc8uit29t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map - gives Number of lists of strings\n",
        "\n",
        "rd2 = rd.map(lambda x:x.split(' '))\n",
        "print(type(rd2))\n",
        "print(rd2.collect())"
      ],
      "metadata": {
        "id": "VnO7twMLmKK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#flatmap - gives single list of strings\n",
        "\n",
        "rd3 = rd.flatMap(lambda x : x.split(' '))\n",
        "print(rd3.collect())"
      ],
      "metadata": {
        "id": "smr8WA0-mgeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assiging value to strings\n",
        "rd4 =  rd3.map(lambda x : (x,1))\n",
        "print(rd4.collect())"
      ],
      "metadata": {
        "id": "v5MLyj-KqTRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduceByKey -  groupping and aggregating same keys from rd4\n",
        "\n",
        "rd5 = rd4.reduceByKey(lambda x ,y : x+y)\n",
        "\n",
        "print(rd5.collect())"
      ],
      "metadata": {
        "id": "BHsbsr60r1I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# word count in single line - Find No.of Occurences of single word"
      ],
      "metadata": {
        "id": "BAuEVPpOuRmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RDD = spark.sparkContext.textFile('/content/word.txt').flatMap(lambda x : x.split(' ')).map(lambda x:(x,1)).reduceByKey(lambda x,y : x+y)\n",
        "\n",
        "RDD.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lQ_cEii5KDk",
        "outputId": "42e5e428-fab2-43a2-db25-c8ec54d98642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 1),\n",
              " ('forest', 1),\n",
              " ('raven', 3),\n",
              " ('also', 2),\n",
              " ('commonly', 1),\n",
              " ('known', 1),\n",
              " ('as', 3),\n",
              " ('the', 4),\n",
              " ('Tasmanian', 2),\n",
              " ('is', 1),\n",
              " ('a', 2),\n",
              " ('passerine', 1),\n",
              " ('bird', 1),\n",
              " ('in', 3),\n",
              " ('family', 1),\n",
              " ('Corvidae', 1),\n",
              " ('native', 1),\n",
              " ('to', 2),\n",
              " ('Tasmania', 1),\n",
              " ('and', 5),\n",
              " ('parts', 2),\n",
              " ('of', 3),\n",
              " ('southern', 1),\n",
              " ('Victoria', 1),\n",
              " ('such', 1),\n",
              " ('Wilsons', 1),\n",
              " ('Promontory', 1),\n",
              " ('Portland', 1),\n",
              " ('Populations', 1),\n",
              " ('are', 2),\n",
              " ('found', 1),\n",
              " ('New', 2),\n",
              " ('South', 2),\n",
              " ('Wales', 2),\n",
              " ('including', 1),\n",
              " ('Dorrigo', 1),\n",
              " ('Armidale', 1),\n",
              " ('it', 1),\n",
              " ('has', 1),\n",
              " ('allblack', 1),\n",
              " ('plumage', 1),\n",
              " ('beak', 1),\n",
              " ('legs', 1),\n",
              " ('As', 1),\n",
              " ('with', 2),\n",
              " ('other', 1),\n",
              " ('two', 1),\n",
              " ('species', 1),\n",
              " ('Australia,', 1),\n",
              " ('its', 1),\n",
              " ('black', 1),\n",
              " ('feathers', 1),\n",
              " ('have', 3),\n",
              " ('grey', 1),\n",
              " ('bases', 1),\n",
              " ('Adults', 1),\n",
              " ('white', 1),\n",
              " ('irises;', 1),\n",
              " ('younger', 1),\n",
              " ('birds', 1),\n",
              " ('dark', 1),\n",
              " ('brown', 1),\n",
              " ('then', 1),\n",
              " ('hazel', 1),\n",
              " ('irises', 1),\n",
              " ('an', 1),\n",
              " ('inner', 1),\n",
              " ('blue', 1),\n",
              " ('rim', 1),\n",
              " ('populations', 1),\n",
              " ('recognised', 1),\n",
              " ('separate', 1),\n",
              " ('subspecies', 2),\n",
              " ('C', 1),\n",
              " ('tasmanicus', 1),\n",
              " ('boreus', 1),\n",
              " ('but', 1),\n",
              " ('appear', 1),\n",
              " ('be', 1),\n",
              " ('nested', 1),\n",
              " ('within', 1),\n",
              " ('genetically', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FqvSfJiiF-Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fill missing data in textFile and convert into Dataframe"
      ],
      "metadata": {
        "id": "pERd85OnPkP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_miss = spark.read.format('csv').option('sep',' ').load('/content/fill missing.txt').fillna('no data')\n",
        "\n",
        "df_miss.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "AClRPWqjPuk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd5b688-c4e7-4c0c-bfd1-e7f3b02768f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|  _c0|    _c1|    _c2|    _c3|    _c4|    _c5|    _c6|    _c7|    _c8|    _c9|   _c10|   _c11|\n",
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|  Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "|Spark|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|no data|\n",
            "+-----+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#udf register"
      ],
      "metadata": {
        "id": "SogxNiM-NzVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# UDF\n",
        "\n",
        "def f1(x):\n",
        "  return ((x*x)-x)\n",
        "\n",
        "spark.udf.register('fun',f1,IntegerType())\n",
        "fun = udf(f1,IntegerType())\n",
        "\n",
        "print(f1(5))"
      ],
      "metadata": {
        "id": "ux58m9HqOdSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df5.select('ENAME',sqrt('SAL'),fun('SAL')).show()"
      ],
      "metadata": {
        "id": "k2sKsO_bPgeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle yy date format in pyspark for before 2000 data"
      ],
      "metadata": {
        "id": "AiXskdNP3leY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yy_df = spark.read.format('csv').option('header',True).option('sep','|').option('inferSchema',True).option('nullValue','null').load('/content/emp_pipe_yy.txt')\n",
        "\n",
        "yy_df.show()\n",
        "yy_df.printSchema()"
      ],
      "metadata": {
        "id": "0mvLKlJd4Amy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Spark defaulty chooses year after 2000\n",
        "\n",
        "# we need to set spark sql legcy timeparserpolicy to legacy -- Useful for less columns data only ,if more data means need ask source system set proper date format\n",
        "\n",
        "spark.conf.set('spark.sql.legacy.timeParserPolicy','LEGACY')\n",
        "\n",
        "yy_df.withColumn('DATE',to_date('UPDATED_DATE','dd-mm-yy')).show()"
      ],
      "metadata": {
        "id": "KZWmZ66P5Iqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle duplicate column error"
      ],
      "metadata": {
        "id": "yRGG8fPJG4jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dept = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/dept.csv')\n",
        "emp = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/emp.csv')\n",
        "\n",
        "dept.show()\n",
        "emp.show()"
      ],
      "metadata": {
        "id": "R53nhCSJHE-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# innerjoin\n",
        "\n",
        "emp_dept = emp.join(dept,emp['DEPTNO'] == dept['depno'],'inner').drop('depno')\n",
        "\n",
        "emp_dept.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CS1DMkVyHScx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# write into a delta table\n",
        "\n",
        "emp_dept.write.saveAsTable('emp_dept_table')"
      ],
      "metadata": {
        "id": "zS5Y4i8iLy8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select count(*) from emp_dept_table').show()"
      ],
      "metadata": {
        "id": "-ls7yebjN8Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# how to handle bad data\n",
        "\n"
      ],
      "metadata": {
        "id": "Af6xhc4-Px-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bad = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/content/channels.csv')\n",
        "\n",
        "bad.show()\n",
        "\n",
        "bad.schema"
      ],
      "metadata": {
        "id": "5ZEw-1NGPvoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark read Mode:\n",
        "\n",
        "1.PERMISSIVE - allows bad data - it's spark default mode\n",
        "\n",
        "2.FAILFAST - won't allows bad data -it raise expection - it won't process further\n",
        "\n",
        "3.DROPMALFORMED - drops bad records based on schema -it won't save bad records\n",
        "\n",
        "4.badrecordsPath - save good data in table and saves bad it another path"
      ],
      "metadata": {
        "id": "IOi9TvtAacB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# need to add _corrupt_record column string type in custom made schema\n",
        "schema  = StructType([StructField('CHANNEL_ID', IntegerType(), True),\n",
        "                      StructField('CHANNEL_DESC', StringType(), True),\n",
        "                      StructField('CHANNEL_CLASS', StringType(), True),\n",
        "                      StructField('CHANNEL_CLASS_ID', IntegerType(), True),\n",
        "                      StructField('CHANNEL_TOTAL', StringType(), True),\n",
        "                      StructField('CHANNEL_TOTAL_ID', IntegerType(), True),\n",
        "                      StructField(\"BadData\", StringType(), True)])"
      ],
      "metadata": {
        "id": "gj8coYWURVLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save bad Records Using mode - PERMISSIVE and _corrupt_record ,columnNameofCorrputRecord\n",
        "\n",
        "bad1 = spark.read.format('csv').schema(schema).option('Mode','PERMISSIVE').option('ColumnNameOfCorruptRecord','BadData').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad1.show()\n",
        "\n",
        "# filter good records\n",
        "goodData = bad1.filter('BadData is Null').drop('BAdData')\n",
        "goodData.show()\n",
        "\n",
        "# filter corrupt records\n",
        "bad3 = bad1.filter('BadData is Not Null')\n",
        "bad3.show()"
      ],
      "metadata": {
        "id": "2gGJjO8fidU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mode - FAILFAST\n",
        "\n",
        "bad = spark.read.format('csv').schema(schema).option('mode','FAILFAST').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad.show()\n"
      ],
      "metadata": {
        "id": "ArLjWoOOUdA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DROPMALFORMED\n",
        "\n",
        "bad = spark.read.format('csv').schema(schema).option('mode','DROPMALFORMED').option('header',True).option('nullValue','null').load('/content/channels.csv')\n",
        "bad.show()"
      ],
      "metadata": {
        "id": "Tek46tmobPMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference Between sort and order by\n",
        "\n",
        "\n",
        "\n",
        "1. Spark_sql : orderBy will do sorting an entire data ,sortby will do Partition wise sorting in sparksql .\n",
        "\n",
        "\n",
        "2. pyspark : orderBy and sort are same pyspark.sortwithinpartitions same as sortby ( it will do Partition wise sorting)\n",
        "\n"
      ],
      "metadata": {
        "id": "_SoyRZHsFe3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ta_df = spark.read.load('/content/spark-warehouse/oracle_db.db/emp_dept_table').repartition(4,'DEPTNO').withColumn('partition',spark_partition_id())\n",
        "\n",
        "ta_df.show()"
      ],
      "metadata": {
        "id": "256lRsObIJMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In Pyspark -orderBy and sort are same pyspark\n",
        "\n",
        "#orderBy\n",
        "\n",
        "ta_df.orderBy('SAL').show()\n"
      ],
      "metadata": {
        "id": "H1LZcRkmI83G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort\n",
        "ta_df.sort('SAL').show()"
      ],
      "metadata": {
        "id": "KxxocSEhMJwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sortWithinPartitions -sortwithinpartitions same as sortby ( it will do Partition wise sorting)\n",
        "\n",
        "ta_df.sortWithinPartitions('SAL').show()"
      ],
      "metadata": {
        "id": "z1Za9R5zK5ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ta_df.createOrReplaceTempView('ta_df')"
      ],
      "metadata": {
        "id": "zUOvhVlRPBYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#order by - sort entire data\n",
        "\n",
        "spark.sql('select * from ta_df order by SAL').show()"
      ],
      "metadata": {
        "id": "zFLHWrMfMe2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort by - will do partition wise sorting\n",
        "spark.sql('select * from ta_df sort by SAL').show()"
      ],
      "metadata": {
        "id": "qD_3TvvfNRWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# coalesce and repartition in rdd\n",
        "\n",
        "coalesce : is a  Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.\n",
        "for increasing partitions we need provide another argument True ,then it will shuffle the data.\n",
        "\n",
        "repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions\n"
      ],
      "metadata": {
        "id": "bAzJc7yDRGeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()"
      ],
      "metadata": {
        "id": "2Fw90GAERUs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(range(10),5)\n",
        "\n",
        "rdd1 = rdd.coalesce(2) # used to decrease no.of partitions ,No shuffle will happen\n",
        "\n",
        "rdd2 = rdd.coalesce(4,True)  # use True to increase no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd3 = rdd.repartition(2)    # use True to decrease no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd4 = rdd.repartition(6)   # use True to increase no.of partitions ,shuffle will happen\n",
        "\n",
        "rdd.coalesce()\n",
        "\n",
        "rdd.repartition()\n",
        "\n",
        "print('original rdd', rdd.glom().collect())\n",
        "print('coalesce 2 ',rdd1.glom().collect())\n",
        "print('coalesce 4',rdd2.glom().collect())\n",
        "print('repartition 2',rdd3.glom().collect())\n",
        "print('repartition 6',rdd4.glom().collect())"
      ],
      "metadata": {
        "id": "_KX9V-ULWfAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18fe192e-b278-4060-db6d-946254552f09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original rdd [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n",
            "coalesce 2  [[0, 1, 2, 3], [4, 5, 6, 7, 8, 9]]\n",
            "coalesce 4 [[4, 5], [0, 1, 2, 3], [], [6, 7, 8, 9]]\n",
            "repartition 2 [[4, 5, 6, 7, 8, 9], [0, 1, 2, 3]]\n",
            "repartition 6 [[6, 7], [], [8, 9], [0, 1], [2, 3], [4, 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# coalesce and repartition in dataframe\n",
        "\n",
        "coalesce : is a  Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.\n",
        "\n",
        "\n",
        "repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions,\n",
        "we can repartition based on column specific to increse the performence"
      ],
      "metadata": {
        "id": "1b2vS5BA6BwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df = spark.read.load('/content/spark-warehouse/oracle_db.db/emp_dept_table')\n",
        "print(cr_df.rdd.getNumPartitions())\n",
        "cr_df.show()"
      ],
      "metadata": {
        "id": "5hiHFFzr6HRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df1 = cr_df.repartition(4).withColumn('partition_id',spark_partition_id())\n",
        "cr_df1.show()"
      ],
      "metadata": {
        "id": "EYgmLeEI6bl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#repartition based on joining columns/filtering column to imporve performance\n",
        "cr_df2 = cr_df.repartition(4,'DEPTNO').withColumn('partition_id',spark_partition_id())\n",
        "cr_df2.show()"
      ],
      "metadata": {
        "id": "Ob07UMiAJRF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr_df3 = cr_df.coalesce(3).withColumn('partition_id',spark_partition_id())\n",
        "cr_df3.show()"
      ],
      "metadata": {
        "id": "ZpLHWRi8FwqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.coalesce()\n",
        "df.repartition()"
      ],
      "metadata": {
        "id": "j2LoRlHWGD8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Data Frame from REST API"
      ],
      "metadata": {
        "id": "CyDwbLPUMKAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#REST API -Accessing the data over internet through Urls\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "\n",
        "api = requests.request('GET','https://api.github.com/users/hadley/orgs')\n",
        "\n",
        "data = api.json()\n",
        "\n",
        "file = open('/content/sample_data/apidata.json','a')\n",
        "\n",
        "for record in data:\n",
        "  file.write(\"%s\\n\" %record)\n",
        "\n",
        "api_df = spark.read.format('json').load('/content/sample_data/apidata.json')\n"
      ],
      "metadata": {
        "id": "WMI64DqaMVaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(api.json()))\n",
        "print(len(api.json()))"
      ],
      "metadata": {
        "id": "UE7H0go3YHPw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ea0bc86-e31d-451a-9c6e-697818e024d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_df.show(10)\n",
        "api_df.printSchema()\n",
        "api_df.count()"
      ],
      "metadata": {
        "id": "81TOTdwxVrT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6e23a0d-4068-4fb3-f8ab-2e055751603b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n",
            "root\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    }
  ]
}