
1.Installing Pyspark and Creating new SparkSession:
===================================================
!pip install pyspark py4j

from pyspark.sql import SparkSession

# Create SparkSession
spark = SparkSession.builder.master("local").appName("dataframe").getOrCreate()




2.Creating dataFrame from file Location and show type:
======================================================

df = spark.read.format('csv').load('/Volumes/workspace/sandeep/input/employee.csv')
df.display()
type(df)


3.difference between show,display :
===================================
#Use df.show() → Anywhere, text output in console.
df.show(5)


#Use df.display() → Only in Databricks, gives a nice UI table and visualization options.
df.display()


4.Read csv data into dataframe with Header,inferschema,Null value. show count, printSchema ,5 records:
======================================================================================================
df_csv = spark.read.format('csv').option('header',True).option('inferSchema',True).option('nullValue','null').load('/Volumes/workspace/sandeep/sample/employee.csv')

df_csv.printSchema()

df_csv.show(5)

df_csv.count()



5.Read csv dataframe with pipe seprated values.Fetch row count,5 rows and printSchema:
=======================================================================================
df_pipe = spark.read.format('csv').option('sep','|').option('header',True).option('inferSchema',True).option('NullValue','null').load('/Volumes/workspace/sandeep/sample/emp_pipe.txt')

df_pipe.show(5)

df_pipe.printSchema()

df_pipe.count()


6.Read csv dataframe with double pipe seprated values ,Fetch row count,5 rows and printSchema:
===============================================================================================
df_double_pipe = spark.read.format('csv').option('sep','||').option('header',True).option('inferSchema',True).option('NullValue','null').load('/Volumes/workspace/sandeep/sample/emp_double_pipe.txt')

df_double_pipe.show(5)
df_double_pipe.printSchema()
df_double_pipe.count()



7.Inspect Data:
===============
#show dataframe
df.display()
df.show()

#printSchema
df_pipe.printSchema()

#column names
df_pipe.columns

#columns with their data types
df_pipe.dtypes

#firstrow
df_double_pipe.first()

#head - first five rows
df_double_pipe.head(5)

#take any random 3 rows
df_double_pipe.take(3)

#count no.of rows
df_double_pipe.count()

#count distinct no.of rows
df_double_pipe.distinct().count()

#tail - last two rows
df_double_pipe.tail(2)

# compute summery satistics for dataframe
df_csv.describe().show()

#explain physical plan and logical plan
df_csv.explain(True)



8.import functions:
===================
from pyspark.sql.functions import *


fun = spark.sql('show functions')
print(fun.count())
print(fun.show())



9.select operations:
====================
df_double_pipe.select(( col('MGR') * 2).alias ('new_mgr'),(col('SAL') + 1000).alias ('New_SAl')).show(1)



10.Adding new columns with default values:
==========================================
df_double_pipe = df_double_pipe.withColumn('source',lit('BOS'))
df_double_pipe.show()



11.Adding new columns from another Columns:
===========================================
df_double_pipe = df_double_pipe.withColumn('New_Sal',col('SAL')*2).withColumn('New_mgr',col('MGR')/2)
df_double_pipe.show()


12.dropping columns:
====================
df_double_pipe = df_double_pipe.drop('MGR','SAL')
df_double_pipe.show()



13.Renamed Column Names:
========================
df_double_pipe = df_double_pipe.withColumnRenamed('New_Sal','Salary').withColumnRenamed('mgr','Commission')
df_double_pipe.show()



14.drop null values from dataFrame:
===================================

#how = any ,it drops any null value in dataframe
df_double_pipe_drop1=df_double_pipe.dropna(how ='any')
df_double_pipe_drop1.show()
df_double_pipe_drop1.count()


#how =all,drops the rows with all nulls in it
df_double_pipe_drop2=df_double_pipe.dropna(how='all')
df_double_pipe_drop2.show()
df_double_pipe_drop2.count()


#how =all,drops the rows with custom nulls in it
df_double_pipe_drop3 = df_double_pipe.dropna(how = 'any',thresh= 3 )
df_double_pipe_drop3.show()
df_double_pipe_drop3.count()

#drop the specific columns null values
df_double_pipe_drop4 = df_double_pipe.dropna(how='any',subset=['salary','New_mgr'])
df_double_pipe_drop4.show()
df_double_pipe_drop4.count()



15.fill null values in dataFrame:
================================
df_double_pipe.fillna( 0,subset=['Salary','DEPTNO','New_mgr'] ).fillna('missing_value',subset=['ENAME','JOB']).show()




16.Filter operations:
=====================

# ~ not operator
df_double_pipe.filter( ~ (col('Salary') == 3000) ).show()


# == operator
df_double_pipe.filter( col('Salary') == 3000 ).show()


#salary of people greter than 4000
df_double_pipe.filter( col('salary') >4000).show()


# & operator
#show employee details who's salary greater than 2500 and job is manager
df_double_pipe.filter( (col("JOB") == 'MANAGER') & (col("Salary") > 2500 ) ).show()


# & operator
#show employee details who's salary less than 2000 and salary should be greater than 1000
df_double_pipe.filter( (col('Salary')<2000) & (col('Salary')>1000)).show()


# | operator
#show employee details who's salary less than 2000 or salary should be greater than 1000
df_double_pipe.filter( (col('Salary')<2000) | ( col('Salary')>1000 ) ).show()


# null operator
# filter individual Columns null values

df_double_pipe.filter('HIREDATE is Null').show()
df_double_pipe.filter('New_mgr is Null').show()



17.GroupBy operations:
======================
#groupby deptno and people count
df_double_pipe.groupBy('DEPTNO').count().show()


#groupby deptno and avg salary
df_double_pipe.groupBy("DEPTNO").avg('Salary').show()


#groupby job role and people count
df_double_pipe.groupBy('JOB').count().show()


#Group by job role and avg of salaries
df_double_pipe.groupBy('JOB').avg('Salary').show()



18.sort/orderby operations - In pyspark orderBy and sort are same:
==================================================================
df_double_pipe.orderBy(col('Salary').desc()).show()
df_double_pipe.sort(col('JOB').desc()).show()



19.sort by/sortwithinpartitions operations- In pyspark sortwithinpartitions same as sortby:
===========================================================================================
df_double_pipe.sortWithinPartitions(col('Salary').desc()).show()



20.repartition:
===============
repartition : is a wide transformation : create new partitions,Data shuffle will happen,used for increase/decrease the partitions, we can repartition based on column specific to increse the performence

df_double_pipe_repart = df_double_pipe.repartition(4).withColumn('partition',spark_partition_id())
df_double_pipe_repart.groupBy('partition').count().show()
df_pipe1.rdd.getNumPartitions()



21.coalesce:
============
coalesce : is a Narrow transformation : adjust data in existing partition,No shuffling ,By defult it will used for decrease the partitions.

df_double_pipe_repart_coal = df_double_pipe.coalesce(1)
df_double_pipe_repart_coal.groupBy('partition').count().show()

df_pipe2.rdd.getNumPartitions()



22.joining two dataframes - Generate Revenue Column and order by highest Revenue:
================================================================================
df_product = spark.read.format('csv').option('header',True).option('inferSchema',True).option('NullValue','null').load('/Volumes/workspace/sandeep/sample/product_info_2.csv')
df_product.show()
df_product.printSchema()
df_product.count()

df_sale = spark.read.format('csv').option('header',True).option('inferSchema',True).option('NullValue','null').load('/Volumes/workspace/sandeep/sample/sales_data_2.csv')
df_sale.show()
df_sale.printSchema()
df_sale.count()

poduct_sales = df_product.join(df_sale,df_product.product_id == df_sale.product_id,'inner').drop(df_sale.sl_no).drop(df_sale.product_id).withColumn( 'Revenue', col('unit_price')* col('qty')).orderBy( col('Revenue').desc())
poduct_sales.show()


23.Save Dataframe into hive table:
=================================
product_sale.write.partitionBy('product_name').saveAsTable('product_sale')



24.Read Dataframe from Hive table:
=================================
df_hive = spark.read.table('product_sales')
df_hive.show()



25.Pivot Table : year wise each product revenue:
================================================
product_sales.pivot = product_sales.groupBy('year').pivot('product_name').sum('Revenue')
product_sales.pivot.show()


26.Save parquet files in external path:
=======================================
product_sales.write.format('parquet').mode('overwrite').option('compression','snappy').save('/Volumes/workspace/sandeep/sample/Ouput/')



27.create external table and load data from external path:
==========================================================
spark.sql("""
  CREATE TABLE product_sales_ext
  USING PARQUET
  LOCATION '/Volumes/workspace/sandeep/sample/Ouput/'
""")